[toc]



### 1、凸问题与非凸问题

在优化问题中，**凸问题**和**非凸问题**是两类不同的优化问题，它们有着不同的性质和求解方法。



1. **凸问题（Convex Problem）**

   

**凸问题**是指其目标函数是**凸函数**，并且其约束条件也构成了一个凸集。凸问题具有良好的数学性质，通常可以通过多种优化算法有效求解。

**凸函数的定义**

一个函数 被称为**凸函数**，如果对于任意的两个点 和 以及 ，该函数满足以下条件：

简单来说，这意味着函数 的图形在任意两点之间的连线在函数图形上方或与函数图形重合。

**凸集的定义**

一个集合 被称为**凸集**，如果对于任意的两个点 和 ，线段 （其中 ）也属于集合 。也就是说，在凸集中的任意两点之间的线段都位于该集合内。

**凸优化问题的形式**

一个典型的凸优化问题可以写作：
$$
\min_{x} f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \quad i = 1, 2, \dots, m
$$
其中：



​	•	 是凸函数，

​	•	 是凸约束函数，定义了一个凸集。



**凸问题的性质**

​	•	**全局最优解**：如果一个凸优化问题有解，那么任何局部最优解也必然是全局最优解。这是凸问题最重要的性质之一。

​	•	**易于求解**：凸优化问题通常可以通过梯度下降法、内点法等算法有效求解。由于局部最优即全局最优，因此这些方法可以稳定收敛到最优解。



**2. 非凸问题（Non-convex Problem）**



与凸问题不同，**非凸问题**的目标函数或约束条件不满足凸性的条件。非凸问题是更为复杂的一类优化问题，求解难度大大增加。

**非凸函数的定义**

一个函数 如果不满足凸函数的定义条件，则称为**非凸函数**。简单来说，==非凸函数的图形可能存在多个**局部极小值**，且连接任意两点的连线并不总是位于函数图形的上方==。

**非凸问题的性质**

​	•	**局部最优解不一定是全局最优解**：非凸问题可能有多个局部最优解，而这些局部最优解不一定是全局最优解。因此，优化算法可能会陷入局部最优解，无法找到全局最优解。

​	•	**难于求解**：由于存在多个局部最优点，许多优化算法（例如梯度下降法）可能会停在局部最优解，无法进一步找到全局最优解。因此，非凸问题的求解通常需要更复杂的算法或启发式方法，如模拟退火、遗传算法等。

**例子：**

​	•	**凸函数**： 是凸函数，因为它在任意区间上的图形是一条向上的抛物线，连接任意两点的线段都位于函数图形的上方。

​	•	**非凸函数**： 是非凸函数，因为它在不同区间上有多个局部极小值，函数图形在某些区间向下弯曲，连接任意两点的线段可能位于函数图形的下方。

**3. 凸问题和非凸问题的对比**

**特性**	**凸问题**	**非凸问题**

**目标函数**	凸函数	非凸函数

**全局最优性**	局部最优解也是全局最优解	**局部最优解不一定是全局最优解**

**解的唯一性**	可能有唯一解（如果严格凸）	可能有多个局部最优解

**求解难度**	相对容易，通常有高效算法	困难，容易陷入局部最优，需复杂算法

**应用场景**	线性回归、支持向量机、Lasso等	神经网络训练、组合优化问题等



**4. 凸问题的求解方法**



​	•	**梯度下降法**：凸问题的梯度下降算法是最常见的求解方法。由于梯度指向函数值下降最快的方向，因此在每一步中沿着负梯度方向更新参数可以有效地收敛到全局最优解。

​	•	**牛顿法**：牛顿法通过使用目标函数的二阶导数信息，可以更快地收敛到最优解，特别是对于凸问题表现很好。

​	•	**内点法**：内点法是解决凸优化问题的经典算法之一，特别适用于具有约束条件的问题。



**5. 非凸问题的求解方法**



由于非凸问题的复杂性，常见的求解方法包括：

​	•	**随机初始化和多次运行**：通过多次从不同的初始点运行算法，增加找到全局最优解的可能性。

​	•	**模拟退火**：一种基于随机搜索的启发式算法，能够跳出局部最优解，尝试找到全局最优解。

​	•	**遗传算法**：模拟自然进化的启发式算法，通过交叉、变异等操作探索全局解。

​	•	**动量法和Adam优化器**：在深度学习中常用的优化器，这些方法通过动量积累帮助算法跳出局部最优解。



**6. 凸问题与非凸问题的应用**



​	•	**凸问题的应用**：许多经典机器学习算法都涉及到凸优化问题，例如线性回归、逻辑回归、支持向量机、Lasso等。这些问题可以高效地通过梯度下降法或其他凸优化方法求解。

​	•	**非凸问题的应用**：深度神经网络中的训练过程通常涉及到非凸优化，因为神经网络的损失函数通常具有多个局部最优点。此外，组合优化问题、图像处理中的某些问题也是非凸的。



### 2、机器学习  SSM





在机器学习领域，**SSM** 通常是指**状态空间模型**（State Space Model），这是一个用于描述动态系统的数学模型，广泛应用于时间序列分析、控制系统、信号处理等领域。



**状态空间模型 (SSM) 的基本概念：**



​	1.	**状态变量**：代表系统在某一时刻的内在状态，通常是不可直接观测的隐藏变量。

​	2.	**观测变量**：代表可观测的变量，通常是从外界获取的与系统状态相关的数据信号。

​	3.	**状态方程**：描述状态变量如何随时间演化，通常通过线性或非线性方程表示。

​	4.	**观测方程**：描述观测变量如何与状态变量相关联，常用线性关系，但也可以是非线性形式。



SSM 的常见形式是线性高斯模型，即状态和观测方程都服从高斯分布，常用的算法如**卡尔曼滤波**就是针对线性高斯状态空间模型的。



**主要用途：**



​	•	**时间序列预测**：可以用于预测时间序列的未来状态。

​	•	**控制系统**：SSM 在控制理论中广泛应用于描述和预测系统行为。

​	•	**信号处理**：处理不完全观测或噪声干扰的信号。



在机器学习中，SSM 可以结合**递归神经网络 (RNN)**、**LSTM** 或**Transformer**模型来处理复杂的序列数据，尤其在自然语言处理和视频分析等领域。



### 3、Mamba



> 推荐参考文章  https://blog.csdn.net/v_JULY_v/article/details/134923301

- 涉及实变和实不变的问题，通俗来讲其实就是提取非连续特征的问题，传统SSM实现是通过卷积实现的，只能提取连续特征





![image-20241023191935732](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023191935732.png)





==SSM 是用于描述这些状态表示并根据某些输入预测其下一个状态可能是什么的模型==



- 至此，总结一下，将 SSM 表示为卷积的一个主要好处是它可以像卷积神经网络CNN一样进行并行训练。然而，由于内核大小固定，它们的推理不如 RNN 那样快速

![image-20241023212557360](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023212557360.png)

==推理用RNN结构，训练用CNN  结构==





如此，S4的定义就出来了：<font color=red>序列的结构化状态空间</font>——Structured State Space for Sequences，一类可以有效处理长序列的 SSM(上文1.3节的开头有提到，S4所对应的论文为：Efficiently Modeling Long Sequences with Structured State Spaces)




![image-20241023215504787](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023215504787.png)





- 四个S  俗称4S        序列的结构化状态空间

- 为什么用mamba   TRM  为什么



---

- 而SSM的问题在于其中的矩阵A B C不随输入不同而不同，即无法针对不同的输入针对性的推理(详见上文的2.3节)
  最终，Mamba的解决办法是，相比SSM压缩所有历史记录，mamba设计了一个简单的选择机制，通过“参数化SSM的输入”，让模型对信息有选择性处理，以便关注或忽略特定的输入

![image-20241028111822637](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241028111822637.png)

==**维度**通常指数据特征的数量，即模型输入空间的特征数==



并行扫描算法







论文的标题是《Mamba: Linear-Time Sequence Modeling with Selective State Spaces》，作者是Albert Gu和Tri Dao，分别来自卡内基梅隆大学的机器学习系和普林斯顿大学的计算机科学系。论文的主要内容是介绍了一种新型的序列模型，名为Mamba，它基于结构化状态空间模型（Structured State Space Models，简称SSMs）并引入了选择性状态空间，以提高对长序列数据的建模效率和效果。

以下是对论文内容的详细介绍：



摘要（Abstract）

- 论文指出，尽管Transformer模型及其核心的注意力机制在深度学习中非常有效，但在处理长序列数据时存在计算效率低下的问题。
- 为了解决这个问题，作者提出了一种新的模型——Mamba。它通过选择性==状态空间机制==，能够在保持线性时间复杂度的同时，对长序列数据进行有效的建模。
- Mamba模型在多种模态（如语言、音频和基因组学）上都取得了最先进的性能，特别是在处理长达百万个长度的序列时。

引言（Introduction）

- 论文介绍了基础模型（Foundation Models，简称FMs）的概念，这些模型通常在大量数据上预训练，然后适应下游任务。
- 作者指出，现有的基于Transformer的模型在处理长序列时存在局限性，如无法建模超出有限窗口范围的内容，以及随着窗口长度的增加，计算复杂度呈二次方增长。



状态空间模型（State Space Models）

- 论文详细介绍了状态空间模型（SSMs）的背景和相关研究，包括它们与递归神经网络（RNNs）和卷积神经网络（CNNs）的联系。
- ==SSMs通过线性或近线性的序列长度缩放，可以非常高效地计算，并且有原则性机制来建模某些数据模态中的长期依赖性==。



选择性状态空间模型（Selective State Space Models）

- 作者提出了选择性状态空间模型（Selective SSMs），这是对现有SSMs的改进。选择性机制允许模型根据当前的输入 token 选择性地传播或忘记信息。
- 为了解决由此带来的计算挑战，作者设计了一种硬件感知的并行算法，该算法以递归模式运行，避免了在GPU内存层次结构之间进行IO访问。



Mamba架构

- <font color=red>论文介绍了Mamba架构，它将==选择性状态空间==集成到一个简化的端到端神经网络中，没有注意力机制或MLP块</font>。

- Mamba在推理速度上比Transformer快5倍，并且在序列长度上具有线性缩放性。在实际数据上，Mamba的性能在长达百万长度的序列上得到了提升。

 

### 4、num_workers



在机器学习中，num_workers 是一个用于控制**数据加载时的并行线程数**或**子进程数**的参数。它通常用于数据加载库，例如 PyTorch 中的 DataLoader。以下是 num_workers 的详细解释：



​	1.	**作用**：

​	•	==num_workers 用于设置数据加载的并行子进程数。在数据加载时，如果设置了多个工作进程，数据加载会并行进行，可以提高数据读取速度，特别是在数据预处理开销较大或者 I/O 较慢（如读取图像或视频文件）时==。

​	2.	**常见使用场景**：

​	•	在训练深度学习模型时，数据通常需要进行一些预处理操作（如数据增强、归一化等），如果不设置并行加载，数据加载的速度可能成为训练的瓶颈，导致 GPU 等计算资源空闲等待数据。

​	•	通过增加 num_workers，可以同时启动多个进程加载数据，让模型的计算与数据的预处理并行进行，从而提高训练效率。

​	3.	**设置建议**：

​	•	一般来说，num_workers 的最佳值取决于硬件配置和数据集大小。

​	•	**常用设置**：可以从 2 开始测试，逐渐增加。对于拥有 4 核心的 CPU，可以尝试设置 num_workers=4 或更高。

​	•	如果 num_workers 设置过高，可能会导致 CPU 资源紧张，出现性能下降，甚至引起内存泄漏或崩溃。

​	4.	**注意事项**：

​	•	在不同的操作系统中，num_workers 的行为可能有所不同，例如在 Windows 系统中，num_workers 必须大于 0，否则会有报错。

​	•	设置过多的 num_workers 可能会导致进程管理的开销增大，导致性能下降，因此需要根据实际情况进行调优。



在 PyTorch 中的示例：



````python
from torch.utils.data import DataLoader
# 假设有一个数据集 dataset`
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
````





在上面的代码中，num_workers=4 表示数据加载时会创建 4 个子进程并行加载数据，提升数据加载速度。







### 5、Hippo矩阵

Hippo 矩阵是一种用于高效记忆时间序列信息的技术，全称是**Hi**dden **P**rocess **Po**lynomials（隐藏过程多项式）。该技术最早由 MIT 的研究团队提出，旨在通过设计一种特殊的状态矩阵，==使得时间序列信息可以在较长时间内有效保留==。这种矩阵技术在自然语言处理、视频理解等任务中具有广泛的应用，尤其在需要长序列信息处理的任务中展现出优势。



**核心思想**



Hippo 矩阵的核心思想是**隐状态矩阵的多项式更新**。==统的 RNN、LSTM 等方法在序列长度增加时，记忆能力往往会衰减==，<font color=red>而 Hippo 矩阵则通过设计一系列多项式更新规则，使得信息可以在序列中更长时间地保留</font>。Hippo 矩阵的基本实现依赖于一组特殊的正交多项式（如勒让德多项式），以实现不同时间步的状态更新。



**原理**



Hippo 矩阵的更新公式大致如下：



其中， 是通过特定设计的状态矩阵， 是输入矩阵， 表示隐状态向量， 是输入序列。Hippo 的创新点在于， **矩阵的结构使得其具有“记忆时间”的能力，能较好地保留时间序列信息。**



**优势**



​	1.	**长程记忆能力**：相比传统的 RNN 或 LSTM，Hippo 矩阵能够显著延长模型的记忆时间。

​	2.	**低计算复杂度**：其设计具有较高的计算效率，适用于资源受限的环境，例如边缘设备。

​	3.	**可扩展性强**：由于其基于矩阵运算的特性，可以容易地与其他序列模型（如 Transformer）进行结合，提高模型的长程依赖能力。



**应用**



Hippo 矩阵已被应用于自然语言处理、视频分析和时序数据建模等领域。例如，在长序列的 NLP 任务中，利用 Hippo 矩阵可以增强模型的上下文记忆能力，从而提升生成文本的流畅性与连贯性。



如果你需要更详细的数学推导或代码实现，我可以提供相关的资料。





### 6 、CMamba  学习



这篇论文的标题是《CMAMBA: Channel Correlation Enhanced State Space Models for Multivariate Time Series Forecasting》 论文的主要内容是提出了一个名为CMamba的==新型状态空间模型，用于多变量时间序列预测==。下面是对论文内容的详细介绍：



摘要（Abstract）

- 论文指出，多变量时间序列预测的最新进展受到了基于<font color=red>线性、Transformer和卷积模型的推动</font>，其中基于Transformer的架构因其在**时间和跨通道混合**方面的效果而受到关注。
- 最近，Mamba模型作为一种状态空间模型，因其在序列和特征混合方面的能力而出现。然而，**原始Mamba设计在处理跨通道依赖性方面存在不足**，这对于提高多变量时间序列预测的性能至关重要。
- 论文提出了CMamba模型，它结合了修改后的Mamba<font color=blue>（M-Mamba）模块用于时间依赖性建模</font>，<font color=green>全局数据依赖的MLP</font></font>（GDD-MLP）用于有效捕获跨通道依赖性，以及通道混合机制来减轻过拟合。
- 通过在七个真实世界数据集上的实验，证明了CMamba模型在提高预测性能方面的有效性。



引言（Introduction）

- ==多变量时间序列预测（MTSF）==在多个应用领域中扮演着重要角色，如天气预报、交通管理、经济和事件预测等。
- 近年来，许多深度学习模型被开发出来，包括基于线性、Transformer和卷积的模型。
- Transformer模型因其能够分别混合时间和通道嵌入而受到特别关注。
- Mamba模型在自然语言处理和其他领域表现出了强大的长序列建模能力，但在时间序列预测中的跨时间依赖性建模方面仍有待探索。



相关工作（Related Work）

- 论文回顾了状态空间模型（SSMs）在序列建模中的应用，包括隐藏马尔可夫模型和递归神经网络（RNNs）。
- 论文还讨论了多变量时间序列预测中的通道策略，包括通道独立（CI）策略和通道依赖（CD）策略。
- 论文提到了mixup技术在多变量时间序列分析中的应用。





方法论（Methodology）

- 论文详细介绍了CMamba模型的整体结构，包括通**道混合模块、M-Mamba模块和GDD-MLP模块。**
- M-Mamba模块负责建模时间依赖性，而GDD-MLP模块负责捕获跨通道依赖性。
- 论文还介绍了通道混合策略，这是一种在训练期间创建虚拟通道的方法，以提高模型的泛化能力。



结论（Conclusion）

- **论文提出了CMamba，这是一个用于多变量时间序列预测的新型状态空间模型。**
- CMamba通过结合M-Mamba模块、GDD-MLP模块和通道混合策略，在多个真实世界数据集上实现了最先进的性能。
- 论文指出，GDD-MLP和通道混合模块可以轻松地插入到其他模型中，并且成本很小，展示了框架的广泛适用性。
- 作者表示，他们计划在未来探索更有效的技术来捕获跨时间和跨通道依赖性。

论文还包含了一些附加信息，如作者的联系方式、代码的可用性以及参考文献。此外，论文还提供了实验的详细设置、数据集描述、超参数设置、基线模型的描述和修改、以及模型的更多评估结果。





<font size=5>M-Mamba模块</font>

M-Mamba模块是CMamba模型中用于**捕捉时间依赖性的部分**。它是对原始Mamba模型的修改和扩展，专门针对多变量时间序列预测任务进行了优化。以下是M-Mamba模块的主要特点：

1. **补丁分割（Patching）**：M-Mamba模块首先将输入的多变量时间序列分割成一系列补丁（patches）。==这是通过移动窗口方法实现的==，其中每个补丁包含固定数量的时间步长（例如，16个时间步长）。

2. **线性投影和位置编码**：每个补丁通过==线性层被投影到一个更高维的空间==，并加上位置编码。位置编码帮助模型理解时间序列中的位置信息。

3. **状态空间模型**：M-Mamba模块使用状态空间模型（State Space Model，SSM）来==捕捉时间序列中的动态变化==。这涉及到状态转移矩阵和观测矩阵，它们定义了如何从当前状态和输入预测下一个状态。

4. **数据依赖性**：M-Mamba模块的设计允许参数依赖于输入数据，这使得模型能够更加灵活地捕捉时间序列中的复杂模式。



<font size=5>GDD-MLP模块</font>

GDD-MLP（Global Data-Dependent Multi-Layer Perceptron）模块是CMamba模型中用于==捕捉跨通道依赖性的部分==。这个模块的核心思想是使用**全局数据依赖的多层感知机来建模不同时间序列通道之间的关系**。以下是GDD-MLP模块的主要特点：

1. **全局数据依赖**：GDD-MLP模块的权重和偏置是数据依赖的，这意味着它们会根据输入数据的不同而变化。这种设计使得模型能够更加灵活地捕捉不同通道之间的复杂关系。

2. **多层感知机**：GDD-MLP模块使用多层感知机来学习跨通道的依赖性。这包括对每个通道的历史序列进行编码，并使用这些编码来预测未来的值。

3. **池化和混合**：在应用MLP之前，GDD-MLP模块使用平均池化和最大池化来捕获每个通道的全局特征。这些特征被用来指导跨通道的混合过程。



<font size=5>通道混合策略（Channel Mixup）</font>

通道混合策略是一种<font color=red>数据增强技术，用于提高模型的泛化能力</font>。它通过在训练期间线性组合不同的通道来创建虚拟通道，这些虚拟通道融合了多个通道的特征，同时保留了它们共享的时间依赖性。以下是通道混合策略的主要特点：

1. **线性组合**：在训练期间，通道混合策略通过线性组合不同通道的数据来创建新的虚拟通道。这种组合是通过随机选择两个通道并应用线性插值来实现的。

2. **增强泛化能力**：通过引入虚拟通道，通道混合策略增加了模型在训练期间看到的样本多样性。这有助于模型学习到更加鲁棒的特征表示，从而提高其在未知数据上的泛化能力。

3. **减轻过拟合**：通道混合策略还可以减轻过拟合的问题，因为它通过引入新的虚拟样本来打破模型对特定通道组合的过度依赖。

这些组件共同构成了CMamba模型，使其能够有效地处理多变量时间序列数据，并捕获必要的时间依赖性和跨通道依赖性，以实现准确的预测。



 ### 7、位置编码

- 位置编码（Positional Encoding）是一种用于为==输入数据中的序列元素==（如单词或图像的顺序特征）编码位置信息的技术，尤其是在Transformer模型中非常关键。<font color=red>由于Transformer模型的自注意力机制不具有天然的序列信息（它是并行计算的，不能捕捉数据的顺序），因此需要将位置编码加入到输入数据中，以提供序列位置信息</font>。



**为什么需要位置编码**

- 在传统的RNN和LSTM等循环神经网络（RNN）中，==序列信息是通过逐步递归传递的==，但在Transformer中，输入序列是并行处理的，没有顺序。因此，为了让模型区分序列中元素的相对位置（如哪个单词在句子的前面，哪个在后面），需要引入位置编码。



位置编码向模型提供了每个位置在序列中的位置信息，这样即使在没有递归的情况下，模型也能感知输入的顺序。

**位置编码的常用方法**



- 在Transformer中，常用的是基于正弦和余弦函数的**固定位置编码**，此外，还有**可学习的位置编码**。





### 8、Mamba 并行算法

**Mamba 的并行扫描**是并行算法中用于==快速计算前缀和（prefix sum）==的一种方法。该方法被广泛应用在图形处理单元（GPU）等需要并行计算的环境中，尤其是在涉及大量数据时，能极大地加速计算。<font color=red>并行扫描通过将输入数据划分为多个部分（block）并在每个部分上进行独立计算，达到减少运算时间的效果</font>。



**什么是前缀和（Prefix Sum）？**



前缀和是一种从左到右累计和的操作。对于给定的数组A=[a1,a2,a3……a4] ，前缀和数组 定义为：



**Mamba 并行扫描的流程**



在 Mamba 的并行扫描中，数据处理被分为两步：**分块扫描（Block Scan）和跨块累加（Block Aggregate）**。



​	1.	**分块扫描（Block Scan）**：

​	•	首先，<font color=blue>将输入数据数组划分为大小相等的多个块（block）。在 GPU 上，每个线程块独立处理一个数据块</font>。

​	•	对于每个块，使用==串行或小规模的并行扫描==计算块内的前缀和。

​	•	这样可以得到每个块的最后一个元素，即该块的整体和。这个和记录在一个新的数组中，用于后续的累加计算。

​	2.	**跨块累加（Block Aggregate）**：

​	•	接下来，将每个块的整体和累加起来，==形成块和的前缀和==。这样，每个块的前缀和可以作为该块的偏移量。

​	•	使用这些偏移量，将累加值添加到每个块的元素上。这样可以得到全局的前缀和。

​	3.	**最后的加和操作**：

​	•	将跨块累加得到的偏移量加回到每个块的元素上，从而完成整个数组的前缀和计算。



**并行扫描的优点**



​	•	**高效性**：Mamba 并行扫描可以利用多线程并行地处理不同块的数据，在 GPU 上可以极大地加速前缀和的计算。

​	•	**扩展性**：通过分块和并行计算，算法可以处理更大规模的数据集，适合高维度数据和海量数据的场景。

​	•	**可移植性**：该方法适用于大多数支持并行计算的硬件平台，尤其是在 GPU 上具有显著的加速效果。



**应用场景**



Mamba 并行扫描广泛应用于需要高效前缀和计算的任务中，如并行排序、图像处理、并行累积和分布式计算等。









