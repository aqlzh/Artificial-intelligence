[toc]



### 1、凸问题与非凸问题





​	•	**回归**：预测的是**连续值**，也就是说输出是一个实数。例如，预测房价、气温、股票价格等。==回归任务的目标是找到一种模型，使得输入数据与输出之间的关系可以用一个连续的函数来描述==。

​	•	**分类**：预测的是**离散类别**，也就是说输出是一组类别中的一个。例如，预测邮件是“垃圾邮件”还是“正常邮件”，或者预测一个图片是“猫”还是“狗”。分类任务的目标是找到一种模型，将输入数据分配到离散的类别标签中。







在优化问题中，**凸问题**和**非凸问题**是两类不同的优化问题，它们有着不同的性质和求解方法。



1. **凸问题（Convex Problem）**

   

**凸问题**是指其目标函数是**凸函数**，并且其约束条件也构成了一个凸集。凸问题具有良好的数学性质，通常可以通过多种优化算法有效求解。

**凸函数的定义**

一个函数 被称为**凸函数**，如果对于任意的两个点 和 以及 ，该函数满足以下条件：

简单来说，这意味着函数 的图形在任意两点之间的连线在函数图形上方或与函数图形重合。

**凸集的定义**

一个集合 被称为**凸集**，如果对于任意的两个点 和 ，线段 （其中 ）也属于集合 。也就是说，在凸集中的任意两点之间的线段都位于该集合内。

**凸优化问题的形式**

一个典型的凸优化问题可以写作：
$$
\min_{x} f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \quad i = 1, 2, \dots, m
$$
其中：



​	•	 是凸函数，

​	•	 是凸约束函数，定义了一个凸集。



**凸问题的性质**

​	•	**全局最优解**：如果一个凸优化问题有解，那么任何局部最优解也必然是全局最优解。这是凸问题最重要的性质之一。

​	•	**易于求解**：凸优化问题通常可以通过梯度下降法、内点法等算法有效求解。由于局部最优即全局最优，因此这些方法可以稳定收敛到最优解。



**2. 非凸问题（Non-convex Problem）**



与凸问题不同，**非凸问题**的目标函数或约束条件不满足凸性的条件。非凸问题是更为复杂的一类优化问题，求解难度大大增加。

**非凸函数的定义**

一个函数 如果不满足凸函数的定义条件，则称为**非凸函数**。简单来说，==非凸函数的图形可能存在多个**局部极小值**，且连接任意两点的连线并不总是位于函数图形的上方==。

**非凸问题的性质**

​	•	**局部最优解不一定是全局最优解**：非凸问题可能有多个局部最优解，而这些局部最优解不一定是全局最优解。因此，优化算法可能会陷入局部最优解，无法找到全局最优解。

​	•	**难于求解**：由于存在多个局部最优点，许多优化算法（例如梯度下降法）可能会停在局部最优解，无法进一步找到全局最优解。因此，非凸问题的求解通常需要更复杂的算法或启发式方法，如模拟退火、遗传算法等。

**例子：**

​	•	**凸函数**： 是凸函数，因为它在任意区间上的图形是一条向上的抛物线，连接任意两点的线段都位于函数图形的上方。

​	•	**非凸函数**： 是非凸函数，因为它在不同区间上有多个局部极小值，函数图形在某些区间向下弯曲，连接任意两点的线段可能位于函数图形的下方。

**3. 凸问题和非凸问题的对比**

**特性**	**凸问题**	**非凸问题**

**目标函数**	凸函数	非凸函数

**全局最优性**	局部最优解也是全局最优解	**局部最优解不一定是全局最优解**

**解的唯一性**	可能有唯一解（如果严格凸）	可能有多个局部最优解

**求解难度**	相对容易，通常有高效算法	困难，容易陷入局部最优，需复杂算法

**应用场景**	线性回归、支持向量机、Lasso等	神经网络训练、组合优化问题等



**4. 凸问题的求解方法**



​	•	**梯度下降法**：凸问题的梯度下降算法是最常见的求解方法。由于梯度指向函数值下降最快的方向，因此在每一步中沿着负梯度方向更新参数可以有效地收敛到全局最优解。

​	•	**牛顿法**：牛顿法通过使用目标函数的二阶导数信息，可以更快地收敛到最优解，特别是对于凸问题表现很好。

​	•	**内点法**：内点法是解决凸优化问题的经典算法之一，特别适用于具有约束条件的问题。



**5. 非凸问题的求解方法**



由于非凸问题的复杂性，常见的求解方法包括：

​	•	**随机初始化和多次运行**：通过多次从不同的初始点运行算法，增加找到全局最优解的可能性。

​	•	**模拟退火**：一种基于随机搜索的启发式算法，能够跳出局部最优解，尝试找到全局最优解。

​	•	**遗传算法**：模拟自然进化的启发式算法，通过交叉、变异等操作探索全局解。

​	•	**动量法和Adam优化器**：在深度学习中常用的优化器，这些方法通过动量积累帮助算法跳出局部最优解。



**6. 凸问题与非凸问题的应用**



​	•	**凸问题的应用**：许多经典机器学习算法都涉及到凸优化问题，例如线性回归、逻辑回归、支持向量机、Lasso等。这些问题可以高效地通过梯度下降法或其他凸优化方法求解。

​	•	**非凸问题的应用**：深度神经网络中的训练过程通常涉及到非凸优化，因为神经网络的损失函数通常具有多个局部最优点。此外，组合优化问题、图像处理中的某些问题也是非凸的。



### 2、机器学习  SSM





在机器学习领域，**SSM** 通常是指**状态空间模型**（State Space Model），这是一个用于描述动态系统的数学模型，广泛应用于时间序列分析、控制系统、信号处理等领域。



**状态空间模型 (SSM) 的基本概念：**



​	1.	**状态变量**：代表系统在某一时刻的内在状态，通常是不可直接观测的隐藏变量。

​	2.	**观测变量**：代表可观测的变量，通常是从外界获取的与系统状态相关的数据信号。

​	3.	**状态方程**：描述状态变量如何随时间演化，通常通过线性或非线性方程表示。

​	4.	**观测方程**：描述观测变量如何与状态变量相关联，常用线性关系，但也可以是非线性形式。



SSM 的常见形式是线性高斯模型，即状态和观测方程都服从高斯分布，常用的算法如**卡尔曼滤波**就是针对线性高斯状态空间模型的。



**主要用途：**



​	•	**时间序列预测**：可以用于预测时间序列的未来状态。

​	•	**控制系统**：SSM 在控制理论中广泛应用于描述和预测系统行为。

​	•	**信号处理**：处理不完全观测或噪声干扰的信号。



在机器学习中，SSM 可以结合**递归神经网络 (RNN)**、**LSTM** 或**Transformer**模型来处理复杂的序列数据，尤其在自然语言处理和视频分析等领域。



### 3、Mamba



> 推荐参考文章  https://blog.csdn.net/v_JULY_v/article/details/134923301

- 涉及实变和实不变的问题，通俗来讲其实就是提取非连续特征的问题，传统SSM实现是通过卷积实现的，只能提取连续特征





![image-20241023191935732](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023191935732.png)





==SSM 是用于描述这些状态表示并根据某些输入预测其下一个状态可能是什么的模型==



- 至此，总结一下，将 SSM 表示为卷积的一个主要好处是它可以像卷积神经网络CNN一样进行并行训练。然而，由于内核大小固定，它们的推理不如 RNN 那样快速

![image-20241023212557360](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023212557360.png)

==推理用RNN结构，训练用CNN  结构==





如此，S4的定义就出来了：<font color=red>序列的结构化状态空间</font>——Structured State Space for Sequences，一类可以有效处理长序列的 SSM(上文1.3节的开头有提到，S4所对应的论文为：Efficiently Modeling Long Sequences with Structured State Spaces)




![image-20241023215504787](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241023215504787.png)





- 四个S  俗称4S        序列的结构化状态空间

- 为什么用mamba   TRM  为什么



---

- 而SSM的问题在于其中的矩阵A B C不随输入不同而不同，即无法针对不同的输入针对性的推理(详见上文的2.3节)
  最终，Mamba的解决办法是，相比SSM压缩所有历史记录，mamba设计了一个简单的选择机制，通过“参数化SSM的输入”，让模型对信息有选择性处理，以便关注或忽略特定的输入

![image-20241028111822637](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241028111822637.png)

==**维度**通常指数据特征的数量，即模型输入空间的特征数==



并行扫描算法







论文的标题是《Mamba: Linear-Time Sequence Modeling with Selective State Spaces》，作者是Albert Gu和Tri Dao，分别来自卡内基梅隆大学的机器学习系和普林斯顿大学的计算机科学系。论文的主要内容是介绍了一种新型的序列模型，名为Mamba，它基于结构化状态空间模型（Structured State Space Models，简称SSMs）并引入了选择性状态空间，以提高对长序列数据的建模效率和效果。

以下是对论文内容的详细介绍：



摘要（Abstract）

- 论文指出，尽管Transformer模型及其核心的注意力机制在深度学习中非常有效，但在处理长序列数据时存在计算效率低下的问题。
- 为了解决这个问题，作者提出了一种新的模型——Mamba。它通过选择性==状态空间机制==，能够在保持线性时间复杂度的同时，对长序列数据进行有效的建模。
- Mamba模型在多种模态（如语言、音频和基因组学）上都取得了最先进的性能，特别是在处理长达百万个长度的序列时。

引言（Introduction）

- 论文介绍了基础模型（Foundation Models，简称FMs）的概念，这些模型通常在大量数据上预训练，然后适应下游任务。
- 作者指出，现有的基于Transformer的模型在处理长序列时存在局限性，如无法建模超出有限窗口范围的内容，以及随着窗口长度的增加，计算复杂度呈二次方增长。



状态空间模型（State Space Models）

- 论文详细介绍了状态空间模型（SSMs）的背景和相关研究，包括它们与递归神经网络（RNNs）和卷积神经网络（CNNs）的联系。
- ==SSMs通过线性或近线性的序列长度缩放，可以非常高效地计算，并且有原则性机制来建模某些数据模态中的长期依赖性==。



选择性状态空间模型（Selective State Space Models）

- 作者提出了选择性状态空间模型（Selective SSMs），这是对现有SSMs的改进。选择性机制允许模型根据当前的输入 token 选择性地传播或忘记信息。
- 为了解决由此带来的计算挑战，作者设计了一种硬件感知的并行算法，该算法以递归模式运行，避免了在GPU内存层次结构之间进行IO访问。



Mamba架构

- <font color=red>论文介绍了Mamba架构，它将==选择性状态空间==集成到一个简化的端到端神经网络中，没有注意力机制或MLP块</font>。

- Mamba在推理速度上比Transformer快5倍，并且在序列长度上具有线性缩放性。在实际数据上，Mamba的性能在长达百万长度的序列上得到了提升。

 

缺乏跨渠道依赖建模能力 --> 捕获跨通道依赖性方面

### 4、num_workers



在机器学习中，num_workers 是一个用于控制**数据加载时的并行线程数**或**子进程数**的参数。它通常用于数据加载库，例如 PyTorch 中的 DataLoader。以下是 num_workers 的详细解释：



​	1.	**作用**：

​	•	==num_workers 用于设置数据加载的并行子进程数。在数据加载时，如果设置了多个工作进程，数据加载会并行进行，可以提高数据读取速度，特别是在数据预处理开销较大或者 I/O 较慢（如读取图像或视频文件）时==。

​	2.	**常见使用场景**：

​	•	在训练深度学习模型时，数据通常需要进行一些预处理操作（如数据增强、归一化等），如果不设置并行加载，数据加载的速度可能成为训练的瓶颈，导致 GPU 等计算资源空闲等待数据。

​	•	通过增加 num_workers，可以同时启动多个进程加载数据，让模型的计算与数据的预处理并行进行，从而提高训练效率。

​	3.	**设置建议**：

​	•	一般来说，num_workers 的最佳值取决于硬件配置和数据集大小。

​	•	**常用设置**：可以从 2 开始测试，逐渐增加。对于拥有 4 核心的 CPU，可以尝试设置 num_workers=4 或更高。

​	•	如果 num_workers 设置过高，可能会导致 CPU 资源紧张，出现性能下降，甚至引起内存泄漏或崩溃。

​	4.	**注意事项**：

​	•	在不同的操作系统中，num_workers 的行为可能有所不同，例如在 Windows 系统中，num_workers 必须大于 0，否则会有报错。

​	•	设置过多的 num_workers 可能会导致进程管理的开销增大，导致性能下降，因此需要根据实际情况进行调优。



在 PyTorch 中的示例：



````python
from torch.utils.data import DataLoader
# 假设有一个数据集 dataset`
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
````





在上面的代码中，num_workers=4 表示数据加载时会创建 4 个子进程并行加载数据，提升数据加载速度。







### 5、Hippo矩阵

Hippo 矩阵是一种用于高效记忆时间序列信息的技术，全称是**Hi**dden **P**rocess **Po**lynomials（隐藏过程多项式）。该技术最早由 MIT 的研究团队提出，旨在通过设计一种特殊的状态矩阵，==使得时间序列信息可以在较长时间内有效保留==。这种矩阵技术在自然语言处理、视频理解等任务中具有广泛的应用，尤其在需要长序列信息处理的任务中展现出优势。



**核心思想**



Hippo 矩阵的核心思想是**隐状态矩阵的多项式更新**。==统的 RNN、LSTM 等方法在序列长度增加时，记忆能力往往会衰减==，<font color=red>而 Hippo 矩阵则通过设计一系列多项式更新规则，使得信息可以在序列中更长时间地保留</font>。Hippo 矩阵的基本实现依赖于一组特殊的正交多项式（如勒让德多项式），以实现不同时间步的状态更新。



**原理**



Hippo 矩阵的更新公式大致如下：



其中， 是通过特定设计的状态矩阵， 是输入矩阵， 表示隐状态向量， 是输入序列。Hippo 的创新点在于， **矩阵的结构使得其具有“记忆时间”的能力，能较好地保留时间序列信息。**



**优势**



​	1.	**长程记忆能力**：相比传统的 RNN 或 LSTM，Hippo 矩阵能够显著延长模型的记忆时间。

​	2.	**低计算复杂度**：其设计具有较高的计算效率，适用于资源受限的环境，例如边缘设备。

​	3.	**可扩展性强**：由于其基于矩阵运算的特性，可以容易地与其他序列模型（如 Transformer）进行结合，提高模型的长程依赖能力。



**应用**



Hippo 矩阵已被应用于自然语言处理、视频分析和时序数据建模等领域。例如，在长序列的 NLP 任务中，利用 Hippo 矩阵可以增强模型的上下文记忆能力，从而提升生成文本的流畅性与连贯性。



如果你需要更详细的数学推导或代码实现，我可以提供相关的资料。





### 6 、CMamba  学习



这篇论文的标题是《CMAMBA: Channel Correlation Enhanced State Space Models for Multivariate Time Series Forecasting》 论文的主要内容是提出了一个名为CMamba的==新型状态空间模型，用于多变量时间序列预测==。下面是对论文内容的详细介绍：



摘要（Abstract）

- 论文指出，多变量时间序列预测的最新进展受到了基于<font color=red>线性、Transformer和卷积模型的推动</font>，其中基于Transformer的架构因其在**时间和跨通道混合**方面的效果而受到关注。
- 最近，Mamba模型作为一种状态空间模型，因其在序列和特征混合方面的能力而出现。然而，**原始Mamba设计在处理跨通道依赖性方面存在不足**，这对于提高多变量时间序列预测的性能至关重要。
- 论文提出了CMamba模型，它结合了修改后的Mamba<font color=blue>（M-Mamba）模块用于时间依赖性建模</font>，<font color=green>全局数据依赖的MLP</font></font>（GDD-MLP）用于有效捕获跨通道依赖性，以及通道混合机制来减轻过拟合。
- 通过在七个真实世界数据集上的实验，证明了CMamba模型在提高预测性能方面的有效性。



引言（Introduction）

- ==多变量时间序列预测（MTSF）==在多个应用领域中扮演着重要角色，如天气预报、交通管理、经济和事件预测等。
- 近年来，许多深度学习模型被开发出来，包括基于线性、Transformer和卷积的模型。
- Transformer模型因其能够分别混合时间和通道嵌入而受到特别关注。
- Mamba模型在自然语言处理和其他领域表现出了强大的长序列建模能力，但在时间序列预测中的跨时间依赖性建模方面仍有待探索。



相关工作（Related Work）

- 论文回顾了状态空间模型（SSMs）在序列建模中的应用，包括隐藏马尔可夫模型和递归神经网络（RNNs）。
- 论文还讨论了多变量时间序列预测中的通道策略，包括通道独立（CI）策略和通道依赖（CD）策略。
- 论文提到了mixup技术在多变量时间序列分析中的应用。





方法论（Methodology）

- 论文详细介绍了CMamba模型的整体结构，包括通**道混合模块、M-Mamba模块和GDD-MLP模块。**
- M-Mamba模块负责建模时间依赖性，而GDD-MLP模块负责捕获跨通道依赖性。
- 论文还介绍了通道混合策略，这是一种在训练期间创建虚拟通道的方法，以提高模型的泛化能力。



结论（Conclusion）

- **论文提出了CMamba，这是一个用于多变量时间序列预测的新型状态空间模型。**
- CMamba通过结合M-Mamba模块、GDD-MLP模块和通道混合策略，在多个真实世界数据集上实现了最先进的性能。
- 论文指出，GDD-MLP和通道混合模块可以轻松地插入到其他模型中，并且成本很小，展示了框架的广泛适用性。
- 作者表示，他们计划在未来探索更有效的技术来捕获跨时间和跨通道依赖性。

论文还包含了一些附加信息，如作者的联系方式、代码的可用性以及参考文献。此外，论文还提供了实验的详细设置、数据集描述、超参数设置、基线模型的描述和修改、以及模型的更多评估结果。





<font size=5>M-Mamba模块</font>

M-Mamba模块是CMamba模型中用于**捕捉时间依赖性的部分**。它是对原始Mamba模型的修改和扩展，专门针对多变量时间序列预测任务进行了优化。以下是M-Mamba模块的主要特点：

1. **补丁分割（Patching）**：M-Mamba模块首先将输入的多变量时间序列分割成一系列补丁（patches）。==这是通过移动窗口方法实现的==，其中每个补丁包含固定数量的时间步长（例如，16个时间步长）。

2. **线性投影和位置编码**：每个补丁通过==线性层被投影到一个更高维的空间==，并加上位置编码。位置编码帮助模型理解时间序列中的位置信息。

3. **状态空间模型**：M-Mamba模块使用状态空间模型（State Space Model，SSM）来==捕捉时间序列中的动态变化==。这涉及到状态转移矩阵和观测矩阵，它们定义了如何从当前状态和输入预测下一个状态。

4. **数据依赖性**：M-Mamba模块的设计允许参数依赖于输入数据，这使得模型能够更加灵活地捕捉时间序列中的复杂模式。



<font size=5>GDD-MLP模块</font>

GDD-MLP（Global Data-Dependent Multi-Layer Perceptron）模块是CMamba模型中用于==捕捉跨通道依赖性的部分==。这个模块的核心思想是使用**全局数据依赖的多层感知机来建模不同时间序列通道之间的关系**。以下是GDD-MLP模块的主要特点：

1. **全局数据依赖**：GDD-MLP模块的权重和偏置是数据依赖的，这意味着它们会根据输入数据的不同而变化。这种设计使得模型能够更加灵活地捕捉不同通道之间的复杂关系。

2. **多层感知机**：GDD-MLP模块使用多层感知机来学习跨通道的依赖性。这包括对每个通道的历史序列进行编码，并使用这些编码来预测未来的值。

3. **池化和混合**：在应用MLP之前，GDD-MLP模块使用平均池化和最大池化来捕获每个通道的全局特征。这些特征被用来指导跨通道的混合过程。



<font size=5>通道混合策略（Channel Mixup）</font>

通道混合策略是一种<font color=red>数据增强技术，用于提高模型的泛化能力</font>。它通过在训练期间线性组合不同的通道来创建虚拟通道，这些虚拟通道融合了多个通道的特征，同时保留了它们共享的时间依赖性。以下是通道混合策略的主要特点：

1. **线性组合**：在训练期间，通道混合策略通过线性组合不同通道的数据来创建新的虚拟通道。这种组合是通过随机选择两个通道并应用线性插值来实现的。

2. **增强泛化能力**：通过引入虚拟通道，通道混合策略增加了模型在训练期间看到的样本多样性。这有助于模型学习到更加鲁棒的特征表示，从而提高其在未知数据上的泛化能力。

3. **减轻过拟合**：通道混合策略还可以减轻过拟合的问题，因为它通过引入新的虚拟样本来打破模型对特定通道组合的过度依赖。

这些组件共同构成了CMamba模型，使其能够有效地处理多变量时间序列数据，并捕获必要的时间依赖性和跨通道依赖性，以实现准确的预测。



 ### 7、位置编码

- 位置编码（Positional Encoding）是一种用于为==输入数据中的序列元素==（如单词或图像的顺序特征）编码位置信息的技术，尤其是在Transformer模型中非常关键。<font color=red>由于Transformer模型的自注意力机制不具有天然的序列信息（它是并行计算的，不能捕捉数据的顺序），因此需要将位置编码加入到输入数据中，以提供序列位置信息</font>。



**为什么需要位置编码**

- 在传统的RNN和LSTM等循环神经网络（RNN）中，==序列信息是通过逐步递归传递的==，但在Transformer中，输入序列是并行处理的，没有顺序。因此，为了让模型区分序列中元素的相对位置（如哪个单词在句子的前面，哪个在后面），需要引入位置编码。



位置编码向模型提供了每个位置在序列中的位置信息，这样即使在没有递归的情况下，模型也能感知输入的顺序。

**位置编码的常用方法**



- 在Transformer中，常用的是基于正弦和余弦函数的**固定位置编码**，此外，还有**可学习的位置编码**。





### 8、Mamba 并行算法

**Mamba 的并行扫描**是并行算法中用于==快速计算前缀和（prefix sum）==的一种方法。该方法被广泛应用在图形处理单元（GPU）等需要并行计算的环境中，尤其是在涉及大量数据时，能极大地加速计算。<font color=red>并行扫描通过将输入数据划分为多个部分（block）并在每个部分上进行独立计算，达到减少运算时间的效果</font>。



**什么是前缀和（Prefix Sum）？**



前缀和是一种从左到右累计和的操作。对于给定的数组A=[a1,a2,a3……a4] ，前缀和数组 定义为：



**Mamba 并行扫描的流程**



在 Mamba 的并行扫描中，数据处理被分为两步：**分块扫描（Block Scan）和跨块累加（Block Aggregate）**。



​	1.	**分块扫描（Block Scan）**：

​	•	首先，<font color=blue>将输入数据数组划分为大小相等的多个块（block）。在 GPU 上，每个线程块独立处理一个数据块</font>。

​	•	对于每个块，使用==串行或小规模的并行扫描==计算块内的前缀和。

​	•	这样可以得到每个块的最后一个元素，即该块的整体和。这个和记录在一个新的数组中，用于后续的累加计算。

​	2.	**跨块累加（Block Aggregate）**：

​	•	接下来，将每个块的整体和累加起来，==形成块和的前缀和==。这样，每个块的前缀和可以作为该块的偏移量。

​	•	使用这些偏移量，将累加值添加到每个块的元素上。这样可以得到全局的前缀和。

​	3.	**最后的加和操作**：

​	•	将跨块累加得到的偏移量加回到每个块的元素上，从而完成整个数组的前缀和计算。



**并行扫描的优点**



​	•	**高效性**：Mamba 并行扫描可以利用多线程并行地处理不同块的数据，在 GPU 上可以极大地加速前缀和的计算。

​	•	**扩展性**：通过分块和并行计算，算法可以处理更大规模的数据集，适合高维度数据和海量数据的场景。

​	•	**可移植性**：该方法适用于大多数支持并行计算的硬件平台，尤其是在 GPU 上具有显著的加速效果。



**应用场景**



Mamba 并行扫描广泛应用于需要高效前缀和计算的任务中，如并行排序、图像处理、并行累积和分布式计算等。



### 9、Mamba 与 Transformer





**Mamba**和**Transformer**都是深度学习领域的重要模型，尽管它们在计算上可能有==相似的并行加速技术==，但它们的用途、结构和设计目的存在显著的区别。以下将详细分析它们的联系和区别。



**Mamba 与 Transformer 的背景与用途**



​	•	**Mamba**：主要用于高效的并行计算，<font color=red>例如并行前缀和（parallel prefix sum），以加速大规模数据的处理</font>。在图形处理单元（GPU）中，Mamba 提供一种非常高效的并行计算方法。这种方法在如图像处理、排序等需要快速数据操作的领域非常重要。

​	•	**Transformer**：是一种深度神经网络模型，最早由 Vaswani 等人在 2017 年提出，用于自然语言处理（NLP）任务中的==序列到序列学习任务==。Transformer 模型的核心是基于**自注意力机制（self-attention）**，它能够高效处理序列中的全局依赖关系，从而在翻译、生成等任务中取得了极大成功。



**结构与原理上的差异**



​	•	**Mamba 的结构**：主要包含并行处理数据的操作，<font color=blue>例如并行扫描、分块和跨块累加</font>。这些结构设计旨在利用 GPU 的并行性，使大数据运算能快速执行。Mamba 本身并不是一个神经网络模型，而是一种用于加速数据操作的算法。

​	•	**Transformer 的结构**：由**编码器（encoder）和解码器（decoder）组成，每个编码器和解码器层包含多个子层（自注意力层、多头注意力、前馈层等）。核心的自注意力机制**允许 Transformer 在处理序列时能够并行计算，并利用输入序列的全局信息来更新每个词的位置向量，从而高效建模序列依赖性。



**计算方法上的相似性：并行性**



​	•	**并行计算**：Transformer 和 Mamba 都通过并行计算提高效率。在 Mamba 中，前缀和等操作通过并行分块和累加实现。在 Transformer 中，==自注意力机制使序列的每一个位置可以在同一层次上同时与其他位置交互==，因此可以并行处理序列中的所有位置，这种并行性使 Transformer 比传统的循环神经网络（RNN）更高效。

​	•	**矩阵运算加速**：Transformer 中大量使用矩阵运算（如向量点积、矩阵乘法等）来实现注意力权重的计算；这些计算在 GPU 上也有很好的并行化加速。而 Mamba 提供的并行技术则可以用于加速这些矩阵运算。



**具体功能的差异**



​	•	**功能用途**：Mamba 侧重于加速低级别的并行计算，==主要用于提升基础计算速度。Transformer 则是一个深度学习模型，用于高层次的任务，如语言理解、机器翻译和生成等==。

​	•	**任务输出**：Mamba 的输出是加速后的计算结果，<font color=red>比如快速的前缀和数组</font>。Transformer 的输出是根据输入文本或图像生成的新的序列或分类结果，如生成的翻译文本或预测的下一句。



**设计目的的不同**



​	•	**Mamba**：<font color=blue>主要是为了高效的数据操作和并行运算。它的设计目的不在于理解数据的内容，而是通过并行化提高计算速度</font>。

​	•	**Transformer**：设计目的是理解并生成自然语言、图像等数据，其核心关注的是数据之间的关联和上下文关系。这种设计帮助 Transformer 在序列学习、生成任务中取得成功。



**总结**



**Mamba 和 Transformer 的区别**主要在于：Mamba 是一种用于高效并行计算的算法，**适用于加速大规模数据的计算**；而 Transformer 是一个基于自注意力机制的深度学习模型，主要用于处理复杂序列数据。两者在并行计算方面有相似性，但设计目标、用途和结构完全不同。



### 10、时间序列



**时间序列**是一组按时间顺序排列的数据点，==通常用于分析随时间变化的现象==。时间序列数据的核心特点是，每一个观测值都与时间点一一对应，从而使得时间成为一个关键的影响因素。这类数据广泛应用于金融、气象、经济、制造、市场分析等领域。



**时间序列的基本概念**



​	1.	**时间点**：==时间序列中的每个数据点都对应一个具体的时间==，通常是均匀间隔（如每日、每月、每年），但也可能是不规则间隔。

​	2.	**观测值**：在每个时间点上记录的数值，比如股票价格、温度、销售额等，这些数值反映了该时刻的状态或事件。

​	3.	**时间间隔**：观测值之间的时间跨度，通常是固定的。常见的时间间隔有日、周、月、季、年等。



**时间序列的特征**



时间序列通常会包含以下几种典型的模式：



​	1.	**趋势（Trend）**：长期的上升或下降趋势，通常是由宏观经济、技术进步、政策变化等因素导致的。趋势可以是线性的，也可以是非线性的。

​	2.	**季节性（Seasonality）**：时间序列中呈现的周期性波动，通常与一年内的季节、月份、季度相关。例如，电力需求在夏季和冬季会有峰值，这就是季节性。

​	3.	**周期性（Cyclicality）**：相对于季节性周期，周期性变化指的是周期不固定的波动，通常与经济周期等较大时间尺度相关。

​	4.	**随机波动（Noise）**：时间序列中的随机噪声成分，通常是不规则且不可预测的波动。



<font color=red>**时间序列的建模方法**</font>



对时间序列进行建模主要有以下几种常见方法：



​	1.	**平滑方法**：用于减少数据的波动，从而更容易观察到趋势。常见的平滑方法包括简单移动平均、指数平滑等。

​	2.	**自回归（AR）模型**：自回归模型假设当前时刻的数据与前几期的数据存在线性关系，可以预测未来的值。自回归模型的核心是建立数据与其历史值之间的关系。

其中 是自回归系数， 是随机误差项。

​	3.	**移动平均（MA）模型**：移动平均模型通过历史随机扰动的线性组合预测当前值。与自回归模型不同的是，它的模型由误差项构成。



​	4.	**ARMA 和 ARIMA 模型**：这类模型综合了自回归和移动平均的特点，适合平稳的时间序列数据。ARIMA 模型对非平稳序列进行差分处理，使其变得平稳后，再套用 ARMA 模型。

​	5.	**SARIMA 模型**：在 ARIMA 模型基础上加入了季节性成分，适用于存在季节性波动的时间序列数据。

​	6.	**长短期记忆网络（LSTM）**：在深度学习中，LSTM 网络是一种处理时间序列的先进方法，尤其在预测长序列或具有复杂模式的时间序列中表现良好。



**时间序列的平稳性**



平稳性是指时间序列的均值和方差在整个时间段内保持不变，且序列的协方差仅取决于时间差异而非具体时间。平稳性对很多时间序列模型的建立非常重要，因为许多模型假设数据是平稳的。若数据不平稳，通常会通过差分等方式使其平稳化。



**时间序列分析的步骤**



​	1.	**数据预处理**：包括处理缺失值、去除异常点、归一化等操作。

​	2.	**探索性分析**：通过可视化或统计方法分析数据的趋势、季节性和周期性等特征。通常用时序图、滚动均值图和滚动标准差图来识别趋势和季节性。

​	3.	**建模与验证**：根据数据特征选择合适的模型，使用训练数据拟合模型，并通过验证集来评估模型性能。常用评估指标包括均方误差（MSE）、平均绝对误差（MAE）等。

​	4.	**预测与解释**：在验证通过后使用模型对未来时间点进行预测，并解释预测结果和模型参数。





### 11、变量时间预测



**变量时间序列预测**是指对多个互相关联的时间序列进行建模和预测的过程。不同于单变量时间序列预测（仅预测单个时间序列），变量时间序列包含多个同时变化的变量，并且这些变量之间可能存在复杂的依赖关系。因此，这种预测方法更适合应用于金融市场、经济分析、能源消耗、流行病学等领域中多个变量的关联预测。



**变量时间序列的建模方法**



​	1.	**向量自回归模型（VAR）**

​	•	VAR 是一种扩展的自回归模型，适用于多个时间序列变量的建模。它通过包含多个自回归方程来预测每个变量，将其他变量的滞后项作为自变量，允许在不同时刻的变量之间存在互相影响。

​	•	**方程形式**：



其中， 是变量向量， 是滞后矩阵， 为误差项。

​	•	**优点**：能够捕捉变量之间的相互影响关系，适合平稳的多变量时间序列。

​	•	**缺点**：需要数据平稳性，如果序列存在非平稳性或异方差性，则预测结果可能不可靠。

​	2.	**向量自回归移动平均模型（VARMA）**

​	•	VARMA 模型结合了 VAR 和移动平均模型（MA）的优点，能够更好地捕捉短期和长期依赖关系。通常适用于更加复杂的多变量时间序列。

​	•	**方程形式**：



​	•	**优点**：在数据特征上更加灵活，能够建模不同变量之间的动态变化。

​	•	**缺点**：相较 VAR，复杂度更高，参数较多，模型拟合困难。

​	3.	**向量自回归积分移动平均模型（VARIMA）**

​	•	VARIMA 是将差分操作引入 VARMA 模型中，以解决非平稳序列的问题。适用于具有长期趋势或季节性的多变量时间序列。

​	•	**优点**：在数据具有非平稳性时效果更好。

​	4.	**动态因子模型（DFM）**

​	•	DFM 通过假设多个时间序列受少量潜在的“因子”影响，来减少参数和建模复杂度。每个观测变量可以表示为因子的线性组合，同时加上误差项。

​	•	**优点**：非常适合高维数据（变量数量较多）场景，模型复杂度低。

​	•	**缺点**：对因子的选择和估计较难，对模型假设较敏感。

​	5.	**长短期记忆网络（LSTM）**

​	•	LSTM 是一种递归神经网络（RNN），它通过记忆门控机制处理长序列中的依赖关系。LSTM 是非线性的，且具备良好的建模能力，适用于序列中短期与长期的依赖。

​	•	**优点**：能够自动学习序列中的复杂模式和非线性关系，适合大型数据集。

​	•	**缺点**：训练耗时，数据需求量大，难以解释。

​	6.	**变分自编码器（VAE）和生成对抗网络（GAN）**

​	•	近年来，VAE 和 GAN 被用于多变量时间序列预测。VAE 通过生成式模型来捕捉数据的潜在分布，而 GAN 则使用生成器-判别器的博弈结构来生成新的时间序列点。

​	•	**优点**：适合复杂的非线性、多变量时间序列，能够生成逼真的未来数据。

​	•	**缺点**：对数据量要求高，难以解释生成的潜在分布。



**变量时间序列预测中的关键问题**



​	1.	**变量之间的相互关系**：需要通过统计或机器学习手段，判断和建模不同变量间的相互作用，如因果关系和时滞效应。VAR、VARMA 等模型可以显式地捕捉这些关系。

​	2.	**数据平稳性**：时间序列中的趋势性和季节性变化可能导致数据非平稳性，需要在建模之前去除。通过差分或对数转换等方法来提高模型的效果。

​	3.	**数据的多维性和高维性**：随着变量数量增加，模型的复杂度显著提高，对算法的训练和计算能力提出更高要求。动态因子模型、稀疏模型等可以减少参数数量。

​	4.	**模型选择与评估**：常见的评价指标有均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）等。交叉验证在时间序列预测中的实现较复杂，通常使用滑动窗口验证法。

​	5.	**可解释性**：尤其在金融等行业中，模型结果的解释往往比准确性更重要。传统统计模型如 VAR 有较好的解释性，而深度学习模型如 LSTM 的可解释性较弱。

 



### 12、玻尔兹曼机



惠普菲尔德网络



玻尔兹曼机（Boltzmann Machine）和霍普菲尔德网络（Hopfield Network）都是==基于能量函数的神经网络模型==，<font color=red>用于模拟记忆和关联学习</font>font>，但它们在结构、功能和训练方式上有显著的区别和联系。以下是两者的主要联系与区别：



**联系**



​	1.	**能量函数**：两者都使用能量函数来定义系统的状态，网络的稳定状态通常对应于低能量的状态。通过能量函数，两者可以寻找数据或模式之间的最优关联。

​	2.	**记忆存储**：两者都可以存储特定的模式或数据点，并在一定程度上执行记忆回忆（Memory Recall）的功能。这些模式是通过网络的权重结构编码的。

​	3.	**对称权重**：两者的连接权重都是对称的，即权重矩阵 满足 。这种对称性使得它们在理论上可以达到稳定状态。

​	4.	**自回归网络**：两者都属于自回归（recurrent）网络，即网络的输出可以反馈到网络的输入中。



**区别**



| 特点             | 霍普菲尔德网络 (Hopfield Network)                            | 玻尔兹曼机 (Boltzmann Machine)                               |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **网络结构**     | 仅有单层的完全连接神经元，没有显式的输入和输出层。           | 通常由可见层和隐藏层组成，有多层结构。                       |
| **状态更新方式** | 采用确定性更新方式，一次更新一个神经元，朝着能量最小化的方向更新，直至达到稳定状态。 | 采用随机更新方式，遵循玻尔兹曼分布，通过引入噪声来探索不同状态。 |
| **温度参数**     | 没有温度参数，确定性下降到局部最低能量状态。                 | 包含温度参数，使用模拟退火（Simulated Annealing）逐步降低温度，以提高状态探索的多样性。 |
| **训练方式**     | 不进行显式训练，通过设置权重来存储固定模式，常用赫布规则或伪逆法。 | 通过最大似然估计或对比散度等方法训练，使用随机梯度下降调整权重。 |
| **适用任务**     | 主要用于联想记忆和模式识别，通常存储少量固定模式。           | 可用于概率生成模型，适合更复杂的数据表示，常用于无监督学习和特征提取。 |
| **计算复杂度**   | 相对较低，计算相对简单，因为只需达到稳定状态。               | 计算复杂度较高，尤其在训练阶段需要大量采样和优化。           |
| **应用领域**     | 简单的模式存储和联想记忆，例如模式识别和小规模数据存储。     | 更广泛的应用领域，包括深度学习的受限玻尔兹曼机（RBM）和深度信念网络（DBN）。 |



**总结**



霍普菲尔德网络适用于存储和回忆少量固定模式，且计算简单，适合联想记忆任务。玻尔兹曼机更灵活，可以生成更复杂的概率分布，适合更复杂的无监督学习任务。玻尔兹曼机的扩展（如RBM）在深度学习中有更广泛的应用。





### 13、自制框架实现启动



这是之前直接 python3 run.py

![image-20241104192006636](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241104192006636.png)

现在的严格命令 ：   python3 run.py --task_name long_term_forecast  --is_training 1 --model_id 001 --model CMamba --data ETTh1



![image-20241104192108717](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241104192108717.png)





运行结果



 ![image-20241104193005685](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241104193005685.png)

![image-20241104192935537](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241104192935537.png)

#### Encoder 框架





![image-20241106204151121](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241106204151121.png)





Encoder 框架就是相当于之前的BaseModel 的改进版本  --> ==相当于进行特征压缩的过程==，最后也会计算 相关参数，但是计算结果不太好

所以来到 BaseModel 框架  --> 这里相当于是全局考虑



![image-20241106205538406](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241106205538406.png)







### 14、论文再阅读

>  IMPROVING DEPRESSIONESTIMATION FROM FACIAL VIDEO SWITH FACE
> ALIGNMENT TRAINING OPTIMIZATION AND SCHEDULING   https://arxiv.org/pdf/2212.06400

#### 摘要

- 深度学习模型在使用<font color=red>**基于视频的面部表情识别抑郁状态方面**</font>显示出有希望的结果。虽然成功的模型通常使用 3DCNN 或视频蒸馏技术，==但实验中对预训练、数据增强、预处理和优化技术的使用不同==，因此很难进行公平的架构比较。相反，我们建议通过使用两种特定的人脸对齐方法和改进的数据增强、优化和调度技术来增强两个基于 ResNet-50 的简单模型，它们仅使用静态空间信息。我们对基准数据集的广泛实验获得了与单个流的复杂时空模型相似的结果，而两个不同流的分数级融合优于 SOTA 方法。我们的研究结果表明，<font color=blue>预处理和训练过程中的特定修改会导致模型的性能出现明显差异</font>，并可能隐藏最初归因于使用不同神经网络架构的实际情况。 



索引词— 情感计算、抑郁检测、机器学习、表情识别。



#### 介绍

- ~~抑郁症是一种常见的心理健康疾病，会对个人的幸福感产生负面影响 [1]。长期的医学抑郁症会导致心理和生理层面的严重并发症。几项研究表明，抑郁症是其他疾病的诱因，如心血管疾病、骨质疏松症、衰老、病理认知变化、阿尔茨海默病和其他痴呆症，甚至会增加早期死亡的风险 [2]。自动识别抑郁症的系统是可取的，因为它们具有潜在的客观性、速度和可靠性，可以避免对患者的健康和福祉产生这种影响~~。在过去十年中，已经提出了许多基于经典统计机器学习算法的方法，以从面部视频、语音和文本数据中识别抑郁迹象 [3]，以帮助医生做出决策。虽然最新颖的架构在抑郁识别模型的准确性方面显示出显着的提高，==但之前的大多数工作都没有讨论或试验机器学习管道的实质性组成部分==，例如预处理或优化。基于这些缺点，在本文中，<font color=red>我们建议仅使用从面部视频帧中提取的静态纹理特征（图像中纹理信息的统计或数学特征，通常不考虑时间变化（因此称为“静态”），而是基于单帧或单张图像提取）来创建用于自动抑郁筛查的深度学习模型</font>。在这种情况下，我们建议进行一系列更改，以使用这种架构来改善结果。我们的主要贡献可以总结如下：
-  我们引入了一组基于 ResNet-50 架构 [4] 的 2D-CNN 模型，该模型通过应用两种不同的人脸对齐技术仅使用视频帧中的静态纹理信息进行训练，并评估它们对最终结果的影响。• 我们探索了新颖的训练优化和调度方案，以进一步改进以前基于空间信息的类似方法的结果。
-  ==我们建议使用融合评分方法==，使用不同的基于纹理的模型来回归抑郁水平，这些模型根据面部对齐情况显示互补。• 我们在 AVEC2013 [5] 和 AVEC2014 数据库 [6] 上训练和验证模型，表明这种方法可以获得与复杂的时空模型相当的结果，==而两种流模型的分数级融合优于文献中最先进的方法==。• 最后，我们发现，仅使用基于纹理的模型表明，<font color=blue>预处理和训练过程中的微小变化可能会导致模型性能的明显差异，这可能会隐藏归因于神经网络架构差异的真实贡献</font>。



并将图像分为抑郁类或对照类

#### 预处理 RGB 图像



- 以学习判别表示（discriminative representation），通常需要将图像数据进行一系列处理，以便模型能够更有效地学习到区分不同类别的特征。==这些预处理步骤有助于标准化图像数据、增强图像质量、减少噪声，并让模型更好地泛化==。以下是常用的图像预处理方法和步骤：



**1. 图像归一化（Normalization）**



将图像像素值缩放到特定范围（通常是 [0, 1] 或 [-1, 1]），有助于加快模型的收敛速度。归一化常见的方法有两种：



​	•	**简单归一化**：将像素值从 [0, 255] 缩放到 [0, 1]。例如：



​	•	**均值-标准差归一化**：将图像像素值减去数据集的均值，并除以标准差，以标准化图像的分布。常用在预训练模型（如 ResNet、VGG 等）上，因为这些模型通常对数据的均值和标准差有特定的要求。例如，对于 ImageNet 数据集，常用的均值和标准差为：



其中，mean = [0.485, 0.456, 0.406]，std = [0.229, 0.224, 0.225]。



**2. 尺度调整（Resizing）**



将所有图像调整为相同的尺寸，以适应模型输入要求。这通常涉及将图像统一到一个固定的宽度和高度（如 224x224），以保证输入一致性。例如，OpenCV、PIL 等库可以轻松实现图像的尺度调整。



**3. 数据增强（Data Augmentation）**



通过图像增强技术，增加训练样本的多样性，可以提高模型的鲁棒性，减少过拟合。常用的增强方法包括：



​	•	**旋转**：将图像旋转一定角度（如随机选择 -30° 到 30°）。

​	•	**翻转**：随机水平或垂直翻转图像。

​	•	**裁剪**：随机裁剪图像的一部分或进行中心裁剪。

​	•	**缩放**：改变图像的缩放比例。

​	•	**颜色抖动**：随机调整亮度、对比度、饱和度等。

​	•	**噪声添加**：为图像添加随机噪声，提高模型对噪声的鲁棒性。



这些数据增强技术可以帮助模型更好地泛化，并提高模型对各种可能的变形和扰动的适应能力。



**4. 边缘检测或纹理提取**



对于某些需要额外判别力的任务，可以预处理图像以突出纹理或边缘特征。例如：



​	•	**Sobel 边缘检测**：使用 Sobel 算子提取图像的边缘信息，有助于强化模型对轮廓和形状的识别能力。

​	•	**高斯模糊**：可以模糊掉不必要的细节，使得模型更关注主要特征。

​	•	**Gabor 滤波器**：可用于提取方向性纹理特征，有助于模型学习区分性更强的特征。



**5. PCA 或颜色归一化（Color Normalization）**



在数据集中，图像的颜色分布可能不一致。颜色归一化或主成分分析 (PCA) 颜色增强可以减少颜色差异的影响，特别适用于分类任务。



​	•	**PCA 颜色增强**：通过对每个 RGB 通道进行主成分分析，可以稍微调整颜色，使图像在色彩上更加一致。

​	•	**颜色归一化**：将颜色信息进行标准化，使得图像的颜色特征更加稳定。



**6. 转换到特定颜色空间**



虽然 RGB 是最常用的颜色空间，但在某些情况下，转换到其他颜色空间（如 HSV、YUV 或 Lab）可以更好地分离颜色和亮度信息，有助于模型学习更有效的表示。特定颜色空间能强调不同的图像特征，从而在某些任务中取得更好的效果。



**7. 特征提取**



如果你的任务中不需要端到端训练，也可以考虑使用预训练模型（如 ResNet、VGG、MobileNet 等）提取深度特征，将这些高层特征作为判别性特征。这些模型通常在大规模数据集上训练过，已经学到了有效的判别特征。提取到的特征可以作为输入，用于进一步的分类或回归任务。



**8. 直方图均衡化（Histogram Equalization）**



直方图均衡化可以增强图像的对比度，尤其是灰度图像中常用。这对于增强图像的细节信息，或者在图像的亮度差异很大时，可以显著提高模型的区分能力。



**小结**



以下是用于学习判别表示的常用 RGB 图像预处理方法的总结：



| 预处理步骤            | 目的和效果                                 |
| --------------------- | ------------------------------------------ |
| 归一化                | 缩放像素值，使模型更易于训练               |
| 尺度调整              | 统一图像尺寸，符合模型输入要求             |
| 数据增强              | 增加数据多样性，减少过拟合                 |
| 边缘检测与纹理提取    | 提取轮廓和纹理，帮助模型学习判别特征       |
| 颜色归一化与 PCA 增强 | 减少颜色差异的影响，使模型关注更有效的特征 |
| 转换颜色空间          | 提取特定颜色空间的特征，例如亮度或饱和度   |
| 特征提取              | 使用预训练模型提取深度特征，减少计算复杂度 |
| 直方图均衡化          | 增强对比度，特别适合亮度不均的图像         |

 



#### 层解冻



**层解冻**（Layer Unfreezing）是深度学习中一种模型训练技巧，主要用于迁移学习。当我们将预训练模型（如 ResNet、VGG 等）应用于新的任务时，通常会“冻结”模型的一部分层（即设置它们的参数不可更新），并仅训练其他层。层解冻则是逐步解冻冻结的层，使得这些层的参数可以进行更新，从而更好地适应新任务。



**层解冻的动机和目的**



在迁移学习中，<font color=red>预训练模型的早期层通常学习到了较为通用的特征（如边缘和纹理），这些特征在大多数视觉任务中都适用。而模型的后几层则捕捉了更特定于预训练任务的高层语义信息</font>。因此：



​	•	**初始冻结**可以保留预训练模型的通用特征，避免在训练数据不足的情况下过度拟合或丢失通用特征。

​	•	**逐步解冻**==允许模型在微调过程中逐步调整较低层的特征，来适应新任务的需求，从而获得更好的泛化能力==。



#### 回归层连续预测

在使用 ResNet-50 作为主干网络并在其顶部添加全连接（FC）层和回归层来估计抑郁水平时，通常我们会设计一个能够==输出连续值的回归层==。具体而言，模型的结构如下：



**1. ResNet-50 主干网络**



​	•	**ResNet-50** 是一种深度卷积神经网络，由 50 层组成，通过残差模块（residual blocks）来学习图像特征。

​	•	作为主干网络，ResNet-50 提取图像的高级特征，例如边缘、纹理、形状等。

​	•	在这里，ResNet-50 的输出是一组高维特征向量，这些特征会作为后续回归层的输入，以便估计抑郁水平。



**2. 两层全连接层（512 个神经元）**



​	•	经过 ResNet-50 提取的特征向量首先通过两层 512 个神经元的全连接层，分别记为 FC1 和 FC2。

​	•	**激活函数**：可以使用 ReLU 激活函数，让网络学习到非线性特征表示。

​	•	**Dropout**：可以在全连接层后使用 Dropout 层（例如 0.5 的概率）来防止过拟合，特别是在数据量有限的情况下。

​	•	这两层的作用是逐步降低特征向量的维度，同时学习到判别力较强的特征，以便更好地拟合回归层。



**3. 回归层（128 个神经元 + 输出层）**



​	•	最后一层由 128 个神经元组成，是回归层的核心部分。

​	•	**激活函数**：这层可以继续使用 ReLU 激活函数，但在输出层一般会使用线性激活，以便预测连续的抑郁水平。

​	•	128 个神经元可以进一步减少特征维度，使模型更简洁，从而降低模型复杂度，提升训练效率。

​	•	**输出层**：最终的输出层通常是一个单神经元的线性层，用于输出连续的抑郁得分。



**回归层的组成和工作机制**



总结下来，回归层的组成和工作机制如下：



​	1.	**FC1**（512 个神经元） → **ReLU 激活** → **Dropout**

​	2.	**FC2**（512 个神经元） → **ReLU 激活** → **Dropout**

​	3.	**回归层**（128 个神经元） → **ReLU 激活**

​	4.	**输出层**（1 个神经元） → **线性激活**



这样设计的回归层能够以连续数值输出的形式估计抑郁水平。

---

**MAE**（Mean Absolute Error，平均绝对误差）和 **RMSE**（Root Mean Squared Error，均方根误差）是回归模型中常用的评价指标，用于衡量模型预测值和真实值之间的误差。一般来说，这两个指标的值越低越好，因为它们反映了预测误差的大小。

---

- <font color=red size=5>抑郁水平介于 0 到 63 之间。根据 BD-II 评分，抑郁症的严重程度可分为四个级别：轻度 （0-13）、轻度 （14-19）、中度 （20-28） 和重度 （29-63）</font>





**Separated tasks** 和 **Joint tasks** 是多任务学习（Multitask Learning）中的两种任务设计策略，用于处理多个相关任务。它们的主要区别在于模型在处理任务时是分离的还是联合的。



**1. Separated Tasks（分离任务）**



​	•	**定义**：在分离任务中，每个任务都有自己独立的模型或参数，任务之间没有共享部分，彼此完全分离。

​	•	**训练方式**：每个任务都是独立训练的，互相不影响。这样可以让每个模型专注于特定任务的特征和目标，而不受其他任务的影响。

​	•	**优点**：适用于差异性较大的任务，模型不会因为其他任务的干扰而偏离特定任务的最佳参数。

​	•	**缺点**：每个任务都需要独立的计算资源，不能利用任务之间的关联特征；并且当任务数量增多时，计算开销会显著增加。



**例子**：如果我们要对图片进行两种不同的分类任务（如物体分类和风格分类），使用完全不同的模型来分别处理这两个任务就是分离任务。



**2. Joint Tasks（联合任务）**



​	•	**定义**：在联合任务中，多个任务共享部分模型或参数，它们在同一个模型框架下联合训练。任务之间会共享信息，相互影响，共同优化。

​	•	**训练方式**：模型通常会有共享的底层特征提取层，再为每个任务添加特定的顶层（如全连接层）。整个模型通过联合损失函数（例如，将每个任务的损失加权求和）进行优化。

​	•	**优点**：联合任务可以利用多个任务之间的相关性，特别是当任务有重叠特征或目标时，联合学习可以提升模型的泛化能力，减少过拟合。

​	•	**缺点**：当任务差异较大时，共享部分可能会导致任务之间的冲突，影响各自的性能；而且需要对损失进行合理加权，以避免一个任务的损失主导整个模型。



**例子**：一个常见的联合任务应用是情感分析和主题分类联合模型。对于同一个句子，模型可以同时预测其情感（正面或负面）和主题（如体育、政治等）。通过共享句子的特征表示，模型能够有效利用情感和主题之间的关联。



**总结**



​	•	**Separated Tasks**：任务独立，适合差异较大的任务，减少互相干扰。

​	•	**Joint Tasks**：任务联合，适合相关性较强的任务，利用共享特征提升性能。



### 15、抑郁症综述再阅读

> 梦开始的地方



设计一个有代表性的特征并提取它来估计抑郁症的严重程度是 ADE 深度学习架构中的重要一步。==ADE 功能可以是手工制作的，也可以基于深度学习模型==。广泛使用的手工制作特征的例子包括局部<font color=red>二进制模式</font>（LBP）[18]、来自三个正交平面的局部相位量化（LPQ-TOP）[19]、来自三个正交平面的局部二进制模式（LBP-TOP）[20]和其他（例如，面部动作单元（FAU）、地标、头部姿势、凝视）[21]。自 2013 年以来，音视觉情感识别挑战赛 （AVEC2013） [22] 和 AVEC2014 [23] 等抑郁识别挑战赛通过人机交互记录抑郁数据。手工制作的特征被认为可以为 ADE 带来良好的性能。但是，它们受到以下限制。首先，提取手工制作的特征需要大量的努力（例如，领域知识、时间和劳动力等）。例如，LBP-TOP 被广泛用于情感识别 ADE。然而，如果我们开发了类似于 LBP-TOP 的手工制作特征，我们将不得不拥有针对特定任务的抑郁症知识，而获得这些知识是消耗劳动力的。*其次，无法很好地提取视听信号中隐含的一些区分模式。最后，开发特征的动机来自研究人员的主观假设。*不同的研究人员从不同的角度设计特征。

幸运的是，深度学习的快速发展推动了深度学习 （DL） 方法的研究，用于抑郁症识别，并且没有遭受上述挑战，与手工制作的功能相比，它获得了有希望的性能。对于通过深度学习学习的特征，广泛的研究采用了深度卷积神经网络（DCNNs）来提取基于视听线索的ADE的多尺度特征表示[24–35]。图 1 显示了根据方法和数据库的 ADE 的这种演变。



#### 手工制作特征

在表情识别和情感分析等计算机视觉任务中，**手工制作特征**被广泛使用。以下是一些常见的特征：



​	1.	**局部二进制模式（Local Binary Pattern, LBP）**

LBP 是一种描述图像纹理的特征。它通过在图像中的每个像素周围创建二进制模式来捕获局部纹理特征。具体来说，LBP将中心像素与其周围像素进行比较，标记大于中心像素的邻居为1，小于的标记为0，形成一个二进制数。这种方法广泛用于面部图像分析中，例如检测纹理变化来识别面部表情。

​	2.	**来自三个正交平面的局部相位量化（Local Phase Quantization from Three Orthogonal Planes, LPQ-TOP）**

LPQ-TOP 是一种从视频序列中提取纹理信息的特征提取方法。它结合了局部相位量化和三正交平面的概念，能够捕获视频中不同时序方向上的局部相位信息，用于表情变化和运动模式分析。由于其对噪声的鲁棒性，这种特征在表情和姿态识别任务中得到了广泛应用。

​	3.	**来自三个正交平面的局部二进制模式（Local Binary Pattern from Three Orthogonal Planes, LBP-TOP）**

LBP-TOP 是 LBP 的一种时序扩展，<font color=red>用于从视频序列中提取空间-时间特征。LBP-TOP在三个正交平面（XY、XT、YT）上应用LBP，以捕获图像随时间变化的特征。这种方法不仅能捕获面部的空间纹理信息，还能捕获随时间变化的动态特征，广泛用于动态表情识别中</font>。

​	4.	**面部动作单元（Facial Action Unit, FAU）**

FAU 是基于面部动作编码系统（FACS）的一种描述面部表情的方法。FACS将面部肌肉动作分解为单独的动作单元，每个单元表示一个特定的肌肉运动。例如，眉毛上扬、嘴角拉起等动作对应不同的FAU编码。FAU可以用于识别复杂的面部表情。

​	5.	**地标（Landmarks）**

地标是一组关键的面部特征点，例如眼睛、鼻子、嘴巴的轮廓点等。通过检测和跟踪这些点的位置和形状变化，算法能够分析面部表情和姿态。

​	6.	**头部姿势（Head Pose）**

头部姿势描述了头部的方向和旋转情况，例如偏转、俯仰和滚动角度。头部姿势变化对于理解面部表情、注意力状态等信息具有重要作用。

​	7.	**凝视（Gaze）**

凝视指的是眼睛注视方向。通过分析凝视方向，系统可以推测观察者的注意力焦点和兴趣点，用于情感分析和行为理解。





近年来，已经发表了一些基于音频 [36] 和视觉线索 [37] 的关于抑郁识别和分析的详尽综述。这些调查为 ADE 提供了一个全面的范围。此外，根据 2010 年至 2017 年的研究发表了一篇关于 DL 情感识别的综述 [38]。然而，仍有两个方面有待探索。==由于现有的综述仅关注用于估计抑郁严重程度的音频或视觉线索，因此尚未充分讨论**视听线索的联合使用**==。此外，现有的调查只考虑了传统方法，他们的分析中尚未涵盖 DL 技术。近年来，DL 技术加快了基于视听线索的抑郁识别的发展和创新。据我们所知，仍然缺乏对抑郁症识别的多模态视听方法的深入回顾。我们的目标是通过包含越来越重要的基于视听线索的深度多模态 ADE 方法来填补现有广泛审查中的空白。







总之，我们对情感计算领域的贡献是：

（1） 我们提出了一项基于视听线索的 DL for ADE 的抑郁症相关综合调查。

（2） 我们对 1994 年至 2021 年最重要的 20 个数据库进行了详细回顾。

（3） 我们首先从音频和视觉线索中回顾了 114 项 ADE 研究，并选择了 78 项采用 DL 方法进行 ADE 的研究，并将它们分为 （1） 用于音频模态的深度 ADE 网络;（2） 用于视频模态的深度 ADE 网络（空间特征提取、时间特征提取）;（3） 用于视听提示的深度 ADE 网络。



（4） 我们指出未解决的问题和有希望的方向。 

1.2. 本文的结构在本文中，我们全面回顾了基于深度神经网络的自动抑郁检测方法，讨论了它们面临的挑战，并指出了未来的研究方向。

在下文中，

第 2 节提供了抑郁症的定义并描述了抑郁症评估的客观标志物。

第 3 节介绍了几**个多模态抑郁数据库**。

第 4 节详细回顾了一般的深度 ADE 方法，并介绍了几种基于视听线索的新型神经网络架构。

第 5 节 中介绍了其他问题。

第 6 节根据我们的论述提供了结论。此外，为了清楚起见，本文的结构如图 2 所示。







![image-20241105161101402](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241105161101402.png)







1980 年，Russell [39] 提出情绪状态可以表示为二维空间中的连续数字向量，称为**价-唤醒 （VA） 空间**，见图 3。

==效价维度是指两种类型的情绪状态==，即积极和消极。唤醒维度代表情绪的强度，从困倦（或无聊）到高度兴奋。如图 3 所示，抑郁位于 VA 空间的第三象限 [39]，对应于低唤醒和负效价。根据美国精神病学协会（APA）[DSM]《精神障碍诊断与统计手册》（DSM）的定义[41]，抑郁症可进一步分为以下几类：重度抑郁症（MDD）、持续性抑郁症（心境恶劣）、破坏性情绪失调障碍（DMDD）、经前焦虑障碍（PDD）、物质/药物诱导的抑郁障碍（S/M-IDD）、其他疾病引起的抑郁症（DDDAMC）、 其他特定抑郁症 （OSDD） 或未特定抑郁症 （UDD）。DSM 提供了根据观察到的症状对精神障碍进行分类的一般标准。当一个人至少出现以下两种症状之一时，（1） 一天中大部分时间情绪低落和/或 （2） 兴趣或愉悦感明显减弱，并伴有表 1 中至少四种或四种以上的症状，这些症状已经持续了至少两周。此外，预计上述症状也会导致社会、职业或其他重要功能领域的临床显着痛苦或损害。尽管如此，在某种程度上，这些不同类型的抑郁症相关疾病以相似的方式表现出来。如何诊断抑郁症的问题引起了来自不同领域的许多研究人员的关注。然而，<font color=red>对抑郁症发病机制的理解尚未统一和达成一致</font>。然而，其发病机制通常被认为与皮质边缘系统功能障碍有关，从而降低了其活动和连接性[42–45]。据信，抑郁取决于遗传易感性与环境因素之间的相互作用[46,47]。在 [48] 中，发现由于遗传易感性的影响，患有抑郁症的猴子可能会失去它们的母亲。在 [49] 中，Remi 等人发现，对于男性来说，由于环境因素的影响，收养家庭中饮酒过多会增加患抑郁症的风险。对于女性来说，收养父母在被收养人 19 岁之前去世，或者收养家庭中存在患有行为障碍的人，都会增加患抑郁症的风险。DSM 经常受到批评，因为精神疾病之间的界限并不总是被正确定义。这导致了在主观偏见中 [50–55]。抑郁症至少有 1497 种不同的特征 [56]。在相同诊断的某些病例中，两名抑郁受试者可能没有任何相同的症状 [57]。通常认为 MDD 可以被称为临床抑郁症。 







- 例如，在 AVEC2013 和 AVEC2014 比赛中，ADE 任务可以被视为一个回归问题，目的是估计抑郁的水平，<font color=red>即每个音频和视频的 BDI-II</font>。在AVEC2016 [123] 中，第一个目标是估计 PHQ-8 分数，这可以被认为是一个回归问题，而第二个目标是将受试者分类为抑郁或不抑郁，这是一个分类问题。AVEC2017 [138] 的任务也是估计 PHQ-8 分数，这可以被视为一个回归问题。



本节介绍了 ADE 中采用的常用程序，即==预处理、深度特征提取和分类/回归==。在下文中，文献分为三组：

（1） 用于音频模态的深度 ADE 网络;

（2） 用于静态图像的深度 ADE 网络

（3） 用于图像序列的深度 ADE 网络。此外，还为上述组介绍了不同的网络类型以及讨论。自 2013 年以来，深度学习方法受到了计算机视觉社区的高度关注。本文的目的是 （i） <font color=red>从视听线索中回顾最近与抑郁症相关的工作信息</font>，以及 （ii） 综合将 DL 应用于抑郁症评估的关键要点。为了实现这些目标，自 2013 年以来，我们进行了两阶段文献检索（IEEE Xplore、Springer Link、Web of Science 和 ACM 数字图书馆）。我们检索的第一阶段产生了 480 项研究。在第 2 阶段，我们将检索限制在采用音频和视觉线索来识别抑郁症的研究。之后，我们按浅层和 DL 方法对研究进行手动排序，获得 114 篇与抑郁症相关的研究。然后，我们关注了本综述中考虑的 78 项采用 DL 治疗 ADE 的研究。为了本综述的目的，这些选定的研究在下一节中按基于 DL 的 ADE 模式（音频和视频）进一步分类。图 5 显示了 2013 年至 2021 年的研究出版物数量。基于图 5，可以进行以下观察：（1） 从 2013 年到 2021 年，相关出版物迅速增加，表明==基于视听线索的 ADE 是一个快速增长的领域==。特别是在 2013 年和 2014 年，抑郁症子挑战提供了从视听线索评估严重程度的动力。然后，在 2016 年和 2017 年与抑郁症相关的挑战中，几项研究试图使用 DL 从另一个角度分析抑郁症，从而发表了一些新的研究。（2） 自 2017 年以来，ADE 的 DL 受到了极大的关注。2017 年和 2018 年兴趣的急剧上升可归因于 2016 年的 [24] 和 2017 年的 [28]。









- 在本节中，为了帮助读者了解 ADE 的最新技术，我们简要回顾了计算机视觉领域采用的突出 DNN 架构，例如 DCNN、递归神经网络 （RNN）、卷积 3D （C3D）、长短期记忆 （LSTM）、编码器解码器和自动编码器架构、生成对抗网络 （GAN） 和其他新变体。



- 到目前为止，CNN 已成为 DL 领域最流行的架构。CNN 最初是由 Fukushima [174] 基于“Neocognitron”提出的，其动机是 Hubel 和 Wiesel 的视觉皮层分层感受野。之后，LeCun et al. [173] 设计了一种文档识别架构。为了对 CNN 进行清晰的解释，我们以 LeNet-5 为例。LeNet-5 包含三种类型的层：卷积层、池化层和全连接层。卷积层的目标是对来自输入的特征表示进行建模。如图 6 所示，可以看到卷积层包含一系列用于计算特征图的卷积内核。池化层用于降低特征图的空间分辨率。具体来说，一些统计



![image-20241105195338179](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241105195338179.png)



 2008年，Moore等[191]研究了各种特征的组合，例如韵律、语音质量、频谱和声门。他们在对抑郁症的不存在/存在进行分类方面获得了相当的性能 [191]。许多 LLD 指标 （例如，韵律、源、共振峰和频谱） 已被确定为对抑郁症的有效识别。有关基于语音的抑郁症识别的深入回顾，请参阅 [36]。正如这篇综述所显示的，==手工制作的特征在识别抑郁症方面取得了有希望的表现==。然而，仍然存在一些问题：例如，手工工作和专业知识对于特征选择仍然很重要，这浪费了劳动力资源。此外，与在多个学科中手工制作相比，通过 DL 学习的表示表现出出色的性能，ADE 也不例外。在下文中，我们按 2016 年至 2021 年的时间线描述了基于 DL 的方法。2016 年，[24] 提出了一种基于深度学习的新模型 DepAudioNet，用于从声音线索中挖掘抑郁表示，<font color=red>采用 LSTM 和 1D-CNN 编码用于抑郁识别的判别性音频表示（见图 11）。1D-CNN 可以从原始波形中对空间特征表示进行建模，而 LSTM 可以从梅尔尺度滤波器组中学习短期和长期特征表示</font>[192]。此外，为了平衡正负样本，在使用 LSTM 之前，在模型训练阶段采用了**随机采样方法**。使用 DepAudioNet，可以提取不同的尺度表示，即高级、短期和长期特征。为了进一步解释健康对照组和抑郁受试者之间的不同表现形式，<font color=blue>图 12 提供了从音频片段中提取的频谱图和滤波器组特征的比较。目标是尝试使用深度学习方法来估计抑郁症的严重程度。最重要的是，尽管训练数据量小，但深度学习方法还可以从音频信号中学习判别模式。尽管所使用的抑郁症数据库只有有限数量的样本，但基于深度学习的抑郁症识别方法已经引起了众多研究人员的高度关注</font>。



### 16、论文再叙述



> CMAMBA: CHANNEL CORRELATION ENHANCED  STATE SPACE MODELS FOR MULTIVARIATE TIME  SERIES FORECASTING





拟解决的问题？  和上次汇报的**多模态学习驱动的对比蒸馏集成方法在抑郁症识别中的应用研究**



①是什么

②现有的

③存不足

④我解决



- 我们的Encoder  是否存在这样的 不足



----



### 问题一



==主要解决的问题是多变量时间序列预测中的**跨通道依赖性捕获问题**==



<font color=red>是什么？</font>  在多变量时间序列预测中，跨通道依赖性捕获问题指的是在**预测过程中理解和建模==不同时间序列通道（或变量）之间相互关系==的能力**。在多变量时间序列数据中，**每个通道（或变量）可能与其他通道存在复杂的相互作用和影响**，这些关系对于准确预测未来值至关重要。



现有的基于Mamba的状态空间模型在处理跨通道依赖性方面存在不足，（因为原始设计没有充分考虑跨通道依赖性的建模。==Mamba模型主要关注序列的时间依赖性==，但缺乏有效捕捉和利用不同通道间相互关系的能力。这导致在多变量时间序列预测任务中，模型可能无法准确理解和预测不同通道之间的复杂相互作用，从而影响预测的准确性和鲁棒性。）这可能会影响模型在多变量时间序列预测任务中的性能。



1. **M-Mamba模块**：这是一个修改过的Mamba模块，专门用于时间依赖性建模。它通过增强Mamba模型在时间序列预测中捕获跨时间依赖性的能力。
2. **GDD-MLP模块**（Global Data-Dependent MLP）：这是一个全局数据依赖的多层感知机（MLP），用于有效捕获跨通道依赖性。它通过引入数据依赖性和全局感受野，改善了原始MLP在捕获跨通道依赖性方面的不足。

- GDD-MLP模块通过在每个通道上应用数据依赖的权重和偏置，使得模型能够自适应地捕捉不同通道之间的依赖关系。
- 这种设计允许模型在训练时，根据输入数据动态调整权重，从而提高模型对新数据集的适应能力。
- 通过实例归一化和最大池化操作，GDD-MLP模块能够提取每个通道的全局特征，这有助于模型在面对具有复杂、长期依赖性的时间序列数据时，更好地泛化。



1. **通道混合策略（Channel Mixup）**：这是一种数据增强技术，通过在训练期间线性组合不同通道来创建虚拟通道，以提高模型的泛化能力，并减轻过拟合问题。

### 问题二

这个程序需要跑好久！！







![image-20241109223829251](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241109223829251.png)





### 17、x-branch  z-branch



在 Mamba 中，x-branch 和 z-branch 可能指的是多模态模型结构中的特定分支，它们被设计来处理不同类型的数据或特征。具体来说，在多模态机器学习中，模型往往会有多个**分支**（branches），每个分支专注于一种输入数据模态或特征，这种设计使得模型能够更好地从不同模态的数据中提取特定的特征信息。



**1. x-branch**



通常，x-branch 可以指的是模型中处理视觉或空间信息的分支。x 可能代表 **“特征 (features)”**，例如图像特征或音频特征的频谱图。在图像、视频或其他视觉相关任务中，x-branch 负责处理和提取视觉模态的特征信息，比如通过卷积神经网络（CNN）来提取图像的空间特征。



在一些特定的多模态任务中，x-branch 也可能代表一个用于处理主模态（例如视频或音频）特征的通道。



**2. z-branch**



z-branch 则可能代表另一种模态的数据处理分支。例如，如果 x-branch 处理视觉特征，z-branch 则可能用于处理**辅助特征**或**非视觉特征**（如文本或其他辅助传感器数据）。在音视频分析、情绪识别或多模态融合的任务中，z-branch 可能会处理音频或某种特定传感器数据，然后将这些特征与 x-branch 的输出融合，以实现更为准确的预测或识别效果。



**x-branch 和 z-branch 的作用**



多模态模型中不同的分支（如 x-branch 和 z-branch）的设置，有助于模型专门学习每个模态独特的特征表达。这样，在融合不同模态信息时，模型能在更高层次上综合利用各个模态的信息，提高预测的准确性和泛化能力。这种架构也可以结合注意力机制，确保每个模态在最终决策时的权重分配是合理的。



如果你是在阅读特定的论文或者 Mamba 的某个框架文档，请告诉我更多上下文，我可以进一步详细解释。

























































