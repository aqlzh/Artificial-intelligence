## 1、隐藏马尔可夫模型

**隐藏马尔可夫模型（Hidden Markov Model, HMM）** 是一种统计模型，用于处理序列数据，尤其擅长建模具有**隐含状态**的**时间序列或标记序列**问题。HMM广泛应用于语音识别、自然语言处理、生物信息学等领域。

**模型结构**

HMM 是由两个随机过程组成的：

​	1.	**隐含状态序列**：

​	•	==由一个马尔可夫过程产生的状态序列，但这些状态是“隐藏”的，即无法直接观测==。

​	•	状态转移遵循**马尔可夫性质**：当前状态只依赖于前一个状态。

​	•	状态转移由状态转移概率矩阵  描述，表示从状态  转移到状态  的概率。

​	2.	**观测序列**：

​	•	每个状态会根据某种概率分布生成一个可观测的输出（观测值）。

​	•	输出由观测概率矩阵  描述，表示在状态  时生成观测值  的概率。



此外，初始状态概率  描述了序列的起点状态分布。





## 2、Mamba  再叙

- **Vanilla Mamba**  基础版  

>  https://www.bilibili.com/video/BV1Xn4y1o7TE/?spm_id_from=333.999.0.0&vd_source=81e5007efea018d7c2e8c28374fcdf34



==自注意机制：两两相乘 求权重==  这里有个天然的缺陷，就是自注意力机制的计算范围**仅限于窗口内**，而无法直接处理窗口外的元素。像极了古语所说：两耳不闻窗外事，一心只读圣贤书。某种程度上造成视野狭窄，信息孤立，缺乏全局观一样，这种机制无法建模超出有限窗口的任何内容，看不到更长序列的世界。



TRM：就是通过位置编码把序列空间化 ，然后就通过计算空间相关度反向建模时序相关度这个过程中忽视了数据内在结构的细腻关联关系，而是采取了一种一视同仁的暴力关联模式，好处是直接简单，但显然参数效率低下，冗余度高，训练起来不易。




### 1、前期工作

 《Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers》 

**摘要：**

- 论文介绍了一种新的序列模型，称为线性状态空间层（Linear State-Space Layer, LSSL），==它通过模拟线性连续时间状态空间表示来映射序列数据==。
- LSSL结合了循环神经网络（RNN）、时序卷积和神经微分方程（NDE）的优势，同时解决了这些模型在建模能力和计算效率方面的不足。
- 理论上，LSSL与上述三种模型家族紧密相关，并继承了它们的优点，如将卷积推广到**连续时间**、解释RNN中的常见启发式方法，以及具有NDE的时间尺度适应性。
- 通过引入可训练的结构化矩阵A，LSSL具备了长时记忆能力。
- 实验结果表明，LSSL在处理长依赖关系的时序图像分类、真实世界的医疗回归任务和语音任务中取得了最先进的结果。

**引言：**

- 论文讨论了在机器学习中高效建模长序列数据的挑战，以及现有模型（RNN、CNN、NDE）的优缺点。

**技术背景：**

- 论文回顾了微分方程的基础知识，包括用于将连续时间模型转换为离散时间模型的近似方案，以及步长或时间尺度Δt的重要性。

**LSSL：**
- 定义了LSSL，**并讨论了如何从多个视角（递归、卷积、连续时间）计算它**。
- 证明了LSSL具有表达性，可以表示卷积和RNN。

**结合LSSL与连续时间记忆：**

- ==讨论了LSSL在处理长依赖关系时的主要限制==，并提出了通过特殊类别的结构化矩阵A来解决这些限制的方法。
- 提出了新的算法，使得具有这些矩阵A的LSSL在特定计算模型下可以理论上加速。

**实验评估：**

- 在多个时间序列数据集上测试了LSSL，包括非常长的序列，验证了其有效性。
- 与现有的RNN、CNN和NDE方法相比，LSSL在多个基准测试中取得了更好的结果。

**讨论：**

- <font color=red>论文讨论了LSSL与现有工作的关联，包括连续时间CNN、连续时间RNN以及RNN中的门控机制。</font>
- 论文还讨论了LSSL的局限性，如计算和空间复杂度问题，并提出了未来工作的方向。

**结论：**

- LSSL作为一种新的模型，能够处理非常长的序列数据，并在多个任务中取得了有希望的结果，为简单、原则性和少工程化的模型提供了新的可能性。

这篇论文的核心贡献在于提出了一种新的模型LSSL，它能够结合现有的主流时间序列模型的优点，并在理论上和实验上证明了其有效性。



- 连续变为离散   微分变为差分



> **时不变模型（Time-Invariant Model）** 是指其结构和参数不会随时间发生变化的数学或统计模型。换句话说，这种模型中的系统行为和规则在任意时间点都是一致的。





看图，**就是把长长的链条一下子弄成了输入输出直接对应的样子**，隐变量关联关系都跑到中间肚子里去了。先说结论：核心思想是用CNN对时序数据建模，==借助不同尺度的卷积核==，从不同时间尺度上捕获时序特征。数学上一番推导猛如虎之后能得到下面的公式

- SSM  通过卷积实现并行化，这是与RNN的区别



![image-20241121105506683](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121105506683.png)









- 在实际问题中 会对ABC  进行简化，设成更简单对角阵方便计算，这就是所谓的结构化SSM,S4模型



- 对应原文，我们已经讲完了简介和第二部分。你可能会问，看上去并行SSM就按好的啊，为啥不行呢？别忘了，这个系统还有两个强假设：
  <font color=red>线性+不变</font>极大的限制了它的应用范围，因为实际系统大多为非线性、时变系统。Mamba本质上就是一个SSM模型的改进版，放开了这两个约束。

### 2、选择性 SSM





接下来咱们着重讲解什么是选择性SSM。Mamba主要体现在设计了一种机制，让状态空间具备选择性，达到了Transformers的建模能力，同时在序列长度上实现了线性扩展，也就是克服了Transformers缺陷。可处理最长达百万长度的序列，而具效率贼高，与GPU硬件适配，Transformers:5倍，准确率相当甚至更好。这就是它为啥牛逼起来的原因啦。





![image-20241121150900219](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121150900219.png)

核心就是搞懂这幅图，其实大致看看就明白，在时间序列模型中间设计了这么一坨**非常类似LSMT的门结构，实现所谓的选择性**，BC都带了t变成了<font color=red>时变参数</font>，A虽然没有直接含t,但其实也是时变的了。下面的蓝色部分就是所谓的选择机制，这个delta t别小瞧，**它就是前面离散函数**，一会有大用。要真正明白其中的细节，先要从增加选择性机制开始讲，促其实简单理解就是把整个系统该用：
一个总开关+若干个旋钮=非线性时变系统

下面的解释可以认为是对这种选择的合理化。

### 3、要解决的问题

从某种角度看，==序列建模的核心就是研究如何将长序列的上下文信息压缩到一个较小的状态中==。比如，语言模型实际上就是在一个有限的词汇集合中不断进行转换。有统计表明，3500多个常用中文字，3000个常用英文词能覆盖90%以上的日常用语。<font color=red>transformer的注意力机制虽然很有效，但效率低，因为它需要存储整个上下文，导致推理和训练时间较长</font>。前面讲的SSM递归模型因为它们的状态是有限的（单纯时不变导致)，效率高但有效性受限于状态的压缩能力。



![image-20241121152303950](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121152303950.png)







![image-20241121153215474](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121153215474.png)





前面的（CNN）全局卷积虽然能用不同的卷积核进行时序特征捕捉，**但是缺内容感知**，也就是不知道输入的重点和逻辑。而transformer人家本身是没有这些限制的，**别说时不变，连线性系统假设都没有**。这么一分析，改进的方向就很明确了哈。放开LTI模型的时不变约束，让模型参数依赖于输入内容不就行了吗？说起来简单，具体看看是怎么实现的。





注意：这里的B/L/N/D 符号乍一看让人很困惑，其实就是==张量的维度==。
B:批次大小(Batch size)。表示一次输入的数据量的大小。
L:序列长度(Sequence length)。表示每个序列中包含的时间步数。
N:特征维度(Feature dimension)。表示每个时间步的特征数量。
D:输入特征维度(Input feature dimension)。





