[toc]



> [深度学习复盘与论文复现B_深度学习论文复现博客-CSDN博客](https://blog.csdn.net/QuantumYou/article/details/139354339?ops_request_misc=%7B%22request%5Fid%22%3A%228B832DBE-D254-4E91-8091-D1BAFA2EDA70%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&request_id=8B832DBE-D254-4E91-8091-D1BAFA2EDA70&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-139354339-null-null.nonecase&utm_term=Convolutional Neural Networks&spm=1018.2226.3001.4450) CNN



## 一、AlexNet  阅读

> 文章链接 [ImageNet classification with deep convolutional neural networks (acm.org)](https://dl.acm.org/doi/pdf/10.1145/3065386)

### 1、introduction

**第一段**

一篇论文的第一段通常是讲个故事

- ==做什么研究==
- 哪个方向
- 这个方向有什么东西
- ==为什么很重要==



**第二段**

- 描述了怎么做神经网络，这里只介绍了CNN

- 写论文的时候，<font color=red>千万不要只说自己这个领域这个小方向大概怎么样，还要提到别的方向怎么样</font>



**第三段**

CNN虽然很好，但是很难训练，但是现在有GPU了，GPU算力能够跟上，所以能够训练很大的东西，**而且数据集够大，确实能够训练比较大的CNN**



前三段基本描述了

- 我做了什么东西
- 为什么能做



**第四段**

paper的贡献

- 训练了一个最大的的神经网络，然后取得了特别好的结果
- 实现了GPU上性能很高的一个2D的卷积
- 网络有一些新的和不常见的一些特性，能够提升性能，降低模型的训练时间
- 使用了什么过拟合的方法使得模型变得更好
- 模型具有5个卷积层，3个全连接层，发现深度很重要，移掉任何一层都不行



结果很好，但是还是有新东西在里面的，如果就结果很好，没有新东西，大概是不会称为奠基作



### 2、the dataset



大概描述了一下所用的数据集



重点是最后一段：ImageNet中图片的分辨率是不一样的，因此将每张图片变成了一个256*256的图片：

- **将图片的短边减少到256，长边是保证高宽比不变的情况下也往下降**，长边如果依然多出来的话，如果多于256的话，<font color=red>就以中心为界将两边裁掉</font>，裁成一个256*256的图片
- 没有做任何的预处理，只是对图片进行了裁剪
- 网络是在raw RGB Value上训练的
- 当时做计算机视觉都是将特征抽出来，抽SIFT也好，抽别的特征也好（imagenet数据集也提供了一个==SIFT==版本的特征），这篇文章说不要抽特征，直接是在原始的Pixels上做了
- <font color=blue>在之后的工作里面基本上主要是**end to end（端到端）**：及那个原始的图片或者文本直接进去，不做任何的特征提取，神经网络能够帮助你完成这部分工作</font>





### 3、the architecture



讲整个网络的架构

- relu非线性激活函数
- 使用了多GPU进行训练
- 正则化、归一化
- overlapping pooling
- 总体架构





![img](https://i0.hdslb.com/bfs/note/fec00a250446e40b26248c49b4e86e1d215d562b.png@630w_!web-note.webp)

- 方框表示每一层的输入和输出的数据的大小
- 输入的图片是一个高宽分别为224*224的3通道RGB图片
- **第一层卷积：卷积的窗口是11*11，有48个输出通道，stride等于4**
- 有两个GPU，GPU1和GPU0都有自己的卷积核参数

![img](https://i0.hdslb.com/bfs/note/787fdba5c593948017ec66d0bc130dc6187dcac3.png@630w_!web-note.webp)

- 第一个卷积层在两个GPU上各有一个
- 第二个卷积层是在每个GPU把当前的卷积结果拿过来（GPU0的第二个卷积层读的是GPU0的第一个卷积层的卷积结果，GPU0和GPU1之间没有任何通讯）
- 到第三个卷积层的时候，GPU还是每个GPU上有自己的卷积核，但是每个卷积核会同时将第二个卷积层中GPU0和GPU1的卷积结果作为输入，两个GPU之间会通讯一次
- 第4、5个卷积层之间没有任何通讯
- 每个卷积层的通道数是不一样的，通道数有所增加，高和宽也有所变化
- 高宽慢慢变小、深度慢慢增加，随着深度的增加，慢慢地将空间信息压缩，直到最后每一个像素能够代表前面一大块的像素，然后再将通道数慢慢增加，可以认为每个通道数是去看一种特定的模式（例如192个通道可以简单地认为，能够识别图片中的192种不同的模式）
- 慢慢将空间信息压缩，语义空间慢慢增加，到最后卷积完之后，进入全连接层
- 全连接层中又出现了GPU之间的通讯，全连接层的输入是每个GPU第五个卷积的输出合并起来做全连接

![img](https://i0.hdslb.com/bfs/note/ccc807a47db1ad370b23c9552cc75f226c50f89e.png@630w_!web-note.webp)

- 最后进入分类层的时候，变成了一个4096长的向量，每一块来自两个GPU，每片是2048，最后拼起来，所以一张图片会表示成一个4096维的向量，最后用一个线性分类做链接
- 深度学习的主要作用是将一张输入的图片，通过**卷积、池化、全连接**等一系列操作，将他压缩成一个长为4096的向量，这个向量能够将中间的语义信息都表示出来（将一个人能够看懂的像素通过一系列的特征提取变成了一个长为4096的机器能够看懂的东西，这个东西可以用来做搜索、分类等）
- 整个机器学习都可以认为是一个知识的压缩过程，不管是图片、语音还是文字或者视频，通过一个模型最后压缩成一个向量，然后机器去识别这个向量，然后在上面做各种事情
- 模型并行（model parallel）：现在在计算机视觉里面用的不多，但是在自然语言处理方面又成为主流了（将模型切开进行训练）



### 4、reducing overfitting



第四章讲述了如何降低过拟合



数据增强（data augmentation）

- 把一些图片人工地变大
- 在图片中随机地抠出一部分区域，做一张新的图片
- 把整个RGB的颜色通道channel上做一些改变，这里使用的是一个**PCA（主成分分析）**的方法，颜色会有不同，因此每次图片跟原始图片是有一定的不同的





dropout

- 随机的把一些隐藏层的输出变成用50%的概率设为0，每一次都是把一些东西设置为0，所以模型也就发生了变化，每次得到一个新的模型，但是这些模型之间权重是共享的除了设置成0的，非0的东西都是一样的，这样就等价于做了模型融合
- 后来大家发现dropout其实也不是在做模型融合，==更多的dropout就是一个正则项（dropout在现行模型上等价于一个L2正则项）==
- 这里将dropout用在了前面的两个全连接层上面
- 文章说没有dropout的话，overfitting会非常严重，有dropout的话，训练会比别人慢两倍
- 现在CNN的设计通常不会使用那么大的全连接层，所以dropout也不那么重要，而且GPU、内存也没那么吃紧了
- dropout在全连接上还是很有用的，在RNN和Attension中使用的非常多



### 5、details of learning



讲述了模型是如何训练的

- 使用SGD（随机梯度下降）来进行训练，SGD调参相对来说可能会比较难调，后来发现SGD里面的噪音对模型的泛化性其实是有好处的，所以现在深度学习中普遍使用SGD对模型进行训练。在这个文章之后SGD基本上在机器学习界成为了最主流的一个优化算法
- 批量大小是128
- momentum是0.9
- weight decay是0.0005，<font color=red>也就是L2正则项，但是这个东西不是加在模型上，而是加在优化算法上</font>，虽然他们两个是等价关系，但是因为深度学习的学习，所以大家现在基本上把这个东西叫做weight decay了
- momentum也是因为这篇文章之后用的特别多，虽然在2010年的时候有大量的加速算法，里面有很fancy的各种加速SGD算法，但是现在看起来似乎用一个简单的momentum也是不错的
- momentum实际上是，当优化的表面非常不平滑的时候，冲量使得不要被当下的梯度过多的误导，可以保持一个冲量从过去那个方向，沿着一个比较平缓的方向往前走，这样子不容易陷入到局部最优解
- 权重用的是一个均值为0，方差为0.01的高斯随机变量来初始化（0.01对很多网络都是可以的，但是如果特别深的时候需要更多优化，但是对于一些相对简单的神经网络，0.01是一个不错的选项）
- 现在就算是比较大的那些BERT，也就是用了0.02作为随机的初始值的方差
- 在第二层、第四层和第五层的卷积层把初始的偏移量初始化成1，剩下的全部初始化成0
- 每个层使用同样的学习率，从0.01开始，然后呢如果验证误差不往下降了，就手动的将他乘以0.1，就是降低十倍
- ResNet中，每训练120轮，学习率每30轮就下降0.1另外一种主流的做法就是，前面可以做得更长一点，必须能够60轮或者是100轮，然后再在后面下降
- 在Alex之后的很多训练里面，都是做规则性地将学习率往下下降十倍，这是一个非常主流的做法，但是现在很少用了，现在使用更加平滑的曲线来降低学习率，比如果用一个cos的函数比较平缓地往下降。一开始的选择也很重要，如果选的太大可能会发生爆炸，如果太小又有可能训练不动，所以现在主流的做法是学习率从0开始再慢慢上升，慢慢下降

![img](https://i0.hdslb.com/bfs/note/c490536f8d63015d57bf2564fd9e249d6e3f3aa8.png@630w_!web-note.webp)

- 模型训练了90个epoch，然后每一遍用的是ImageNet完整的120万张图片，需要5-6天在两个GTX GPU上训练





### 6、result



- 有时候结果可能不重要

- 有些东西可能还不是很理解，可以去看文章所引用的文章





## 二、ResNet 阅读

- xavier初始化

### 1、introduction

- 深度神经网络好在可以加很多层把网络变得特别深，==然后不同程度的层会得到不同等级的feature，比如低级的视觉特征或者是高级的语义特征==



提出问题：随着网络越来越深，<font color=red>梯度就会出现爆炸或者消失</font>

- 解决他的办法就是：1、*在初始化的时候要做好一点，就是权重在随机初始化的时候，权重不要特别大也不要特别小*。2、在中间加入一些normalization，包括BN（batch normalization）可以使得校验每个层之间的那些输出和他的梯度的均值和方差相对来说比较深的网络是可以训练的，避免有一些层特别大，有一些层特别小。使用了这些技术之后是能够训练（能够收敛），虽然现在能够收敛了，但是当网络变深的时候，性能其实是变差的（精度会变差）
- 文章提出出现精度变差的问题不是因为层数变多了，模型变复杂了导致的过拟合，而是因为训练误差也变高了（overfitting是说训练误差变得很低，但是测试误差变得很高），训练误差和测试误差都变高了，所以他不是overfitting。虽然网络是收敛的，但是好像没有训练出一个好的结果





深入讲述了**深度增加了之后精度也会变差** 

- 考虑一个比较浅一点的网络和他对应的比较深的版本（在浅的网络中再多加一些层进去），如果浅的网络效果还不错的话，深的网络是不应该变差的：深的网络新加的那些层，总是可以把这些层学习的变成一个**identity mapping**（输入是x，输出也是x，等价于可以把一些权重学成比如说简单的n分之一，是的输入和输出是一一对应的），但是实际情况是，虽然理论上权重是可以学习成这样，但是实际上做不到：假设让SGD去优化，深层学到一个跟那些浅层网络精度比较好的一样的结果，上面的层变成identity（相对于浅层神经网络，深层神经网络中多加的那些层全部变成identity），这样的话精度不应该会变差，应该是跟浅层神经网络是一样的，但是实际上SGD找不到这种最优解
- 这篇文章提出显式地构造出一个identity mapping，使得深层的神经网络不会变的比相对较浅的神经网络更差，它将其称为deep residual learning framework
- 要学的东西叫做H（x），假设现在已经有了一个浅的神经网络，他的输出是x，然后要在这个浅的神经网络上面再新加一些层，让它变得更深。新加的那些层不要直接去学H（x），而是应该去学H（x）-x，x是原始的浅层神经网络已经学到的一些东西，新加的层不要重新去学习，而是去学习学到的东西和真实的东西之间的残差，最后整个神经网络的输出等价于浅层神经网络的输出x和新加的神经网络学习残差的输出之和，将优化目标从H（x）转变成为了H（x）-x

![img](https://i0.hdslb.com/bfs/note/0a418ebf24535ae9494157b84c95460d67c4f11a.png@706w_!web-note.webp)

- 上图中最下面的红色方框表示所要学习的H（x）
- 蓝色方框表示原始的浅层神经网络
- 红色阴影方框表示新加的层
- o表示最终整个神经网络的输出
- 这样的好处是：只是加了一个东西进来，没有任何可以学的参数，不会增加任何的模型复杂度，也不会使计算变得更加复杂，而且这个网络跟之前一样，也是可以训练的，没有任何改变





非常深的residual nets非常容易优化，但是如果不添加残差连接的话，效果就会很差。越深的网络，精度就越高



<font color=red>introduction是摘要的扩充版本，也是对整个工作比较完整的描述</font>



### 2、related work



一篇文章要成为经典，不见得一定要提出原创性的东西，很可能就是把之前的一些东西很巧妙的放在一起，能解决一个现在大家比较关心难的问题





残差连接如何处理输入和输出的形状是不同的情况：

- 第一个方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加
- 第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做1*1的卷积层，这个卷积层的特点是在空间维度上不做任何东西，主要是在通道维度上做改变。所以只要选取一个1*1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上





### 3、Deep Residual Learning

implementation中讲了实验的一些细节

- 把短边随机的采样到256和480（AlexNet是直接将短边变成256，而这里是随机的）。随机放的比较大的好处是做随机切割，切割成224*224的时候，随机性会更多一点
- 将每一个pixel的均值都减掉了
- 使用了颜色的增强（AlexNet上用的是PCA，现在我们所使用的是比较简单的RGB上面的，调节各个地方的亮度、饱和度等）
- 使用了BN（batch normalization）
- 所有的权重全部是跟另外一个paper中的一样（作者自己的另外一篇文章）。注意写论文的时候，尽量能够让别人不要去查找别的文献就能够知道你所做的事情
- 批量大小是56，学习率是0.1，然后每一次当错误率比较平的时候除以10
- 模型训练了60*10^4个批量。建议最好不要写这种iteration，因为他跟批量大小是相关的，如果变了一个批量大小，他就会发生改变，所以现在一般会说迭代了多少遍数据，相对来说稳定一点
- 这里没有使用dropout，因为没有全连接层，所以dropout没有太大作用
- 在测试的时候使用了标准的10个crop testing（给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后再每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。
- 采样的时候是在不同的分辨率上去做采样，这样在测试的时候做的工作量比较多，但是在实际过程中使用比较少







### 4、experiments



- 如何评估ImagNet
- 各个不同版本的ResNet是如何设计的

首先阐述了ImageNet

描述了plain networks

没有带残差的时候，使用了一个18层和34层

![img](https://i0.hdslb.com/bfs/note/bcecbccc1aeaeb5620412f501396378706cbd175.png@686w_!web-note.webp)

- 上表是整个ResNet不同架构之间的构成信息（5个版本）
- 第一个7*7的卷积是一样的
- 接下来的pooling层也是一样的
- 最后的全连接层也是一样的（最后是一个全局的pooling然后再加一个1000的全连接层做输出）
- 不同的架构之间，主要是中间部分不一样，也就是那些复制的卷积层是不同的
- conv2.x：x表示里面有很多不同的层（块）
- 【3*3,64】:46是通道数
- 模型的结构为什么取成表中的结构，论文中并没有细讲，这些超参数是作者自己调出来的，实际上这些参数可以通过一些网络架构的自动选取
- **flops**：整个网络要计算多少个浮点数运算。卷积层的浮点运算等价于输入的高乘以宽乘以通道数乘以输出通道数再乘以核的窗口的高和宽



表1：ImageNet的架构

表1详细列出了用于ImageNet分类任务的不同残差网络（ResNet）的架构。这些架构通过使用不同数量的残差模块来构建不同深度的网络。以下是表1中的主要内容：

- **layer name**: 层的名称，表示网络中不同层次的名称。
- **output size**: 该层输出的特征图尺寸。
- **18-layer, 34-layer, 50-layer, 101-layer, 152-layer**: 这些列分别展示了不同深度的残差网络架构，包括18层、34层、50层、101层和152层。

每个层级包含的详细信息如下：

- **卷积层**: 表示为 `3x3 conv, 64` 意味着这是一个3x3的卷积核，有64个滤波器的卷积层。
- **步幅**: 如 `/2` 表示该层操作会将宽度和高度缩小一半。
- **池化层**: 表示为 `3x3 max pool, stride 2` 表示使用3x3的最大池化，步幅为2。
- **线性层**: 如 `1x1, 256` 表示1x1的卷积核用于改变特征图的深度，这里是将特征图深度从64变为256。
- **平均池化层**: `1x1 average pool` 表示1x1的平均池化层。
- **全连接层**: `fc 1000` 表示1000个神经元的全连接层。

**FLOPs**: 表示每层操作的浮点运算次数，用于衡量计算复杂度。



图4：在ImageNet上的训练情况

图4展示了18层和34层的普通网络（plain networks）与残差网络（ResNet）在ImageNet数据集上的训练和验证误差对比。

- **左侧**: 展示了18层和34层普通网络的训练过程。图中细线表示训练误差，粗线表示验证误差。可以观察到，随着网络深度的增加，34层网络的训练误差和验证误差都比18层网络要高，这表明普通网络在增加深度时会遇到优化难题。

- **右侧**: 展示了18层和34层残差网络的训练过程。同样，细线表示训练误差，粗线表示验证误差。与普通网络相反，34层残差网络不仅训练误差比18层残差网络要低，而且验证误差也更低。这说明残差网络能够通过增加深度来提高准确率，并且解决了普通网络遇到的优化难题。

总结

表1和图4共同展示了残差网络架构的细节以及它们在ImageNet数据集上的性能表现。通过引入残差学习，可以有效地训练更深的网络，并在图像识别任务中取得更好的性能。



![img](https://i0.hdslb.com/bfs/note/316285db5ae7f1c4dcab40937540681a5b0a7f04.png@686w_!web-note.webp)

- 上图中比较了18层和34层在==有残差连接和没有残差连接的结果==
- 左图中，红色曲线表示34的验证精度（或者说是测试精度）
- 左图中，粉色曲线表示的是34的训练精度
- 一开始训练精度是要比测试精度高的，因为在一开始的时候使用了大量的数据增强，使得寻来你误差相对来说是比较大的，而在测试的时候没有做数据增强，噪音比较低，所以一开始的测试误差是比较低的
- 图中曲线的数值部分是由于学习率的下降，每一次乘以0.1，对整个曲线来说下降就比较明显。为什么现在不使用乘0.1这种方法：在什么时候乘时机不好掌控，如果乘的太早，会后期收敛无力，晚一点乘的话，一开始找的方向更准一点，对后期来说是比较好的
- 上图主要是想说明在有残差连接的时候，34比28要好；另外对于34来说，有残差连接会好很多；其次，有了残差连接以后，收敛速度会快很多，核心思想是说，在所有的超参数都一定的情况下，有残差的连接收敛会快，而且后期会好





<font color=red>输入输出形状不一样的时候怎样做残差连接</font>

- 填零
- 投影
- 所有的连接都做投影：就算输入输出的形状是一样的，一样可以在连接的时候做个1*1的卷积，但是输入和输出通道数是一样的，做一次投影



对比以上三种方案

![img](https://i0.hdslb.com/bfs/note/8c75c28d1b2ec0943678d40962aa0c5101d4fd4f.png@686w_!web-note.webp)

- A表示填0
- B表示在不同的时候做投影
- C表示全部做投影
- B和C的表现差不多，但是还是要比A好一点
- B和C虽然差不多，但是计算复杂度更高，B对计算量的增加比较少，作者采用了B





怎样构建更深的ResNet

如果要做50或者50层以上的，会引入bottleneck design

![img](https://i0.hdslb.com/bfs/note/9a6493ed10b6a1cb12739837870d10c86c030419.png@686w_!web-note.webp)

- 左图是之前的设计，当通道数是64位的时候，通道数不会发生改变

- 如果要做到比较深的话，可以学到更多的模式，可以把通道数变得更大，右图从64变到了256

- 当通道数变得更大的时候，计算复杂度成平方关系增加，这里通过1个1*1的卷积，将256维投影回到64维，然后再做通道数不变的卷积，然后再投影回256（将输入和输出的通道数进行匹配，便于进行对比）。等价于先对特征维度降一次维，在降一次维的上面再做一个空间上的东西，然后再投影回去

- 虽然通道数是之前的4倍，但是在这种设计之下，二者的算法复杂度是差不多的

  32:25

  

>每一个卷积核的通道数增加到四倍，为了保持**输出通道数**不变，卷积核数量也变为四倍，一共十六倍
>
>ImageNet标号的错误率本来挺高的，估计有1%



CIFAR-10是一个很小的数据集，跑起来相对来说比较容易，32*32，五万个样本，10类的数据集





在整个残差连接，如果后面新加上的层不能让模型变得更好的时候，因为有残差连接的存在，新加的那些层应该是不会学到任何东西，应该都是靠近0的，这样就等价于就算是训练了1000层的ResNet，但是可能就前100层有用，后面的900层基本上因为没有什么东西可以学的，基本上就不会动了





这篇文章没有结论





==**mAP**：目标检测上最常见的一个精度，锚框的平均精度，越高越好==





**为什么ResNet训练起来比较快？**

- 一方面是因为梯度上保持的比较好，新加一些层的话，加的层越多，梯度的乘法就越多，因为梯度比较小，一般是在0附近的高斯分布，所以就会导致在很深的时候就会比较小（梯度消失）。虽然batch normalization或者其他东西能够对这种状况进行改善，但是实际上相对来说还是比较小，但是如果加了一个ResNet的话，它的好处就是在原有的基础上加上了浅层网络的梯度，深层的网络梯度很小没有关系，浅层网络可以进行训练，变成了加法，一个小的数加上一个大的数，相对来说梯度还是会比较大的。也就是说，不管后面新加的层数有多少，前面浅层网络的梯度始终是有用的，这就是从误差反向传播的角度来解释为什么训练的比较快

![img](https://i0.hdslb.com/bfs/note/9793eefee9774a679bd59f35543ba79ce40cd3b5.png@686w_!web-note.webp)

- 在CIFAR上面加到了1000层以上，没有做任何特别的regularization，然后效果很好，overfitting有一点点但是不大。SGD收敛是没有意义的，SGD的收敛就是训练不动了，收敛是最好收敛在比较好的地方。做深的时候，用简单的机器训练根本就跑不动，根本就不会得到比较好的结果，所以只看收敛的话意义不大，但是在加了残差连接的情况下，因为梯度比较大，所以就没那么容易收敛，所以导致一直能够往前（SGD的精髓就是能够一直能跑的动，如果哪一天跑不动了，梯度没了就完了，就会卡在一个地方出不去了，所以它的精髓就在于需要梯度够大，要一直能够跑，因为有噪音的存在，所以慢慢的他总是会收敛的，所以只要保证梯度一直够大，其实到最后的结果就会比较好）

![img](https://i0.hdslb.com/bfs/note/4f84d817570adb88364b9dad35af6db13710a6a4.png@686w_!web-note.webp)





为什么ResNet在CIFAR-10那么小的数据集上他的过拟合不那么明显？

open question

虽然模型很深，参数很多，但是因为模型是这么构造的，所以使得他内在的模型复杂度其实不是很高，也就是说，很有可能加了残差链接之后，使得模型的复杂度降低了，一旦模型的复杂度降低了，其实过拟合就没那么严重了

- 所谓的模型复杂度降低了不是说不能够表示别的东西了，而是能够找到一个不那么复杂的模型去拟合数据，就如作者所说，不加残差连接的时候，理论上也能够学出一个有一个identity的东西（不要后面的东西），但是实际上做不到，因为没有引导整个网络这么走的话，其实理论上的结果它根本过不去，所以一定是得手动的把这个结果加进去，使得它更容易训练出一个简单的模型来拟合数据的情况下，等价于把模型的复杂度降低了





这篇文章的residual和gradient boosting是不一样的

- gradient boosting是在标号上做residual
- 这篇文章是在feature维度上





## 三、Transformer 阅读

**transformer**



- 最近三年以内深度学习里面最重要的文章之一
- 可以认为是开创了继MLP、CNN、RNN之后的第四大类模型

### 1、标题：Attention is all you need



作者后面打上星号表示同等贡献

![img](https://i0.hdslb.com/bfs/note/308006683fd345e35b1f028664a22a545a1d01c4.png@630w_!web-note.webp)



### 2、摘要



- 在主流的序列转录模型里面，主要是依赖于比较复杂的循环或者是卷积神经网络，一般是使用encoder和decoder的架构

- 序列转录模型：给定一个序列，然后生成另外一个序列，比如机器翻译



在性能最好的模型之中，通常也会在编码器和解码器之间使用注意力机制



这篇文章提出了一个新的简单的架构（simple，之前都倾向于写成novel），这个模型仅仅依赖于注意力机制，而没有用之前的循环或者卷积



做了两个机器翻译的实验，显示这个模型在性能上特别好，可以并行度更好然后使用更少的时间来训练



模型达到了28.4的BLEU

- BLEU score：机器翻译里面常用的一个衡量标准



这篇文章一开始写的时候是针对机器翻译这个小任务写的，但是随着之后BERT、GPT把这个架构用在更多的语言处理的任务上的时候，整个工作就出圈了，最近用在了图片和video上面，几乎什么东西都能用



### 3、导言



==transformer这个模型是第一个仅仅使用注意力机制做序列转录的模型，将之前的循环层全部换成了multi-headed self-attetion==



在机器翻译的这个任务上面，transformer能够训练的比其他的架构都要快很多，而且在实际的结果上确实是效果更好



作者对于这种纯基于注意力机制的模型感到非常激动，作者想要把它用在文本以外的别的数据上面，使得生成不那么时序化也是另外的一个研究方向



这篇文章所有的代码放在tensor2tensor这个库里面，作者将整个代码放在了结论的最后（如果有代码的话，通常会把这个代码放在摘要的最后一句话，因为现在神经网络的文章里面细节通常是比较多的，简简单单的一篇文章很难把所有的细节都讲清楚，所以最好第一时间公布代码，让别人能够很方便地复现文章，然后这样就能够扩大文章的影响力）



### 4、结论



这里的导言写的比较短，基本可以认为是前面摘要的前面一半的扩充



在时序模型里面，2017最常用的是RNN、LSTM、GRU，有两个比较主流的模型：

- 一个叫做语言模型
- 另一个是当输出结构化信息比较多的时候会使用**编码器和解码器架构**



**RNN的特点**

- RNN中给定一个序列，它的计算是把这个序列从左到右一步一步往前做，假设序列是一个句子的话，就是一个一个词，对第t个词会计算出一个输出叫做ht（也叫做它的隐藏状态），ht是由前面一个词的隐藏状态（h（t-1））和当前第t个词本身决定的，这样就可以把前面学到的历史信息通过h（t-1）放到当下，然后和当前的词做一些计算，然后得到输出



**RNN如何有效处理时序信息的关键**：他将之前的信息全部放在隐藏状态里，然后一个一个放下去。他的问题也来自于这里：

1. <font color=red>他是一个一步一步计算（时序）的过程，比较难以并行，在时间上难以并行，使得在计算上性能比较差</font>
2. 历史信息是一步一步地往后传递的，如果时序比较长的话，那么很早期的时序信息在后面的时候可能丢掉，如果不想丢掉的话，==可能需要ht要比较大，如果要做比较大的ht，在每一个时间步都得把他存下来，导致内存开销比较大==。



**attention在RNN上的应用**

- 在这篇文章之前，attention已经成功地用在编码器和解码器里面了，主要是用在怎么样把编码器的东西有效地传给解码器



这篇文章提出来的transformer是一个新的模型，不再使用之前被大家使用的循环神经层，而是纯基于注意力机制，并行度比较高，这样的话它能够在比较短的时间之内做到比之前更好的结果



NeurIPS是一个篇幅比较短的会议，单列八页



### 5、相关工作



如何使用卷积神经网络替换循环神经网络，减少时序的计算



主要问题：用卷积神经网络的话，对于比较长的序列难以建模

- ==卷积做计算的时候每次是去看一个比较小的窗口，如果两个像素相隔比较远的话，需要用很多层卷积堆积起来==，最后才能够把这两个隔得比较远的像素融合起来。但是如果是使用transformer里面的注意力机制的话，每一次都能够看到所有的像素，使用一层就能看到整个序列
- 卷积的好处是可以做多个输出通道，<font color=blue>一个输出通道可以认为是识别不一样的模式，作者也想要实现这种效果，所以提出了一个叫做multi-headed attention（多头注意力机制），用于模拟卷积神经网络多输出通道的一个效果</font>



自注意力机制



memory networks



### 6、模型



序列模型里面比较好的是编码器和解码器的架构

- 编码器：将一个长为n的x1到xn的输入，编码器会把它表示成为一个也是长为n但是其中每一个zt对应xt的向量的表示，这就是编码器的输出，就是将原始的输入变成机器学习可以理解的一系列向量

- 解码器：拿到编码器的输出，然后生成一个长为m的一个序列（n和m是不一样长的，可以一样，也可以不一样）。他和编码器最大的不同之处在于，在解码器中，词是一个一个生成的（**因为对于编码器来讲，很可能是一次性能看全整个句子，但是在解码的时候只能一个一个的生成**），即自回归（auto-regressive）,在这里面输入又是输出，在过去时刻的输出也会作为当前时刻的输入

  18:13

- transformer是使用了编码器和解码器的架构，具体来讲它是将自注意力和point-wise、fully connected layers一个一个堆在一起

![img](https://i0.hdslb.com/bfs/note/0e3d20df953dae975726187ccb0f650bdd0ffa7a.png@630w_!web-note.webp)

- 写论文的话，有一张比较漂亮的能够把整个全局话清楚的图是非常重要的

![img](https://i0.hdslb.com/bfs/note/b0776a09bed44621f07c49727b13b23b5068864a.png@630w_!web-note.webp)

- 上图是一个编码器和解码器的架构

![img](https://i0.hdslb.com/bfs/note/810dcd1555f3667928075a49d562ecb201d75090.png@630w_!web-note.webp)

- 左边圈出来的部分是编码器，右边圈出来的部分是解码器
- 左下角的input是编码器的输入，如果是中文翻英文的话，输入就是中文的句子
- 右下角的outputs是解码器的输入，解码器在做预测的时候是没有输入的，<font color=red>实际上就是解码器在之前时刻的一些输出作为输入</font>
- shifted right就是一个一个往右移
- input embedding：嵌入层。进来的是一个一个的词，需要将它们表示成向量
- poositional encoding：
- Nx：N代表层数

![img](https://i0.hdslb.com/bfs/note/e51891cb1c295c0dd3e694e67e8bf482e2ee8201.png@630w_!web-note.webp)

- 红色圆圈圈出来的部分可以看作*transformer block*
- multi-headed attention
- feed forward：前馈神经网络
- add&norm：**add表示残差的连接**

![img](https://i0.hdslb.com/bfs/note/1a2a1c716c249a06927cef2dd67c068bd2e4ea26.png@630w_!web-note.webp)

- 编码器的输出会作为解码器的输入

![img](https://i0.hdslb.com/bfs/note/4a4253d3db880e386c30502213c33ba0c8e11761.png@630w_!web-note.webp)

- 解码器和编码器有点像，红色圆圈圈出来的部分是一样的，但是多了下面的masked multi-headed attention（多头注意力机制）

![img](https://i0.hdslb.com/bfs/note/7451688c71b3902744ff608362c07c0c4248b62b.png@630w_!web-note.webp)

- 解码器可以认为是红色圆圈中的三部分组成的一个块重复n次
- 解码器的输出进入一个输出层，然后做一个softmax就能得到最终的输出

![img](https://i0.hdslb.com/bfs/note/53b444cad5e6bf92cc4583be019995733d10aac8.png@630w_!web-note.webp)

- 红色方括号所扩起来的部分就是标准的神经网络的做法
- 上图所示的确是一个标准的编码器解码器的架构，只是说中间的每一块有所不同，然后是编码器和解码器之间的连接有所不同





**具体模块的实现**



**编码器**



编码器是用一个n等于6的一个完全一样的层，下图中的红色阴影部分算作一层，然后重复6次

- 每一个layer中会有2个sub-layer
- 第一个sub-layer叫做multi-headed self-attention
- 第二个sub-layer是一个simple position-wise fully connected feed-forward network，说白了就是一个MLP
- 对于每一个子层采用了一个残差连接，最后再使用layer normalization

![img](https://i0.hdslb.com/bfs/note/8135b4566e409c3da99494834984b8b26c2128d5.png@630w_!web-note.webp)

- 子层的公式如下图中黄色高亮部分所示

![img](https://i0.hdslb.com/bfs/note/aecfc2f2f4340d1f4b542272d3b39c32ed3a579e.png@630w_!web-note.webp)

- sublayer（x）：输入进来以后先进入子层
- x+sublayer（x）：因为是残差连接，所以将输入和输出加在一起，最后进入他的layernorm
- 因为残差连接需要输入和输出是一样大小的，如果大小不一样的话，需要做投影，为了简单起见，讲么一个层的输出维度变成512（固定长度表示是的模型相对来说是比较简单的，调参的话只需要调一个参数就行了，就是模型的输出维度，另外一个参数是要复制多少块n）





### batch norm对比layer norm

- 与 Batch Normalization 不同的是，Layer Normalization 是<font color=red>在单个样本的维度上进行归一化的，因此更加适合处理变长的序列数据，比如自然语言处理任务中的文本</font>。

考虑一个最简单的二维输入的情况，在二维输入的情况下输入是一个矩阵，**每一行是一个样本，每一列是一个特征**

![img](https://i0.hdslb.com/bfs/note/7e9f1aed6a40e400cb368bdab253ee1a70ea3d2e.png@630w_!web-note.webp)

batch norm所干的事情就是每一次将每一列（特征）在它的一个小mini-batch里面的均值变成零，方差变成1

- 把这个向量本身的均值减掉，然后再除以他的方差就可以了
- 在计算均值的时候，是在每个小批量里面（一条向量里面算出他的均值和方差）
- 在训练的时候可以做小批量，在预测的时候会把全局的均值算出来
- 在预测的时候会把全局的均值算出来，整个数据扫一遍之后，在所有数据上平均的均值方差存起来，在预测的时候再使用

![img](https://i0.hdslb.com/bfs/note/4295a86121344721f7c408f8f68f07fabed9e5b6.png@630w_!web-note.webp)

- batchnorm还会学 lambda和beta 出来：可以把向量通过学习放成一个方差为任意某个值，均值为任意某个值的东西



**layernorm和batpchnorm在很多时候几乎是一样的**

- 对于一个同样的二维输入来说（最简单的情况），layer norm对每个样本做normalization而不是对每个特征做normalization（之前是将每个列的均值变为0，方差变为1，现在是把每一个行变成均值为0，方差为1，这里的每一个行表示一个样本，<font color=red>所以可以认为layernorm就是把整个数据转置一下放到batchnorm里面出来的结果，再转置回去）</font>
- 在transformer或者正常的RNN里面，输入是一个三维的东西，输入的是一个序列的样本，每一个样本其实里面有很多元素（比如一个句子里面有n个词，每个词有个向量hebatch的话，就是一个3D的东西）。最大的正方形表示batch（样本），但是列不再是特征了，而是序列的长度（sequence），对每个sequence（每个词）都有自己的向量，即feature

![img](https://i0.hdslb.com/bfs/note/3dacd4d421cb79e1730f1fab0379416e0b436db3.png@630w_!web-note.webp)

- 如果还是用batchnorm的话，每次是取一根特征，然后把他的每个样本里面的所有元素，以及他的整个batch取出来，如下图立方体中蓝色正方形所示，然后把他的均值变为0方差变成1，就相当于是切一块下来然后拉成一个向量，然后再进行运算

![img](https://i0.hdslb.com/bfs/note/4f45e9a27d369ab5e70da71b8f23e5b7271c5665.png@630w_!web-note.webp)

- 如果是laynorm的话，就是对每个样本进行横切，如上图立方体中橘黄色正方形所示 



切法不一样会带来不同的效果，为什么layernorm用的多一点？

- 在时序序列模型中，每个样本的长度可能会发生变化，如下图中红色阴影所示，没有的东西一般是会放零进去

![img](https://i0.hdslb.com/bfs/note/3054b67ce539f6bfdc90a1f08935d1aaaebd6e5d.png@630w_!web-note.webp)

- 如果是用batchnorm的话，切出来的效果如下图中所示，其余地方补零

![img](https://i0.hdslb.com/bfs/note/330c7dbd93d170182fc29136ca76d42ad6753d64.png@630w_!web-note.webp)

- 如果是layernorm的话，切出来的效果如下图所示

![img](https://i0.hdslb.com/bfs/note/7cabc373f27ccd8bb867abc27ef85518c2d8179a.png@630w_!web-note.webp)

- 这里主要的问题是在算均值和方差上面，对于batchnorm来说，会对上图中切出来的阶梯形的部分进行求解（只有这部分是有效值，其他地方因为是补零，所以其实没有太多作用），如果样本长度变化比较大的时候，每次做小批量的时候，算出来的均值和方差的抖动相对来说是比较大的
- 另外，在做预测的时候要把全局的均值和方差记录下来，这个全局的均值和方差如果碰到一个新的预测样本，如果碰到一个特别长的，因为在训练的时候没有见过这种长度的，那么在之前计算的均值和方差可能就不那么好用了。
- 相反，对于layernorm相对来说没有太多的问题，因为他是按照每个样本来进行均值和方差的计算，同时也不需要存下一个全局的均值和方差（不管样本的长短，均值和方差的计算都是以样本为单位的），这样的话相对来讲更稳定一些





**解码器**



解码器跟编码器很像，跟编码器一样是由（n=6）个同样的层构成的，每个层里面跟编码器一样有两个子层



解码器和编码器的不同之处在于解码器里面用了第三个子层，他同样是多头的注意力机制，跟编码器一样同样用了残差连接，用了layernorm



解码器中做的是自回归，也就是说当前的输出的输入集是上面一些时刻的输出，**意味着在做预测的时候不能看到之后的那些时刻的输出**

- 在注意力机制中，每一次能够看到完整的输入，这里要避免这个情况的发生，也就是说在解码器训练的时候，在预测第t个时刻的输出时候不应该看到t时刻以后的那些输入，<font color=blue>他的做法是通过一个带掩码的注意力机制，如下图中的masked所示，这也是与解码器其他地方的不同之处，这个masked的作用是保证输入进来的时候，在t时间是不会看到t时间以后的那些输入，从而保证训练和预测的时候行为是一致的</font>

![img](https://i0.hdslb.com/bfs/note/dfe1b6afe0df3580ac9da156feb63028e340e173.png@630w_!web-note.webp)





**子层**



注意力层



注意力：注意力函数是一个将 query 和一些 key-value对 映射成输出的函数

- 里面所有的query、key-value、输出都是向量

- 输出是value的加权和，所以输出的维度和value的维度是一样的

- 对于每一个value的权重，他是value对应的key和query的相似度（compatibility function，不同的注意力机制有不同的算法，不同的相似度函数导致不一样的注意力的版本）计算得来的

  34:45



transformer中使用到的注意力机制：

scaled dot-product attention

- **query和key长度是等长的，都等于dk（可以不等长的，不等长有别的计算方法）**
- value的长度等于dv（输出的长度也应该是dv）
- 具体的计算方法：对每一个query和key最内积，然后将其作为相似度（如果两个向量做内积的话，如果这两个向量的 long 是一样的，那么内积的值（余弦值）越大，就表示这两个向量的相似度就越高，如果内积等于零（两个向量正交），就是说这两个向量没有相似度），**算出来之后再除以根号dk（即向量的长度**），然后再用softmax来得到权重。因为对于一个query，假设给n个key、value对的话，就会算出n个值，因为这个query会跟每个key做内积，算出来之后再放进softmax就会得到n个非负的和为1的权重（对于权重来说，非负、和为1是比较好的权重），然后将这些权重作用在value上面，就能得到输出了。
- 在实际中不能一个一个做运算，运算起来比较慢，文章提出query可以写成矩阵，可能不只是一个query，也可能有n个query，query的个数和key value的个数可能是不一样的，但是长度一定是一样的，这样才能做内积
- 给定query和key这两个矩阵，相乘就会得到一个n*m的矩阵，如下图所示，他的每一行（如图中蓝色的线所示），就是一个query对所有key的内积值，再除以根号dk后做softmax（对每一行做softmax，行与行之间是独立的），这样就能得到权重，然后再乘V（V是一个m行dv列的矩阵），得到一个n*dv的矩阵（这个矩阵的每一行就是所需要的输出）

![img](https://i0.hdslb.com/bfs/note/041d075f36729f3cf7dad1ea1b6412c724e5068c.png@630w_!web-note.webp)

- 对于key、value对和n个query，可以通过两次矩阵乘法来做整个计算，key和value在实际中对应的就是序列，这样就等价于是在并行地计算里面的每个元素（矩阵乘法便于并行）





文中所提出的注意力机制和其他注意力机制的区别

一般有两种比较常见的注意力机制

- 加型注意力机制：可以处理query和key不等长的情况
- 点积注意力机制：点积注意力机制和文中所提出的注意力机制是一样的（唯一的区别就是文中所提出来的注意力机制多除以了一个根号dk，这个根号dk就是命名中提到的scaled）

这两种注意力机制其实都差不多，文章选用的是点积注意力机制，==因为实现起来比较简单，而且效果比较好，两次矩阵乘法就能算好==

这里为什么要除以根号dk？

- 当dk不是很大的时候除不除都没关系，但是当dk比较大的时候，也就是两个向量长度比较长的时候，做点积的时候值可能比较大也可能比较小
- 当值比较大的时候，向量之间相对的差距就会变大，就导致值最大的那个值进行softmax操作后就会更接近1，剩下的值就会更靠近于0，值就会向两极分化，当出现这种情况后，在算梯度的时候，梯度会比较小（softmax最后的结果是所希望的预测值置信的地方尽量靠近1，不置信的地方尽量靠近0，这样就差不多收敛了，梯度就会变得比较小，就会跑不动）
- 在transformer里面一般用的dk比较大（512），所以除以根号dk是一个不错的选择





整个注意力机制的计算过程如下图左图所示

![img](https://i0.hdslb.com/bfs/note/5873f1135c233f4bf9ca53377f2d444a52e99ea4.png@630w_!web-note.webp)

- Q代表query矩阵
- K代表key矩阵
- mask主要是为了避免在第t时刻的时候看到以后时间的东西，具体来说，假设query和key是等长的，长度都为n，而且在时间上是能对应起来的，对第t时刻的qt在做计算的时候，应该只是看到k1一直到k（t-1），而不应该看到kt和它之后的东西，因为kt在当前时刻还没有。
- 但是在做注意力机制的时候，会发现其实qt在跟所有k里面的东西全部做运算，从k1一直算到kn，只要保证在计算权重的时候，**不要用到后面的东西就可以了**
- mask是说对于qt和kt之后的用于计算的那些值，**把他们替换成非常大的负数**，这些大的负数在进入softmax做指数的时候就会变成0，所以导致softmax之后出来对应的权重都会变成0，而kt之前所对应的值会有权重
- 这样在计算输出的时候就只用到了v对应的v1一直到v（t-1）的结果，而vt后面的东西并没有用到
- 所以mask的效果是在训练的时候，让第t个时间的query只看到对应的前面的那一些key、value对，使得在做预测的时候能够进行一一对应





multi-head

与其做一个单个的注意力函数，不如把整个query、key、value投影到一个低维，**投影h次，然后做h次的注意力函数，再将每一个函数并在一起，再投影回来得到最终的输出，如下图右图所示**

![img](https://i0.hdslb.com/bfs/note/235b323c873517333e90e3da0329e7aab3ff5da6.png@630w_!web-note.webp)

- 原始的value、key、query进入一个线性层（线性层将其投影到比较低的维度），然后再做一个scaled dot-product attention（如上图左图所示），做h次，得到h个输出，再把这些输出向量全部合并到一起，最后做一次线性的投影，然后回到multi-head attetion



为什么要做多头注意力机制？

- dot-product的注意力中没有什么可以学的参数，具体函数就是内积。有时候为了识别不一样的模式，希望有一些不一样的计算像素的办法
- 如果是用加型attention的话，里面其实是有一个权重可以学习到的，但是本文使用的是内积，它的做法是先投影到低维，这个投影的w是可以学的，也就是说，有h次机会希望可以学到不一样的投影方法，使得再投影进去的度量空间中能够匹配不同模式所需要的相似函数，然后最后把所得到的东西再做一次投影（这里有点像在卷积神经网络里面有多个输出通道的感觉）
- 具体的计算（公式如下图）：在multi-head的情况下，还是以前的Q、K、V，但是输出已经是不同头的输出做contact运算再投影到一个WO里面，对每一个头，就是把Q、K、V通过不同的可以学习的WQ、WK、WV投影到dv上面，再做注意力函数，然后再出来就可以了

![img](https://i0.hdslb.com/bfs/note/524ad9abd746aa1f05c549e42bb0ba567128b273.png@630w_!web-note.webp)

- 实际上h是等于8的，就是用8个头
- 注意力的时候，因为有残差连接的存在，使得输入和输出的维度是一样的，所以他投影的时候，投影的就是输出的维度除以h（因为输出维度是512，除以8之后，就是每一次把它投影到64维的维度，然后在这个维度上面计算注意力函数，最后再投影回来）
- 虽然公式中看起来有很多小矩阵的乘法，实际上在实现的时候也可以通过一次矩阵的乘法来实现（可以作为一个练习题来练习如何实现）





在transformer模型中是如何实现注意力的？

三种实现情况

![img](https://i0.hdslb.com/bfs/note/e5eabdd645598be803f6fe2e83b53866807351b0.png@630w_!web-note.webp)

- 上图中三个阴影表示三个注意力层，这三个注意力层各不相同



第一个注意力曾的使用：

![img](https://i0.hdslb.com/bfs/note/adda262c75f3b2b6214c7cd8027aaca973aa9139.png@630w_!web-note.webp)

- 上图中红色圈出来的部分是编码器的注意力层。编码器的输入（假设句子长度是n的话，他的输入其实是n个长为d的向量，每一个输入的词对应的是一个长为d的向量，一共有n个）

- 这个注意力层有三个输入，分别表示的是key、value、query。这里一根线复制成三根线表示同样一个东西，既作为key，也作为value和query，这个东西叫做自注意力机制，就是说key、value和query其实是一个东西，就是他自己本身

- 这里输入了n个query，每个query会得到一个输出，最终会得到n个输出，而且这个输出和value因为长度是一样的，所以输出的维度其实也是d，即输入和输出的大小其实是一样的，输出长也为n。

- 输出其实就是value的加权和，权重来自query和key

  49:48

- 假设不考虑多头和有投影的情况，输出其实是输入的加权和，权重来自于自己本身跟各个向量之间的相似度。如果有多头的话，因为有投影，会学习出h个不一样的距离空间出来，使得得出来的东西会有点不一样



第二个注意力层的使用：

![img](https://i0.hdslb.com/bfs/note/eb85c3be8b1398c59e37455e34eecf875cbdbbb6.png@630w_!web-note.webp)

- 如上图中红色圆圈圈出的部分所示
- 解码器也是一样的，也是同一个东西复制了三次
- 解码器的输入也是一样的，只是长度可能变成了m，维度其实也是一样的，所以它跟编码器一样的也是自注意力，唯一不一样的是里面有一个mask（mask的作用：在解码器计算query对应的输出的时候，不应该看到第t时刻后面的东西，意味着后面的东西要设为0）





第三个注意力层：

![img](https://i0.hdslb.com/bfs/note/372e3f43acc43abc457f882067609ef56ecc7255.png@630w_!web-note.webp)

- 如上图中红线箭头所指的部分

- 它不再是自注意力了，key和value来自编码器的输出，query来自解码器下一个attention的输入

- 编码器最后一层的输出就是n个长为d的向量

- 解码器的masked attention的输出是m个长为d的向量

- 编码器的输出作为key和value传入到这个注意力层中，解码器的下一层输出作为query传入到这个注意力层中，意味着对于解码器的每一个输出，作为query要计算出一个所要的输出，这个输出是来自于value的一个加权和（来自于编码器的输出的加权和，权重的粗细程度取决于query与编码器输出的相似度，如果相似度比较高的话，权重就会大一点，相反，如果相似度比较低的话，权重就会小一点）

- 这个attention中所要做的其实就是有效地把编码器里面的输出根据需要截取出来。例如

  53:20

- attention如何在编码器和解码器之间传递信息的时候起作用：根据在解码器输入的不同，根据当前的向量在编码器的输出里面挑选感兴趣的东西，也就是去注意感兴趣的东西，那些不那么感兴趣的东西就可以忽略掉





feed forward：



其实就是一个fully connected feed-forward network，就是一个MLP，但是不同之处在于他是applied to each position seperately and identically（就是把同一个MLP对每个词作用一次，即position-wise，说白了就是MLP只是作用在最后一个维度）

- position：输入序列中有很多个词，每一个词就是一个点，这些点就是position
- 具体公式如下图所示，xW1+b1就是一个线性层，max就是relu激活层，最后再有一个线性层

![img](https://i0.hdslb.com/bfs/note/aa19f938abdb89ea54e7f388d97a379c2c80b1f1.png@630w_!web-note.webp)

- 在注意力层的输入（每一个query对应的输出）的长为512，x就是一个512的向量，W1会把512投影成2048（等价于将他的维度扩大了4倍），以为最有有残差连接，所以还需要投影回去，所以W2又把2048投影回512
- 这其实就是一个单隐藏层的MLP，中间的隐藏层将输入扩大4倍，最后输出的时候又回到输入的大小（如果用pytorch来实现的话其实就是把两个线性层放在一起，而不需要改任何参数，因为pytorch在输入是3d的时候，默认就是在最后一个维度做计算）





整个transformer是如何抽取序列信息，然后把这些序列信息加工成最后所想要的语义空间向量？



56:53



首先考虑一个最简单的情况（没有残差连接、attention也是单头、没有投影），如下图所示

![img](https://i0.hdslb.com/bfs/note/288e6174d7c9e420fd77c7cda728e3c15ae09297.png@630w_!web-note.webp)

输入就是长为n的向量，在进入attention之后，就会得到同样长度的输出，最简单的attention其实就是对输入进行加权求和，加权和之后进入MLP，每个MLP对每一个输入的点做运算会得到输出，最后就得到整个transformer块的输出（输入和输出的大小都是一样的）

- 在整个过程中attetion所起到的作用就是把整个序列里面的信息抓取出来做一次汇聚（aggregation），因为已经抓取序列中感兴趣的信息，所以在做投影、MLP、映射成为更想要的语义空间的时候，因为加权之后的结果已经包含了序列种的信息，所以每个MLP只要再每个点独立进行运算就行了，因为序列信息已经汇聚完成，所以在做MLP的时候是可以分开做的



作为对比，RNN（没有隐藏层的MLP，纯线性）的实现过程

RNN的输入也是向量，对于第一个点也是做一个线性层

对于下一个点，如何利用序列信息，还是用之前的MLP（权重跟之前是一样的），但是时序信息（下图中绿色曲线表示之前的信息，蓝色曲线表示当前的信息）方面，是将上一个时刻的输出放回来作为输入的一部分与第二个点一起作为输入，这样就完成了信息的传递



RNN和transformer都是用一个线性层或者MLP来进行语义空间的转换，但是不同之处在于传递序列信息的方式：RNN十八上一时刻信息的输出传入下一时候做输入，但是在transformer中是通过attention层全局地拉取整个序列里面的信息，然后再用MLP做语义转换

- 他们的关注点都是如何有效地使用序列信息





embeding

因为输入是一个一个的词（或者叫词源，token），需要将其映射成向量。embeding的作用就是给任何一个词，学习一个长为d的向量来表示它（本文中d等于512）

- 编码器的输入需要embeding
- 解码器的输入也需要embeding
- 在softmax前面的线性也需要embeding

本文中这3个embeding是一样的权重，这样子训练起来会简单一点

另外还将权重乘了根号d：维度一大的话，权重值就会变小，之后要加上positional encoding，他不会随着长度变长把他的long固定住，因此乘上了根号d之后，使得他们在scale差不多

01:01:15







positional encoding

attention不会有时序信息，输出是value的加权和，权重是query和key之间的距离，和序列信息无关（也就是说给定一句话，把顺序任意打乱之后，attetion出来的结果都是一样的，顺序会变，但是值不会变，这样是存在问题的，所以需要把时序信息加入进来）

RNN是如何添加时序信息的？RNN将上一个时刻的输出作为下一个时刻的输入来传递历史信息

attention是在输入里面加入时序信息，将输入词所在的位置信息加入到输入里面（positional encoding），具体公式如下图所示

![img](https://i0.hdslb.com/bfs/note/c7647aebdd77684ce356c7e89d8ea81e28e0b913.png@630w_!web-note.webp)



01:03:23



- 在计算机中，数字是用一定长度的向量来表示的
- 词在嵌入层会表示成一个长为d的向量，同样用一个长为d的向量来表示数字，也就是词的位置，这些数字的不同计算方法是用周期不一样的sin和cos函数的值来算出来的，所以说任何一个值可以用长为d的向量来表示，然后这个长为d的记录了时序信息的向量和嵌入层相加，就完成了将时序信息加进数据中的操作，如下图中的红色线团所示，因为是cos和sin的函数所以是在+1和-1之间抖动的，所以乘了一个根号d，使得每个数字也是差不多在正负1的数值区间里面

![img](https://i0.hdslb.com/bfs/note/d23b1523d97fb0363e9e71f8a27f5d4aa3d41728.png@630w_!web-note.webp)









**4、为什么要用自注意力？**

相对于使用循环层或者卷积层，使用自注意力有多好

下表比较了四种不一样的层

![img](https://i0.hdslb.com/bfs/note/9c7cf0185f83d58bc0105949ec377ad7c8d8af49.png@630w_!web-note.webp)

- 第一个是自注意力层
- 第二个是循环层
- 第三个是卷积层
- 第四个是构造出来的受限的自注意力
- 第一列是计算复杂度，越低越好
- 第二列是顺序的计算，越少越好。指的是在算layer的时候，下一步计算必须要等前面多少步计算完成。相当于是说非并行度
- 第三列说的是信息从一个数据点走到另一个数据点要走多远，越短越好
- n是序列的长度
- d是向量的长度
- 整个自注意力就是几个矩阵做运算，其中一个矩阵运算时query矩阵（n行，n个query，列数是d，也就是维度是d）乘以key矩阵（也是n*d），两个矩阵相乘，算法复杂度就是n平方乘以d，别的矩阵运算复杂度也是一样的。因为只是牵涉到矩阵的运算，矩阵中可以认为并行度是比较高的，所以是o（1），最大长度是说从一个点的信息想跳到另一个点要走多少步，在attention里面，一个query可以跟所有的key做运算，而且输出是所有value的加权和，所以query跟任何一个很远的key、value对只要一次就能过来，所以长度是比较短的
- 对循环层来说，如果序列是乘了n的话就一个一个做运算，每个里面主要的计算就是n*n的矩阵的dense layer然后再乘以长为d的输入，所以是n平方，然后要做n次，所以是n*d平方
- 对比自注意力和RNN的计算复杂度，其实是有一定的区别的，取决于n大还是d大。在本文中d是512，n也差不多是几百，现在比较大的模型d可以做到2048甚至更大，n相对来说也会做的比较多，所以现在看起来其实这两种的计算复杂度是差不多的（n和d在差不多的数据上面），但是在循环的时候因为要一步一步做运算，当前时刻的值需要等待前面完成，所以导致了是一个长为n的序列化的操作，在并行上比较吃亏。在最初点的那个历史信息到最后一个点的时候需要走过n步才能过去，所以循环的最长距离是o（n），即RNN对特别长的序列的时候做的不够好，因为信息从一开始走，走着走着就走丢了，而不是像attention一样直接一步就能到
- 卷积在序列上的具体做法是用一个1d的卷积，所以它的kernel就是k，n是长度，d就是输入的通道数和输出的通道数，所以这里是k乘n乘d的平方。k一般不大（一般是3或者5，所以一般可以认为是常数），所以导致卷积的复杂度和RNN的复杂度差不多，但是卷积的好处在于只用卷积就完成了，并行度很高，所以卷积做起来通常比RNN要快一点，另外卷积每一次一个点是由一个长为k的窗口来看，所以信息在k距离内是能够一次完成传递的，如果超过k的话，传递信息就需要通过多层一层一层上去，但是由于是log的操作，所以也不会有太大的麻烦
- 最后一个层是在做注意力的时候，query只跟最近的r个邻居做运算，这样就不用算n平方了，但是问题在于这样的话，两个比较长的远一点的点需要走几步才能过来
- 一般来说使用attetion主要是关心对于特别长的序列是否能将整个信息揉的比较好一点所以在实际过程中，带限制的自注意力使用的不是特别多，一般都是用最原始的版本，而不做受限，所以基本就是考虑前三种层
- 实际中，当序列的长度和整个模型的宽度差不多而且深度都一样的话的时候，基本上前三个模型的算法复杂度都是差不多的，attention和卷积相对来说计算会好一点，attention在信息的糅合性上会好一点，所以用了self-attention看上去对长数据处理更好一些，但是实际上attention对整个模型做了更少的假设，导致需要更多的数据和更大的模型才能训练出来跟RNN和CNN同样的效果，所以导致现在基于transformer的模型都特别大

### 7、实验



训练的一些设置



训练数据集和batching

使用了两个任务：

- 英语翻德语标准的WMT 2014的数据，里面有4.5w个句子，使用了byte-pair encoding（bpe，不管是英语还是德语，一个词里面有很多种变化，如果直接把每个词做成一个token的话，会导致字典里面的东西会比较多，而且一个动词可能有好几种不同的变化形式，在做成不一样的词的时候，它们之间的区别模型是不知道的，bpe相对来说就是把词根给提出来，这样的好处是整个字典比较小），这里使用的是37000个token的字典，他是在英语和德语之间共享的，也就是说不再为英语构造一个字典也不再为德语构造一个字典，这样的好处是整个编码器和解码器的embeding就可以使用同一个东西，而且整个模型变得更加简单，也就是之前说的编码器和解码器的embeding是权重共享的
- 英语翻法语：使用了一个更大的数据集



硬件和schedule

- 训练使用了8个P100的GPU，后来使用tpu（tpu更适合做大的矩阵乘法）
- base模型使用小一点的参数，每一个batch训练的时间是0.4秒，一共训练了10w步，一共在8台gpu上训练了12个小时（基本上一个模型训练12个小时也是一个不错的性能）
- 这个大的模型，一个batch训练需要一秒钟，一共训练了30W步，最后一台机器训练了3.5天，其实也是一个可以承受的范围





在训练器上使用的是Adam

学习率是根据以下公式计算出来的：学习律是根据模型宽度的-0.5次方（就是说当模型越宽的时候，学习的向量越长的时候，学习率会低一点）

![img](https://i0.hdslb.com/bfs/note/de02350ab647617c8db3ff593db62d2e2ff8af3e.png@630w_!web-note.webp)

- 存在一个warmup，就是从一个小的值慢慢地爬到一个高的值，爬到之后，再根据步数按照0.5次方衰减，最后warmup是4000
- 学习率几乎是不用调的：第一，adam对学习率不敏感；学习率已经把模型考虑进来了，schedule也已经算是不错的schedule了，所以学习率是不需要调的





正则化

总共使用了三个正则化

residual dropout：对每一个子层（包括多头的注意力层和之后的MLP），在每个层的输出上，在进入残差连接之前和进入到layernorm之前，使用dropout（dropout率为0.1，也就是把输出的10%的那些元素值乘0.1，剩下的值乘1.1）。另外在输入加上词嵌入再加上position encoding的时候，也用了一个dropout。也就是说，基本上对于每一个带权重的层，在输出上都使用了dropout，虽然dropout率不是特别高，但是使用了大量的dropout层来对模型做正则化





label smoothing（inception v3）

使用softmax去学东西的时候，正确的标号是1，错误的标号是0，对于正确的label的softmax值，让他去逼近于1，但是softmax的值是很难去逼近于1的，因为他里面是指数，比较soft（需要输出接近无限大的时候，才能逼近于1），这样使得训练比较困难。

一般的做法是不要搞成特别难的0和1，可以把1的值往下降一点，比如降成0.9，本文中是直接降成了0.1，就是说对于正确的那个词，只需要softmax的输出（置信度）到0.1就可以了，而不需要特别高，剩下的值就可以是0.9除以字典大小，这里会损失perplexity（log lost做指数），基本上可以认为是模型的不确信度（正确答案只要10%是对的就行了，不确信度会增加，所以这个值会变高），但是在模型不那么确信的情况下会提升精度和BLUE的分数（这两个才是真正所要关心的点）





下表表示的是不同的超参数之间的对比

![img](https://i0.hdslb.com/bfs/note/ffd017934f4674fdc9a2fb7ad5610768e2e45d32.png@630w_!web-note.webp)

- n表示要堆多少层

- d表示模型的宽度，即token进来要表示成一个多长的向量

- dff表示MLP中间隐藏层的输出的大小

- h表示注意力层的头的个数

- dk、dv分别是一个头里面key和value的维度

- Pdrop表示dropout的丢弃率

- els表示最后label smoothing的时候要学的label的真实值是多少

- train steps表示要训练多少个batch

  01:19:54

- 整个模型参数相对来说还是比较简单的，基本上能调的就是上面的这些超参数剩下的东西基本上都是可以按比例算过来的，这也是transformer的一个好处，虽然看上出模型比较复杂，但是实际上没有太多可以调的东西，这个设计对后面的人来说相对更加方便一点



### 8、评价



写作

- 非常简洁，每一句上基本上在讲一个事情
- 没有使用太多的写作技巧
- 这种写法并不是很推荐，因为对一篇文章来说，需要讲一个故事来让读者有代入感，能够说服读者
- 一般说可以选择把东西减少一点，甚至把一些不那么重要的东西放在附录里面，但是在正文的时候，最好还是讲个故事：为什么做这些事情以及设计的理念、对整个文章的一些思考，这让会使得文章更加有深度



transformer模型

- 现在不仅仅是用在机器翻译上，也能够用在几乎所有的NLP任务上面，在bert，gpt，后续能够训练很大的预训练模型，能够极大提升所有NLP任务的性能，类似于CNN对整个计算机视觉的改变：能够训练一个大的CNN模型，使得别的任务也能够从中受益，CNN使得整个计算机视觉的研究者提供了一个同样的框架，使得只要学会CNN就行了，而不需要去管以前跟任务相关的海量专业知识（比如特征提取、任务建模等）
- 对于transformer来说，之前需要做各种各样的数据文本的预处理，然后根据NLP的任务设计不也一样的架构，现在不需要了，使用了整个transforner架构就能够在各个任务上取得非常好的成绩，而且它的预训练模型也让大家的训练变得更加简单
- 现在transformer不仅仅是用在自然语言上面，也在图片、语音、video上面取得了很大的进展
- 之前计算机视觉的研究者使用CNN，而在语言处理使用RNN，在别的方面用别的模型，现在发现同样一个模型能够在所有领域都能用，让大家的语言的一样了，任何一个领域的研究做的一些突破能够很快地在别的领域被使用，能够极大地减少一个新的技术在机器学习里面各个领域被应用的时间
- 人对世界的感知是多模态的：图片、文字、语音，现在transformer能够把这些所有不同的数据给融合起来，因为大家都用一个同样的架构抽取特征的话，就可以抽取到一个同样的语义空间，使得我们可以用文本、图片、语音、视频等训练更好更大的模型
- 虽然transformer这些模型取得了非常好的实验性的结果，但是对它的理解还是处于比较初级的阶段。
- 虽然标题说只需要attention就够了，但是最新的研究表明，attention只是在transformer里面起到把整个序列的信息聚合起来的作用，但是后面的MLP以及残差连接是缺一不可的，如果把这些东西去掉的话，attention基本上什么东西都训练不出来所以模型也不只是说只需要attention就够了
- attention根本就不会对数据的顺序做建模，为什么能够打赢RNN呢？RNN能够显示建模的序列信息理论上应该比MLP效果更好现在大家觉得它使用了一个更广泛的归纳偏置，使得他能够处理一些更一般化的信息，这也是为什么说attetion并没有做任何空间上的一些假设，它也能够跟CNN甚至是比CNN取得更好的结果，但是他的代价是因为他的假设更加一般，所以他对数据里面抓取信息的能力变差了，以至于说需要使用更多的数据、更大的模型，才能训练出想要的效果，这也是为什么现在transformer模型越来越大
- attention也给了研究者一些鼓励，原来CNN和RNN之外也会有新的模型能打败他们。现在也有一些工作说，就用MLP或者就用一些更简单的架构也能够在图片或者文本上面取得很好的结果
- 未来肯定会有很多新的架构出现，让整个领域更加有意思一些





