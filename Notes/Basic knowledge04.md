## 一、隐藏马尔可夫模型

**隐藏马尔可夫模型（Hidden Markov Model, HMM）** 是一种统计模型，用于处理序列数据，尤其擅长建模具有**隐含状态**的**时间序列或标记序列**问题。HMM广泛应用于语音识别、自然语言处理、生物信息学等领域。

**模型结构**

HMM 是由两个随机过程组成的：

​	1.	**隐含状态序列**：

​	•	==由一个马尔可夫过程产生的状态序列，但这些状态是“隐藏”的，即无法直接观测==。

​	•	状态转移遵循**马尔可夫性质**：当前状态只依赖于前一个状态。

​	•	状态转移由状态转移概率矩阵  描述，表示从状态  转移到状态  的概率。

​	2.	**观测序列**：

​	•	每个状态会根据某种概率分布生成一个可观测的输出（观测值）。

​	•	输出由观测概率矩阵  描述，表示在状态  时生成观测值  的概率。



此外，初始状态概率  描述了序列的起点状态分布。





## 二、Mamba  再叙

- **Vanilla Mamba**  基础版  

>  https://www.bilibili.com/video/BV1Xn4y1o7TE/?spm_id_from=333.999.0.0&vd_source=81e5007efea018d7c2e8c28374fcdf34



==自注意机制：两两相乘 求权重==  这里有个天然的缺陷，就是自注意力机制的计算范围**仅限于窗口内**，而无法直接处理窗口外的元素。像极了古语所说：两耳不闻窗外事，一心只读圣贤书。某种程度上造成视野狭窄，信息孤立，缺乏全局观一样，这种机制无法建模超出有限窗口的任何内容，看不到更长序列的世界。



TRM：就是通过位置编码把序列空间化 ，然后就通过计算空间相关度反向建模时序相关度这个过程中忽视了数据内在结构的细腻关联关系，而是采取了一种一视同仁的暴力关联模式，好处是直接简单，但显然参数效率低下，冗余度高，训练起来不易。




### 1、前期工作

 《Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers》 

**摘要：**

- 论文介绍了一种新的序列模型，称为线性状态空间层（Linear State-Space Layer, LSSL），==它通过模拟线性连续时间状态空间表示来映射序列数据==。
- LSSL结合了循环神经网络（RNN）、时序卷积和神经微分方程（NDE）的优势，同时解决了这些模型在建模能力和计算效率方面的不足。
- 理论上，LSSL与上述三种模型家族紧密相关，并继承了它们的优点，如将卷积推广到**连续时间**、解释RNN中的常见启发式方法，以及具有NDE的时间尺度适应性。
- 通过引入可训练的结构化矩阵A，LSSL具备了长时记忆能力。
- 实验结果表明，LSSL在处理长依赖关系的时序图像分类、真实世界的医疗回归任务和语音任务中取得了最先进的结果。

**引言：**

- 论文讨论了在机器学习中高效建模长序列数据的挑战，以及现有模型（RNN、CNN、NDE）的优缺点。

**技术背景：**

- 论文回顾了微分方程的基础知识，包括用于将连续时间模型转换为离散时间模型的近似方案，以及步长或时间尺度Δt的重要性。

**LSSL：**
- 定义了LSSL，**并讨论了如何从多个视角（递归、卷积、连续时间）计算它**。
- 证明了LSSL具有表达性，可以表示卷积和RNN。

**结合LSSL与连续时间记忆：**

- ==讨论了LSSL在处理长依赖关系时的主要限制==，并提出了通过特殊类别的结构化矩阵A来解决这些限制的方法。
- 提出了新的算法，使得具有这些矩阵A的LSSL在特定计算模型下可以理论上加速。

**实验评估：**

- 在多个时间序列数据集上测试了LSSL，包括非常长的序列，验证了其有效性。
- 与现有的RNN、CNN和NDE方法相比，LSSL在多个基准测试中取得了更好的结果。

**讨论：**

- <font color=red>论文讨论了LSSL与现有工作的关联，包括连续时间CNN、连续时间RNN以及RNN中的门控机制。</font>
- 论文还讨论了LSSL的局限性，如计算和空间复杂度问题，并提出了未来工作的方向。

**结论：**

- LSSL作为一种新的模型，能够处理非常长的序列数据，并在多个任务中取得了有希望的结果，为简单、原则性和少工程化的模型提供了新的可能性。

这篇论文的核心贡献在于提出了一种新的模型LSSL，它能够结合现有的主流时间序列模型的优点，并在理论上和实验上证明了其有效性。



- 连续变为离散   微分变为差分



> **时不变模型（Time-Invariant Model）** 是指其结构和参数不会随时间发生变化的数学或统计模型。换句话说，这种模型中的系统行为和规则在任意时间点都是一致的。





看图，**就是把长长的链条一下子弄成了输入输出直接对应的样子**，隐变量关联关系都跑到中间肚子里去了。先说结论：核心思想是用CNN对时序数据建模，==借助不同尺度的卷积核==，从不同时间尺度上捕获时序特征。数学上一番推导猛如虎之后能得到下面的公式

- SSM  通过卷积实现并行化，这是与RNN的区别



![image-20241121105506683](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121105506683.png)









- 在实际问题中 会对ABC  进行简化，设成更简单对角阵方便计算，这就是所谓的结构化SSM,S4模型



- 对应原文，我们已经讲完了简介和第二部分。你可能会问，看上去并行SSM就按好的啊，为啥不行呢？别忘了，这个系统还有两个强假设：
  <font color=red>线性+不变</font>极大的限制了它的应用范围，因为实际系统大多为非线性、时变系统。Mamba本质上就是一个SSM模型的改进版，放开了这两个约束。

### 2、选择性 SSM





接下来咱们着重讲解什么是选择性SSM。Mamba主要体现在设计了一种机制，让状态空间具备选择性，达到了Transformers的建模能力，同时在序列长度上实现了线性扩展，也就是克服了Transformers缺陷。可处理最长达百万长度的序列，而具效率贼高，与GPU硬件适配，Transformers:5倍，准确率相当甚至更好。这就是它为啥牛逼起来的原因啦。





![image-20241121150900219](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121150900219.png)

核心就是搞懂这幅图，其实大致看看就明白，在时间序列模型中间设计了这么一坨**非常类似LSMT的门结构，实现所谓的选择性**，BC都带了t变成了<font color=red>时变参数</font>，A虽然没有直接含t,但其实也是时变的了。下面的蓝色部分就是所谓的选择机制，这个delta t别小瞧，**它就是前面离散函数**，一会有大用。要真正明白其中的细节，先要从增加选择性机制开始讲，促其实简单理解就是把整个系统该用：
一个总开关+若干个旋钮=非线性时变系统

下面的解释可以认为是对这种选择的合理化。

### 3、要解决的问题

从某种角度看，==序列建模的核心就是研究如何将长序列的上下文信息压缩到一个较小的状态中==。比如，语言模型实际上就是在一个有限的词汇集合中不断进行转换。有统计表明，3500多个常用中文字，3000个常用英文词能覆盖90%以上的日常用语。<font color=red>transformer的注意力机制虽然很有效，但效率低，因为它需要存储整个上下文，导致推理和训练时间较长</font>。前面讲的SSM递归模型因为它们的状态是有限的（单纯时不变导致)，效率高但有效性受限于状态的压缩能力。



![image-20241121152303950](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121152303950.png)





| **特性**       | **GRU**                            | **LSTM**                                 |
| -------------- | ---------------------------------- | ---------------------------------------- |
| **结构复杂度** | 简单，只有两个门（更新门、重置门） | 复杂，有三个门（输入门、遗忘门、输出门） |
| **参数数量**   | 较少                               | 较多                                     |
| **计算效率**   | 更快                               | 较慢                                     |
| **性能**       | 表现与LSTM接近，在某些任务上略优   | 在需要捕获更长时间依赖时表现更好         |
| **适用场景**   | 较小的数据集或较简单的任务         | 大规模数据集或较复杂的任务               |

![image-20241121153215474](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121153215474.png)





前面的（CNN）全局卷积虽然能用不同的卷积核进行时序特征捕捉，**但是缺内容感知**，也就是不知道输入的重点和逻辑。而transformer人家本身是没有这些限制的，**别说时不变，连线性系统假设都没有**。这么一分析，改进的方向就很明确了哈。放开LTI模型的时不变约束，让模型参数依赖于输入内容不就行了吗？说起来简单，具体看看是怎么实现的。





注意：这里的B/L/N/D 符号乍一看让人很困惑，其实就是==张量的维度==。
B:批次大小(Batch size)。表示一次输入的数据量的大小。
L:序列长度(Sequence length)。表示每个序列中包含的时间步数。
N:特征维度(Feature dimension)。表示每个时间步的特征数量。
D:输入特征维度(Input feature dimension)。



其中步长大小**deta像是个放大镜观察窗口**，影响信息处理的焦点。<font color=red>步长较小时，模型倾向于忽略具体的单词，而更多地依赖于之前的上下文信息</font>。你可以简单的认为，就是靠着它，实现了注意力的选择。拿着放大镜忽远忽近的看。



![image-20241121161619886](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121161619886.png)





### 4、实现的细节



- 这部分主要研究如何==充分利用GPU实现选择性SSM的并行计算==，也就是前面的图和算法。原文细节很多，大量引用了前人的工作，读起来有些费劲，尤其在没有阅读之前文章的情况下。简单说，就是努力解决好“**既要又要**“的问题，要立又要当，<font color=blue>立住表现力强的人设需要隐状态维度够大</font>，而要当，<font color=red>速度和内存不能牺牲</font>。为此，提出了三种创新的解决方案：**内核融合、并行扫描和重计算**。**所谓的内核融合：就是把离散化和循环在GPU SRAM内存中实现**，快！然后加载和存储参数ABC矩阵都用HBM高带宽内存。这俩简写对GPU不熟的同学可能不知道，其实就是一种分层提升效率的新技术，前者快单内存带宽小，后者慢但是带宽大。这是随着AI崛起的芯片新技术。



Mamba  的构成



![image-20241121195131874](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121195131874.png)







水管系统的图封装到 Selective SSM



![image-20241121194759349](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121194759349.png)







1.Transformer的死穴：<font color=red>注意力机制的窗口小了效果差，大了计算复杂度平方暴张两难</font>。时序问题空间化出现了瓶颈，单纯的注意力机制有缺陷，并非万能，不是机制本身有问题，而是实现方式。这促使人们思考更换视角。

2.SSM模型：==从LTI连续空间线性时不变系统讲起==，类似RNN离散化，CNN并行化，但都没离开线性+参数矩阵时不变两个假设。

3.Mamba原理：提出了选择机制，**其实就是时序门控单元**。通过离散化函数delta这个非线性总开关控制ABC矩阵三个小旋钮开关，相当于放开了时不变约束，实现时变。与此同时，在选择性SSM这个核心模块之外，还通过增加激活函数进一步提升模型表征力，利用卷积层增强空间特征捕捉力。可以认为Mamba=RNN(变形GRU)+CNN+选择性注意力机制

4,思想精髓：流体力学系统+李指数映射+固定矩阵A最优主管道=独特的处理方法





## 三、Mamba out



整个摘要的重点，也就是结论性的东西作者其实用斜体给你标出来了：
1、long-sequence and auto regressive 这方面Mamba依然擅长，承认优点.
2.图像分类不是auto regressive自回归任务， 也不是long-sequence,因此用不着Mamba,所以MambaOut,比如在ImageNet分类任务上
3.还有第三个结论也很有意思，即使是视觉领域，目标检测和实例分割任务上Mamba还OUT不了，依然有潜力。明白了吗？



#### 1、研究重点

整篇文章的研究重点其实就是前言中几行斜体字：
Do we really need Mamba for Vision？视觉问题真得需要Mamba模型吗

Hypothesis1 *:SSM对于图像分类没有必要*，**因为该任务既不具有长序列特征也不具有自回归特征**。
Hypothesis2 :SSM可能对对象检测和实例分割有潜在好处，因为这些任务具有长序列特征，但不具有自回归特征。
这些在摘要中讲过，重要的是三个问题：怎么分析的，模型怎么实现的，以及怎么用实验证明
的。





第二部分相关工作简要小结了**Transformer典型模型BERT和GPT系列**，以及VIT强调了**Transformer中的注意力模块会随序列长度增加而扩展**，带来显著的计算挑战。许多研究探索了各种策略来缓解这一问题，如低秩方法、内核化、tokn混合范围限制和历史记忆压缩。这都是水文章的号方向。



最近，RNN-ike方法（特别是**RWKV**和Mamba)因其在大规模语言模型中的出色表现而受到关注，这点到目前为止还是毋庸置疑的。

这里的分析也佐证了梗直哥的观点：上次讲Mamba时提到的，对于Transformer的改进或者说平替，现在学界的一种典型思路就是==回归传统模型==，从故纸堆里找灵感。这篇文章的作者显然也认同这种观点，而目直接露骨的把它们称作RNN-k方法，**其实最新的还有xLSTM**。但这种视角还是浅了，仅仅是从模型结构视角来看，做一种时序回归而已。今天梗直哥还会带你从更高的记忆本质视角来分析。



第二段小结了Mamba最新的各种变体，包括Vision Mamba整合了SSM来开发类似VIT的等向性视觉模型；VMamba则利用Mamba构建类似AlexNet和ResNet的分层视觉模型：**LocalMamba通过引入局部l归纳偏置**来增强视觉Mamba模型；

PlainMamba旨在进一步提升等向性Mamba模型的性能，还有EfficientVMamba等等。你看事实上大家这半年来以及像吸血鬼一样迅速扑上去搞Mamba了，把它作为Transformer的平替。而这篇文章试图把自己打扮成"半血猎人"来拯救世界。你说它是Mamba吧，它说自己不是，你说它就是CNN吧，它非要把自己和Mamba比，还起了这么个名。有意思，也很拧巴。到底为什么，咱们来看核心原理部分

#### 2、核心原理



其实用流体力学视角看Mamba更透像本质上就是当成一个记忆流淌的管道系统  Selective SSM就是个带着总开关deta+两个阀门BC+主管道A的系统。因为A与时间无关，因此隐藏状态h可以视为固定大小的记忆，存储所有历史信息。固定大小意味着记忆不可避免地丢失，但保证了与当前输入集成的计算复杂性保持不变。而通过总开关dlta+两个阀门BC门控机制实现了一种选择性注意力机制。这种设计更加的高效，从更抽象的数学角度理解，是用李指数映射拟合数据，替换了原有的牛顿力学运动方程。



#### 3、自注意机制的类别



相比之下，Transformer中的自注意力机制更加复杂，如下图有两种：<font color=red>一种叫因果模式</font>，其实就是只能看过去，不能看未来，只有记忆没有未卜先知；<font color=blue>另一种是全可见模式</font>，左右都知道。Transformer本质上两种都可以，因果模式的比如GPT,全可见的比如BERT,前者适合自回归用来生成和预测，以史为鉴，后者适合理解，左顾右看瞻前顾后。



![image-20241121221002498](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221002498.png)



按照这种分类，Mamba的选择性机制算那种呢？显然不是全可见模式，看公式就知道是因果模式，但和Transformer的有什么不同呢？
 $\begin{aligned}&h_t=\overline{\mathbf{A}}h_{t-1}+\overline{\mathbf{B}}x_t,\\&y_t=\mathbf{C}h_t,\end{aligned}$
下图展示了Transformer因果注意力与Mamba中因果注意力的区别，<font color=red>前者是组合（叠加)之前所有的记忆</font>，记忆无损但复杂度增加，越累越长，计算复杂度同样为O(L^):后者合之前的记忆到新的隐藏状态，记忆有损但复杂度恒定

![image-20241121221220425](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221220425.png)

![image-20241121221140767](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221140767.png)





基于Mamba的这种特性，显然它适用于以下特征的任务： 

● ==特征1：任务涉及处理因为复杂度低，更高效==
● 特征2：任务需要因果token混合模式

但这样以来还怎么OUT呀！于是他们反向思考：
首先，什么时候不需要长序列呢？视觉作为空间数据，那种最不需要呢？你说是鸡蛋里挑骨头也好，逆向思维也好。既然逻辑上它擅长长序列，那就说明短序列一般，那咱们就摁着短序列搞不就成了。
其次，什么时候不需要因果注意力呢？什么问题需要全局可见注意力呢？摁着这个方向搞，不也能证明Mamba不行吗？
这种创新的思维方式确实聪明，典型的田忌赛马思路，你打你的，我打我的，拉到我擅长的地方打，你还打得过吗？





#### 4、视觉任务特点分析

在视觉识别任务中，感觉上==图像分就不属于长序列任务==，<font color=red>因为主要关注整体特征空间特征就够了，目标也只是粗狂类别标号，因此不涉及什么序列信息，而目需要全局信息</font>。但是目标检测和语义分割则不一定，比如要考虑边缘的连贯性，因此可能有序列问题。但是，这种假设或者感觉怎么证明呢？
首先，文章针对图像分类任务，做了全可见模式和因果模式性能的分析实验。如图所示：





![image-20241121224431112](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121224431112.png)





左图是全可见模式，横纵轴互相都能看BERT和MT的自注意力机制都是这种。中间图是因果模式，GPT的自注意力Mamba的SSM是这种，比如y3只能看到x1-x3,看不见x4-x5。右图显示以VT为例，将自注意力机制从全可见模式切换到因果模式后，性能有所下降，说明对于图像分类问题，用因果模式没必要。

既然注意力机制的类型明确了，**在图像分类这一亩三分地上干掉Mamba的可能性暴增**。但老问题又回来了，怎么确定它是不是长序列任务呢？整篇文章最有点数学理论含量，也是最有看点的就是3.2关于图像处理任务是否属于长序列问题的分析。这部分没点基出则很容易一片汇然，不伯，有





![image-20241122202029837](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241122202029837.png)



#### 5、VIT

通过定理分析

![image-20241122223828191](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241122223828191.png)

#### 6、模型架构

下图展示于MambaOut模型的总体框架以及Gated CNN块的具体结构。整体框架类似ResNet,通过降采样逐步减少特征图的尺寸，同时增加特征的抽象层次。





MambaOut 的架构与Swin Transformer和 **DenseNet在分层结构和降采样方面有相似之处**，==但在特征提取和信息混合机制上有所不同==。MambaOut使用**Gated CNN块**，而**Swin Transformer使用窗口注意力机制的Transformer块**，DenseNet.则使用密集连接的卷积层。这些差异决定了它们在处理不同任务时的特性和优势。





> 结论：如果说Mamba是回归RNN+新型注意力机制，那MambaOut其实是回归CNN+新型注意力机制。

## 四、RWKV



RWKV（Receptance-Weighted Key-Value）是一种==新型序列建模架构==，融合了**Transformer**和**RNN**的优点，主要用于自然语言处理（NLP）等任务。它的目标**是解决传统Transformer在长序列建模中的效率问题，同时保持RNN处理序列的顺序灵活性。**



RWKV是由 **BlinkDL** 开发的一种模型架构，<font color=red>它以Transformer的注意力机制为基础，同时引入了RNN的权重更新机制</font>。



**RWKV 的核心特性**



​	1.	**RNN与Transformer的结合**

​	•	**RNN特性**：==能够逐步处理序列，节省内存，同时自然地支持在线预测（即一次处理一个时间步）==。

​	•	**Transformer特性**：<font color=blue>通过注意力机制增强全局依赖捕获的能力</font>。

​	2.	**高效的序列处理**

​	•	RWKV是**时间步递归的**，不像传统的Transformer需要对整个序列进行并行计算。

​	•	内存和计算需求更低，尤其适合长序列建模。

​	3.	**更低的硬件需求**

​	•	RWKV在训练和推理阶段占用的显存更少，因此在资源有限的设备上（如单卡GPU）也能运行。

​	4.	**支持在线推理（Streaming）**

​	•	RWKV支持增量计算，可以实时处理序列流数据，而不需要一次性读取整个输入序列。



**RWKV的工作原理**



RWKV的基本思想是模仿RNN的序列计算方式，同时用Transformer中的键（Key）、值（Value）对序列信息进行建模。其计算过程包括以下几个步骤：

​	1.	**Receptance（接收权重）**

类似RNN中的门控机制，控制如何更新隐藏状态。



​	2.	**Key 和 Value 的计算**

使用注意力机制更新隐藏状态：



​	3.	**加权更新**

将Receptance的权重与Key-Value对结合，更新隐藏状态。



​	4.	**输出结果**

最终的隐藏状态可以作为序列处理的输出。



**RWKV与其他模型的对比**

 

| **特性**           | **RWKV** | **Transformer** | **RNN**  |
| ------------------ | -------- | --------------- | -------- |
| **并行化**         | 部分支持 | 完全支持        | 不支持   |
| **内存占用**       | 较低     | 较高            | 较低     |
| **长序列建模能力** | 良好     | 优秀            | 较差     |
| **实时推理**       | 支持     | 不支持          | 支持     |
| **复杂度**         | \(O(n)\) | \(O(n^2)\)      | \(O(n)\) |
| **训练难度**       | 中等     | 高              | 低       |

**总结**



RWKV是一种创新的序列建模架构，结合了Transformer和RNN的优势。它在长序列建模中的效率和灵活性使其成为一种具有前景的模型架构，尤其是在内存受限或需要在线推理的场景中。





![image-20241121224827942](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121224827942.png)



