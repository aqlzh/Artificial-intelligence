## 一、隐藏马尔可夫模型

**隐藏马尔可夫模型（Hidden Markov Model, HMM）** 是一种统计模型，用于处理序列数据，尤其擅长建模具有**隐含状态**的**时间序列或标记序列**问题。HMM广泛应用于语音识别、自然语言处理、生物信息学等领域。

**模型结构**

HMM 是由两个随机过程组成的：

​	1.	**隐含状态序列**：

​	•	==由一个马尔可夫过程产生的状态序列，但这些状态是“隐藏”的，即无法直接观测==。

​	•	状态转移遵循**马尔可夫性质**：当前状态只依赖于前一个状态。

​	•	状态转移由状态转移概率矩阵  描述，表示从状态  转移到状态  的概率。

​	2.	**观测序列**：

​	•	每个状态会根据某种概率分布生成一个可观测的输出（观测值）。

​	•	输出由观测概率矩阵  描述，表示在状态  时生成观测值  的概率。



此外，初始状态概率  描述了序列的起点状态分布。





## 二、Mamba  再叙

- **Vanilla Mamba**  基础版  

>  https://www.bilibili.com/video/BV1Xn4y1o7TE/?spm_id_from=333.999.0.0&vd_source=81e5007efea018d7c2e8c28374fcdf34



==自注意机制：两两相乘 求权重==  这里有个天然的缺陷，就是自注意力机制的计算范围**仅限于窗口内**，而无法直接处理窗口外的元素。像极了古语所说：两耳不闻窗外事，一心只读圣贤书。某种程度上造成视野狭窄，信息孤立，缺乏全局观一样，这种机制无法建模超出有限窗口的任何内容，看不到更长序列的世界。



TRM：就是通过位置编码把序列空间化 ，然后就通过计算空间相关度反向建模时序相关度这个过程中忽视了数据内在结构的细腻关联关系，而是采取了一种一视同仁的暴力关联模式，好处是直接简单，但显然参数效率低下，冗余度高，训练起来不易。




### 1、前期工作

 《Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers》 

**摘要：**

- 论文介绍了一种新的序列模型，称为线性状态空间层（Linear State-Space Layer, LSSL），==它通过模拟线性连续时间状态空间表示来映射序列数据==。
- LSSL结合了循环神经网络（RNN）、时序卷积和神经微分方程（NDE）的优势，同时解决了这些模型在建模能力和计算效率方面的不足。
- 理论上，LSSL与上述三种模型家族紧密相关，并继承了它们的优点，如将卷积推广到**连续时间**、解释RNN中的常见启发式方法，以及具有NDE的时间尺度适应性。
- 通过引入可训练的结构化矩阵A，LSSL具备了长时记忆能力。
- 实验结果表明，LSSL在处理长依赖关系的时序图像分类、真实世界的医疗回归任务和语音任务中取得了最先进的结果。

**引言：**

- 论文讨论了在机器学习中高效建模长序列数据的挑战，以及现有模型（RNN、CNN、NDE）的优缺点。

**技术背景：**

- 论文回顾了微分方程的基础知识，包括用于将连续时间模型转换为离散时间模型的近似方案，以及步长或时间尺度Δt的重要性。

**LSSL：**
- 定义了LSSL，**并讨论了如何从多个视角（递归、卷积、连续时间）计算它**。
- 证明了LSSL具有表达性，可以表示卷积和RNN。

**结合LSSL与连续时间记忆：**

- ==讨论了LSSL在处理长依赖关系时的主要限制==，并提出了通过特殊类别的结构化矩阵A来解决这些限制的方法。
- 提出了新的算法，使得具有这些矩阵A的LSSL在特定计算模型下可以理论上加速。

**实验评估：**

- 在多个时间序列数据集上测试了LSSL，包括非常长的序列，验证了其有效性。
- 与现有的RNN、CNN和NDE方法相比，LSSL在多个基准测试中取得了更好的结果。

**讨论：**

- <font color=red>论文讨论了LSSL与现有工作的关联，包括连续时间CNN、连续时间RNN以及RNN中的门控机制。</font>
- 论文还讨论了LSSL的局限性，如计算和空间复杂度问题，并提出了未来工作的方向。

**结论：**

- LSSL作为一种新的模型，能够处理非常长的序列数据，并在多个任务中取得了有希望的结果，为简单、原则性和少工程化的模型提供了新的可能性。

这篇论文的核心贡献在于提出了一种新的模型LSSL，它能够结合现有的主流时间序列模型的优点，并在理论上和实验上证明了其有效性。



- 连续变为离散   微分变为差分



> **时不变模型（Time-Invariant Model）** 是指其结构和参数不会随时间发生变化的数学或统计模型。换句话说，这种模型中的系统行为和规则在任意时间点都是一致的。





看图，**就是把长长的链条一下子弄成了输入输出直接对应的样子**，隐变量关联关系都跑到中间肚子里去了。先说结论：核心思想是用CNN对时序数据建模，==借助不同尺度的卷积核==，从不同时间尺度上捕获时序特征。数学上一番推导猛如虎之后能得到下面的公式

- SSM  通过卷积实现并行化，这是与RNN的区别



![image-20241121105506683](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121105506683.png)









- 在实际问题中 会对ABC  进行简化，设成更简单对角阵方便计算，这就是所谓的结构化SSM,S4模型



- 对应原文，我们已经讲完了简介和第二部分。你可能会问，看上去并行SSM就按好的啊，为啥不行呢？别忘了，这个系统还有两个强假设：
  <font color=red>线性+不变</font>极大的限制了它的应用范围，因为实际系统大多为非线性、时变系统。Mamba本质上就是一个SSM模型的改进版，放开了这两个约束。

### 2、选择性 SSM





接下来咱们着重讲解什么是选择性SSM。Mamba主要体现在设计了一种机制，让状态空间具备选择性，达到了Transformers的建模能力，同时在序列长度上实现了线性扩展，也就是克服了Transformers缺陷。可处理最长达百万长度的序列，而具效率贼高，与GPU硬件适配，Transformers:5倍，准确率相当甚至更好。这就是它为啥牛逼起来的原因啦。





![image-20241121150900219](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121150900219.png)

核心就是搞懂这幅图，其实大致看看就明白，在时间序列模型中间设计了这么一坨**非常类似LSMT的门结构，实现所谓的选择性**，BC都带了t变成了<font color=red>时变参数</font>，A虽然没有直接含t,但其实也是时变的了。下面的蓝色部分就是所谓的选择机制，这个delta t别小瞧，**它就是前面离散函数**，一会有大用。要真正明白其中的细节，先要从增加选择性机制开始讲，促其实简单理解就是把整个系统该用：
一个总开关+若干个旋钮=非线性时变系统

下面的解释可以认为是对这种选择的合理化。

### 3、要解决的问题

从某种角度看，==序列建模的核心就是研究如何将长序列的上下文信息压缩到一个较小的状态中==。比如，语言模型实际上就是在一个有限的词汇集合中不断进行转换。有统计表明，3500多个常用中文字，3000个常用英文词能覆盖90%以上的日常用语。<font color=red>transformer的注意力机制虽然很有效，但效率低，因为它需要存储整个上下文，导致推理和训练时间较长</font>。前面讲的SSM递归模型因为它们的状态是有限的（单纯时不变导致)，效率高但有效性受限于状态的压缩能力。



![image-20241121152303950](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121152303950.png)





| **特性**       | **GRU**                            | **LSTM**                                 |
| -------------- | ---------------------------------- | ---------------------------------------- |
| **结构复杂度** | 简单，只有两个门（更新门、重置门） | 复杂，有三个门（输入门、遗忘门、输出门） |
| **参数数量**   | 较少                               | 较多                                     |
| **计算效率**   | 更快                               | 较慢                                     |
| **性能**       | 表现与LSTM接近，在某些任务上略优   | 在需要捕获更长时间依赖时表现更好         |
| **适用场景**   | 较小的数据集或较简单的任务         | 大规模数据集或较复杂的任务               |

![image-20241121153215474](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121153215474.png)





前面的（CNN）全局卷积虽然能用不同的卷积核进行时序特征捕捉，**但是缺内容感知**，也就是不知道输入的重点和逻辑。而transformer人家本身是没有这些限制的，**别说时不变，连线性系统假设都没有**。这么一分析，改进的方向就很明确了哈。放开LTI模型的时不变约束，让模型参数依赖于输入内容不就行了吗？说起来简单，具体看看是怎么实现的。





注意：这里的B/L/N/D 符号乍一看让人很困惑，其实就是==张量的维度==。
B:批次大小(Batch size)。表示一次输入的数据量的大小。
L:序列长度(Sequence length)。表示每个序列中包含的时间步数。
N:特征维度(Feature dimension)。表示每个时间步的特征数量。
D:输入特征维度(Input feature dimension)。



其中步长大小**deta像是个放大镜观察窗口**，影响信息处理的焦点。<font color=red>步长较小时，模型倾向于忽略具体的单词，而更多地依赖于之前的上下文信息</font>。你可以简单的认为，就是靠着它，实现了注意力的选择。拿着放大镜忽远忽近的看。



![image-20241121161619886](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121161619886.png)





### 4、实现的细节



- 这部分主要研究如何==充分利用GPU实现选择性SSM的并行计算==，也就是前面的图和算法。原文细节很多，大量引用了前人的工作，读起来有些费劲，尤其在没有阅读之前文章的情况下。简单说，就是努力解决好“**既要又要**“的问题，要立又要当，<font color=blue>立住表现力强的人设需要隐状态维度够大</font>，而要当，<font color=red>速度和内存不能牺牲</font>。为此，提出了三种创新的解决方案：**内核融合、并行扫描和重计算**。**所谓的内核融合：就是把离散化和循环在GPU SRAM内存中实现**，快！然后加载和存储参数ABC矩阵都用HBM高带宽内存。这俩简写对GPU不熟的同学可能不知道，其实就是一种分层提升效率的新技术，前者快单内存带宽小，后者慢但是带宽大。这是随着AI崛起的芯片新技术。



Mamba  的构成



![image-20241121195131874](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121195131874.png)







水管系统的图封装到 Selective SSM



![image-20241121194759349](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121194759349.png)







1.Transformer的死穴：<font color=red>注意力机制的窗口小了效果差，大了计算复杂度平方暴张两难</font>。时序问题空间化出现了瓶颈，单纯的注意力机制有缺陷，并非万能，不是机制本身有问题，而是实现方式。这促使人们思考更换视角。

2.SSM模型：==从LTI连续空间线性时不变系统讲起==，类似RNN离散化，CNN并行化，但都没离开线性+参数矩阵时不变两个假设。

3.Mamba原理：提出了选择机制，**其实就是时序门控单元**。通过离散化函数delta这个非线性总开关控制ABC矩阵三个小旋钮开关，相当于放开了时不变约束，实现时变。与此同时，在选择性SSM这个核心模块之外，还通过增加激活函数进一步提升模型表征力，利用卷积层增强空间特征捕捉力。可以认为Mamba=RNN(变形GRU)+CNN+选择性注意力机制

4,思想精髓：流体力学系统+李指数映射+固定矩阵A最优主管道=独特的处理方法





## 三、Mamba out

>   https://arxiv.org/pdf/2405.07992   

整个摘要的重点，也就是结论性的东西作者其实用斜体给你标出来了：
1、long-sequence and auto regressive 这方面Mamba依然擅长，承认优点.
2.图像分类不是auto regressive自回归任务， 也不是long-sequence,因此用不着Mamba,所以MambaOut,比如在ImageNet分类任务上
3.还有第三个结论也很有意思，即使是视觉领域，目标检测和实例分割任务上Mamba还OUT不了，依然有潜力。明白了吗？



#### 1、研究重点

整篇文章的研究重点其实就是前言中几行斜体字：
Do we really need Mamba for Vision？视觉问题真得需要Mamba模型吗

Hypothesis1 *:SSM对于图像分类没有必要*，**因为该任务既不具有长序列特征也不具有自回归特征**。
Hypothesis2 :SSM可能对对象检测和实例分割有潜在好处，因为这些任务具有长序列特征，但不具有自回归特征。
这些在摘要中讲过，重要的是三个问题：怎么分析的，模型怎么实现的，以及怎么用实验证明
的。





第二部分相关工作简要小结了**Transformer典型模型BERT和GPT系列**，以及VIT强调了**Transformer中的注意力模块会随序列长度增加而扩展**，带来显著的计算挑战。许多研究探索了各种策略来缓解这一问题，如低秩方法、内核化、tokn混合范围限制和历史记忆压缩。这都是水文章的号方向。



最近，RNN-ike方法（特别是**RWKV**和Mamba)因其在大规模语言模型中的出色表现而受到关注，这点到目前为止还是毋庸置疑的。

这里的分析也佐证了梗直哥的观点：上次讲Mamba时提到的，对于Transformer的改进或者说平替，现在学界的一种典型思路就是==回归传统模型==，从故纸堆里找灵感。这篇文章的作者显然也认同这种观点，而目直接露骨的把它们称作RNN-k方法，**其实最新的还有xLSTM**。但这种视角还是浅了，仅仅是从模型结构视角来看，做一种时序回归而已。今天梗直哥还会带你从更高的记忆本质视角来分析。



第二段小结了Mamba最新的各种变体，包括Vision Mamba整合了SSM来开发类似VIT的等向性视觉模型；VMamba则利用Mamba构建类似AlexNet和ResNet的分层视觉模型：**LocalMamba通过引入局部l归纳偏置**来增强视觉Mamba模型；

PlainMamba旨在进一步提升等向性Mamba模型的性能，还有EfficientVMamba等等。你看事实上大家这半年来以及像吸血鬼一样迅速扑上去搞Mamba了，把它作为Transformer的平替。而这篇文章试图把自己打扮成"半血猎人"来拯救世界。你说它是Mamba吧，它说自己不是，你说它就是CNN吧，它非要把自己和Mamba比，还起了这么个名。有意思，也很拧巴。到底为什么，咱们来看核心原理部分

#### 2、核心原理



其实用流体力学视角看Mamba更透像本质上就是当成一个记忆流淌的管道系统  Selective SSM就是个带着总开关deta+两个阀门BC+主管道A的系统。因为A与时间无关，因此隐藏状态h可以视为固定大小的记忆，存储所有历史信息。固定大小意味着记忆不可避免地丢失，但保证了与当前输入集成的计算复杂性保持不变。而通过总开关dlta+两个阀门BC门控机制实现了一种选择性注意力机制。这种设计更加的高效，从更抽象的数学角度理解，是用李指数映射拟合数据，替换了原有的牛顿力学运动方程。



#### 3、自注意机制的类别



相比之下，Transformer中的自注意力机制更加复杂，如下图有两种：<font color=red>一种叫因果模式</font>，其实就是只能看过去，不能看未来，只有记忆没有未卜先知；<font color=blue>另一种是全可见模式</font>，左右都知道。Transformer本质上两种都可以，因果模式的比如GPT,全可见的比如BERT,前者适合自回归用来生成和预测，以史为鉴，后者适合理解，左顾右看瞻前顾后。



![image-20241121221002498](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221002498.png)



按照这种分类，Mamba的选择性机制算那种呢？显然不是全可见模式，看公式就知道是因果模式，但和Transformer的有什么不同呢？
 $\begin{aligned}&h_t=\overline{\mathbf{A}}h_{t-1}+\overline{\mathbf{B}}x_t,\\&y_t=\mathbf{C}h_t,\end{aligned}$
下图展示了Transformer因果注意力与Mamba中因果注意力的区别，<font color=red>前者是组合（叠加)之前所有的记忆</font>，记忆无损但复杂度增加，越累越长，计算复杂度同样为O(L^):后者合之前的记忆到新的隐藏状态，记忆有损但复杂度恒定

![image-20241121221220425](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221220425.png)

![image-20241121221140767](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121221140767.png)





基于Mamba的这种特性，显然它适用于以下特征的任务： 

● ==特征1：任务涉及处理因为复杂度低，更高效==
● 特征2：任务需要因果token混合模式

但这样以来还怎么OUT呀！于是他们反向思考：
首先，什么时候不需要长序列呢？视觉作为空间数据，那种最不需要呢？你说是鸡蛋里挑骨头也好，逆向思维也好。既然逻辑上它擅长长序列，那就说明短序列一般，那咱们就摁着短序列搞不就成了。
其次，什么时候不需要因果注意力呢？什么问题需要全局可见注意力呢？摁着这个方向搞，不也能证明Mamba不行吗？
这种创新的思维方式确实聪明，典型的田忌赛马思路，你打你的，我打我的，拉到我擅长的地方打，你还打得过吗？





#### 4、视觉任务特点分析

在视觉识别任务中，感觉上==图像分就不属于长序列任务==，<font color=red>因为主要关注整体特征空间特征就够了，目标也只是粗狂类别标号，因此不涉及什么序列信息，而目需要全局信息</font>。但是目标检测和语义分割则不一定，比如要考虑边缘的连贯性，因此可能有序列问题。但是，这种假设或者感觉怎么证明呢？
首先，文章针对图像分类任务，做了全可见模式和因果模式性能的分析实验。如图所示：





![image-20241121224431112](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121224431112.png)





左图是全可见模式，横纵轴互相都能看BERT和MT的自注意力机制都是这种。中间图是因果模式，GPT的自注意力Mamba的SSM是这种，比如y3只能看到x1-x3,看不见x4-x5。右图显示以VT为例，将自注意力机制从全可见模式切换到因果模式后，性能有所下降，说明对于图像分类问题，用因果模式没必要。

既然注意力机制的类型明确了，**在图像分类这一亩三分地上干掉Mamba的可能性暴增**。但老问题又回来了，怎么确定它是不是长序列任务呢？整篇文章最有点数学理论含量，也是最有看点的就是3.2关于图像处理任务是否属于长序列问题的分析。这部分没点基出则很容易一片汇然，不伯，有





![image-20241122202029837](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241122202029837.png)



#### 5、VIT

通过定理分析

![image-20241122223828191](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241122223828191.png)

#### 6、模型架构

下图展示于MambaOut模型的总体框架以及Gated CNN块的具体结构。整体框架类似ResNet,通过降采样逐步减少特征图的尺寸，同时增加特征的抽象层次。





MambaOut 的架构与Swin Transformer和 **DenseNet在分层结构和降采样方面有相似之处**，==但在特征提取和信息混合机制上有所不同==。MambaOut使用**Gated CNN块**，而**Swin Transformer使用窗口注意力机制的Transformer块**，DenseNet.则使用密集连接的卷积层。这些差异决定了它们在处理不同任务时的特性和优势。





> 结论：如果说Mamba是回归RNN+新型注意力机制，那MambaOut其实是回归CNN+新型注意力机制。



- 1、SSM有没有对图像分类意义不大，==因为时序关系不重要==。
- 2、不如最新的CAFormer-M36使用简单的可分离卷积和原始注意力机制，比所有同等大小的视觉Mamba模型高出超过%的准确率85.2%。大家才是纯种的CNN transformer

#### 7、目标检测与实例分割



- 使用标准的COCO数据集，MambaOut作为Mask R-CNN的主干网络使用，结果：尽管MambaOut在COCO上的目标检测和实例分割任务中可以超越一些视觉Mamba模型，但它仍然落后于最先进的视觉Mamba模型，例如VMamba和LocalVMamb

  

- 这种性能差距强调了在==长序列视觉任务中整合Mamba的好处==。当然，与最先进的卷积-注意力混合模型TransNeXt相比51.7%%，视觉Mamba仍表现出显著的性能差距49.2%。仍然需要努力！这个合理也不合理，两点：
  1.Transformer优化了多少年了，Mamba才多久
  2.即使是实例分割问题，所谓的长序列建模，但序列长度并没有NLP那么长，因此效果有限正常。



####  8、总结

本文主要的贡献在于：
1.定量分析论证了<font color=red>图像分类任务不是长序列建模问题</font>，<font color=blue>而目标检测和实例分割是</font>。前者不需要RNN这种机制，因此MambaOut,后者OUT不了

2.借鉴Mamba的GatedCNN结构微调了ResNet,实现了一种新型全局可见注意力机制下的改进版模型。

总体而言，这是一篇证伪的文章，名字起的不错，但创新点并非顾名思义。建议大家综合Mamba来看待，成系列学习。多从中学习人家入手的角度，看问题的深度，创新的刁钻度。


## 四、RWKV



RWKV（Receptance-Weighted Key-Value）是一种==新型序列建模架构==，融合了**Transformer**和**RNN**的优点，主要用于自然语言处理（NLP）等任务。它的目标**是解决传统Transformer在长序列建模中的效率问题，同时保持RNN处理序列的顺序灵活性。**



RWKV是由 **BlinkDL** 开发的一种模型架构，<font color=red>它以Transformer的注意力机制为基础，同时引入了RNN的权重更新机制</font>。



**RWKV 的核心特性**



​	1.	**RNN与Transformer的结合**

​	•	**RNN特性**：==能够逐步处理序列，节省内存，同时自然地支持在线预测（即一次处理一个时间步）==。

​	•	**Transformer特性**：<font color=blue>通过注意力机制增强全局依赖捕获的能力</font>。

​	2.	**高效的序列处理**

​	•	RWKV是**时间步递归的**，不像传统的Transformer需要对整个序列进行并行计算。

​	•	内存和计算需求更低，尤其适合长序列建模。

​	3.	**更低的硬件需求**

​	•	RWKV在训练和推理阶段占用的显存更少，因此在资源有限的设备上（如单卡GPU）也能运行。

​	4.	**支持在线推理（Streaming）**

​	•	RWKV支持增量计算，可以实时处理序列流数据，而不需要一次性读取整个输入序列。



**RWKV的工作原理**



RWKV的基本思想是模仿RNN的序列计算方式，同时用Transformer中的键（Key）、值（Value）对序列信息进行建模。其计算过程包括以下几个步骤：

​	1.	**Receptance（接收权重）**

类似RNN中的门控机制，控制如何更新隐藏状态。



​	2.	**Key 和 Value 的计算**

使用注意力机制更新隐藏状态：



​	3.	**加权更新**

将Receptance的权重与Key-Value对结合，更新隐藏状态。



​	4.	**输出结果**

最终的隐藏状态可以作为序列处理的输出。



**RWKV与其他模型的对比**

 

| **特性**           | **RWKV** | **Transformer** | **RNN**  |
| ------------------ | -------- | --------------- | -------- |
| **并行化**         | 部分支持 | 完全支持        | 不支持   |
| **内存占用**       | 较低     | 较高            | 较低     |
| **长序列建模能力** | 良好     | 优秀            | 较差     |
| **实时推理**       | 支持     | 不支持          | 支持     |
| **复杂度**         | \(O(n)\) | \(O(n^2)\)      | \(O(n)\) |
| **训练难度**       | 中等     | 高              | 低       |

**总结**



RWKV是一种创新的序列建模架构，结合了Transformer和RNN的优势。它在长序列建模中的效率和灵活性使其成为一种具有前景的模型架构，尤其是在内存受限或需要在线推理的场景中。





![image-20241121224827942](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241121224827942.png)

## 五、VGG

VGG 是一种用于**图像分类和特征提取**的卷积神经网络（CNN）架构，全名是 **Visual Geometry Group

**VGG 的特点**



​	1.	**深层结构：**

​	•	VGG 网络由多个==卷积层==和池化层堆叠而成，层数深（如 VGG16 有 16 层，VGG19 有 19 层）。

​	•	卷积核统一采用 3x3，步长为 1；池化层采用2x2 。

​	•	深度的增加使得网络能够捕获更复杂的特征。

​	2.	**简单而统一的设计：**

​	•	每个卷积层的配置保持一致，没有复杂的改进。

​	•	激活函数采用 ReLU。

​	•	使用多层全连接（Fully Connected，FC）作为分类器。

​	3.	**大规模训练：**

   	•	VGG 在 ImageNet 数据集（超过 1000 万张图片，1000 个类别）上训练，性                                                                                                                                                                                                       能优秀。



**VGG 的主要版本**



​	1.	**VGG-16：**

​	•	包括 13 个卷积层和 3 个全连接层，总共 16 层。

​	2.	**VGG-19：**

​	•	包括 16 个卷积层和 3 个全连接层，总共 19 层。



**VGG 的优点**



​	•	**高准确率：** 在大型图像分类任务中表现出色。

​	•	**结构简单：** 模块化设计便于理解和实现。



**VGG 的缺点**



​	1.	**计算量大：**

​	•	由于层数深且参数多，VGG 对内存和计算能力要求高。

​	2.	**训练时间长：**

​	•	需要大量数据和强大的硬件支持。

​	3.	**参数冗余：**

​	•	权重参数较多，不适合资源受限的环境。



**VGG 的应用**



​	•	**图像分类：** 用于识别图片中的类别。

​	•	**特征提取：** 常用 VGG 的卷积层作为特征提取器，将输出传给其他任务（如目标检测、分割）。



在实际应用中，VGG 虽然被更高效的网络（如 ResNet 和 EfficientNet）取代，但由于其结构简单，仍然是研究深度学习的重要起点之一。



## 六、梗哥的 Resnet  TRM BERT



端到端： 直接从问题的输入到输出，省去中间的过程（没有预处理  和特征工程）



- 那么是不是越多层，就是越好呢   --->   并不是 有梯度消失  和梯度爆炸问题 



![image-20241123114335651](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241123114335651.png)

![image-20241123144230624](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241123144230624.png)

RNN  的解剖图



![image-20241123164937580](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241123164937580.png)

问题在于不能解决并行   ，不能解决long term  问题 



多头注意力机制  用来模拟  CNN  的多通道效果



就是空间换时序的思路



掩码结合 将输入嵌入 偏移一个位置 ，确保对预测位置 只能依赖 小于 i 的已知输入



> 查询矩阵 Q （哪些词与自己有关） 键矩阵 K（回答查询）   值矩阵V(修正词向量)

![image-20241123172322001](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241123172322001.png)

因为attention   没有时序信息所以要进行位置编码

![image-20241123172614525](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241123172614525.png)





**B**idirectional **E**ncoder **R**epresentations from **T**ransformers

both left and right context, pretrain, fine-tune,  one additional output layer



Bert  只使用了 encoder

GPT  只是用了  decoder



feature-based: ELMo

fine-tuning: GPT





limitation: unidirectional, sub-optimal for sentence-level tasks

MLM: masked language model

next sentence prediction  (NSP)



BERT的贡献：

- 双向的重要性
- 预训练替代了复杂的模型结构射击
- 11个NLP任务的SOTA



## 七、EM  算法

- 常见的隐变量估计算法

​       EM 算法 (Expectation-Maximization Algorithm)

EM 算法是一种用于**含有隐变量或不完全数据的概率模型参数估计**的迭代优化算法。它通过交替执行 **期望步骤 (E-step)** 和 **最大化步骤 (M-step)**，估计模型参数，直到收敛。

==似然 --->  可能性==

核心思想

<font color=red>在含隐变量或不完全数据的问题中，直接使用最大似然估计可能会遇到困难</font>。EM 算法通过以下两步解决：

1. **E 步骤（Expectation, 期望）：**
   - 计算当前参数下隐变量的**条件期望**。

2. **M 步骤（Maximization, 最大化）：**
   - 利用条件期望重新估计模型参数，最大化对数似然函数。              

两步交替进行，直到参数收敛。



数学描述

设：
- 观测数据： \(X\)
- 隐变量： \(Z\)
- 模型参数： $\theta$
- 完整数据的对数似然： $\ln p(X, Z | \theta)$

目标是最大化观测数据的对数似然：
$\ln p(X | \theta) = \ln \int p(X, Z | \theta) dZ$

直接求解困难，EM 算法采用以下步骤：





1. E-step (计算期望)

计算 Q 函数，即完整数据对数似然的条件期望：

$Q(\theta | \theta^{(t)}) = \mathbb{E}_{Z | X, \theta^{(t)}} [\ln p(X, Z | \theta)]$

其中，$\theta^{(t)}$​ 是当前参数估计。





2. M-step (最大化期望)

最大化 Q 函数，更新参数：

$\theta^{(t+1)} = \arg\max_{\theta} Q(\theta | \theta^{(t)})$



应用场景

1. **混合高斯模型 (GMM)：**
   用于聚类任务，估计混合高斯分布的参数（均值、方差和混合系数）。
2. **隐马尔可夫模型 (HMM)：**
   用于语音识别、时间序列分析。
3. **图像处理：**
   图像分割或去噪中，用 EM 算法估计像素分类概率。
4. **缺失数据问题：**
   数据缺失时，用 EM 补充缺失值。



优点

- 能有效处理含隐变量的问题。
- 理论上保证每次迭代后对数似然值不降低。



缺点

1. **局部最优：** 初值敏感，容易陷入局部最优解。
2. **计算复杂度高：** 随隐变量复杂度增加，计算成本较高。
3. **收敛速度慢：** 需要较多迭代。



案例：高斯混合模型 (GMM) 聚类

1. **模型：** 假设数据由 \(K\) 个高斯分布生成：
   
   $p(X | \theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$
   
   其中 \(\pi_k\) 是混合权重，\(\mu_k\)、\(\Sigma_k\) 是第 \(k\) 个高斯分布的均值和协方差。

2. **EM 步骤：**
   
   - **E-step：** 计算每个数据点属于第 \(k\) 个高斯分布的概率（后验概率）。
   - **M-step：** 根据后验概率更新参数 \(\pi_k\)、\(\mu_k\)、\(\Sigma_k\)。
   
3. **迭代：** 直到参数收敛。

---

EM 算法通过**观测数据**推断**隐变量**，再利用隐变量更新模型参数，是一种经典且强大的概率优化方法。



### 梗哥说法

![image-20241125104306989](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241125104306989.png)

从观测结果y出发 ,求分布函数参数为 $\sigma$ 的可能性



以最大概率 生成观测数据



![image-20241125192446171](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241125192446171.png)





### 清华哥说法



<font color=red>具有隐变量的混合模型的参数估计</font>   极大似然问题



![image-20241125212454090](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241125212454090.png)

这个公式描述的是**期望最大化算法（Expectation-Maximization, EM）**中的M步更新公式。它是一种迭代优化算法，常用于包含隐变量的概率模型参数估计。以下是详细的公式解释：

公式结构：

1. **优化目标：**
   
   $\theta^{t+1} = \arg\max_{\theta} \int_{z} \log p(x, z | \theta) p(z | x, \theta^t) dz$
   
   
   - **含义：** 在每次迭代中，找到新的参数 \(\theta^{t+1}\)，使得某种目标函数（联合对数似然的期望）最大化。
   - **\(\theta\)：** 模型的参数。
   - **\(x\)：** 观测数据（已知）。
   - **\(z\)：** 隐变量（未知或需要估计）。
   - **$p(x, z | \theta)$：** 联合分布，对应模型假设。
   - **$p(z | x, \theta^t)$：** 在当前参数 \(\theta^t\) 下，隐变量的后验分布。
   
2. **分解部分：**
   
   - 第一部分：
     
     $\int_{z} \log p(x, z | \theta) p(z | x, \theta^t) dz$
     
     这是对联合对数似然 $\log p(x, z | \theta)$ 在当前后验分布 $p(z | x, \theta^t)$ 下的期望。
   - 第二部分：
     
     $\mathbb{E}_{z | x, \theta^t} [\log p(x, z | \theta)$
     这一部分是第一部分的简化表达式，表示对 $\log p(x, z | \theta)$ 的期望。

### 背后逻辑：
EM算法的目标是最大化观测数据的对数似然 \(\log p(x | \theta)\)。由于直接优化 \(\log p(x | \theta)\) 很难（尤其是当隐变量存在时），EM引入了间接的优化方式：
1. **E步（Expectation Step，期望步骤）：**
   - 计算隐变量的后验分布 \(p(z | x, \theta^t)\)。
   - 根据后验分布，计算联合对数似然的期望：
     \[
     Q(\theta | \theta^t) = \mathbb{E}_{z | x, \theta^t} [\log p(x, z | \theta)]
     \]

2. **M步（Maximization Step，最大化步骤）：**
   - 更新参数：
     \[
     \theta^{t+1} = \arg\max_{\theta} Q(\theta | \theta^t)
     \]
   - 即通过最大化 \(Q\) 函数找到最优的参数。

### 总结：
公式本质上描述了EM算法中的M步，其核心思想是利用当前隐变量的分布（E步的结果），对联合对数似然进行最大化更新参数。

这种方法被广泛用于：
- 混合模型（如高斯混合模型，GMM）
- 隐马尔可夫模型（HMM）
- 潜变量模型（如LDA主题模型）等。

## 八、指数族分布 (Exponential Family Distribution)



**指数族分布**是一类==常用的概率分布==，其<font color=red>概率密度函数或概率质量函数可以统一写成一种通用的数学形式</font>。这类分布广泛应用于统计建模、机器学习中，特别是在广义线性模型 (GLM) 中具有重要意义。



定义

一个分布属于指数族，如果它的概率密度函数 (PDF) 或概率质量函数 (PMF) 可以表示为以下形式：


$p(x | \eta) = h(x) \exp\left( \eta^T T(x) - A(\eta) \right)$

其中：
- \(x\)：观测数据（随机变量）
- \(\eta\)：**自然参数** (Natural Parameter)
- \(T(x)\)：充分统计量 (Sufficient Statistic)
- \(A(\eta)\)：**对数规范化函数** (Log Partition Function)，保证分布积分为1
- \(h(x)\)：基函数 (Base Measure)



应用场景

1. **机器学习：**
   - 用于分类、回归等任务中建模分布，如逻辑回归和朴素贝叶斯。
   
2. **统计建模：**
   - 指数族分布的通用形式简化了参数估计问题，特别是在最大似然估计中。

3. **信息理论：**
   - 用于熵、KL散度等计算的基础分布。

---

分布曲线



![image-20241125193950721](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241125193950721.png)

EM算法的不足
1.缺失数据较多时，收敛速度慢
2.某些特殊的模型，M步对似然函数估计较困难
3.某些情况下，获得E步期望显示较困难







## 九、变分推断

**变分推断（Variational Inference, VI）** 是一种 **近似推断** 方法，用于在概率模型中估计复杂后验分布。它通过将推断问题转化为优化问题，用优化技术高效地逼近后验分布。



**1. 背景与动机**

在贝叶斯推断中，我们的目标是计算后验分布：
$
P(Z | X) = \frac{P(X, Z)}{P(X)},
$
其中：

- \( P(X, Z) \) 是联合分布。
- \( P(X) \) 是边际似然，通过积分计算：
  $
  P(X) = \int P(X, Z) dZ.
  $

对于复杂的模型，直接计算 \( P(Z | X) \) 或 \( P(X) \) 通常是不可行的，因为积分高维度或无法解析求解。

**2. 核心思想**

变分推断通过引入一个简单的分布 \( Q(Z) \)，用它来近似后验分布 \( P(Z | X) \)。然后通过优化 \( Q(Z) \) 的参数，使 \( Q(Z) \) 与 \( P(Z | X) \) 尽可能接近。

**（1）目标：最小化 KL 散度**

变分推断的目标是最小化 \( Q(Z) \) 与 \( P(Z | X) \) 之间的 KL 散度：
$
D_{\text{KL}}(Q(Z) \| P(Z | X)) = \int Q(Z) \log \frac{Q(Z)}{P(Z | X)} dZ.
$

**（2）等价目标：最大化证据下界（ELBO）**

由于后验 \( P(Z | X) \) 中包含 \( P(X) \)，KL 散度无法直接计算。通过变换目标，我们得到 **证据下界（Evidence Lower Bound, ELBO）**：
$
\text{ELBO}(Q) = \mathbb{E}_{Q(Z)} [\log P(X, Z)] - \mathbb{E}_{Q(Z)} [\log Q(Z)].
$

优化目标转化为最大化 ELBO，因为：
$
\log P(X) = \text{ELBO}(Q) + D_{\text{KL}}(Q(Z) \| P(Z | X)),
$
其中：

- \( \log P(X) \) 是观测数据的对数边际似然。
- $ D_{\text{KL}}(Q(Z) \| P(Z | X)) \geq 0 $)。

---

###  
| **特性**     | **变分推断**             | **MCMC（贝叶斯采样）** |
| ------------ | ------------------------ | ---------------------- |
| **计算方式** | 将推断问题转化为优化问题 | 基于随机采样           |
| **效率**     | 高效，适合大规模数据     | 计算代价高，慢         |
| **结果形式** | 给出后验的近似分布       | 给出后验分布的采样点   |
| **适用场景** | 高维、大规模数据模型     | 小规模、高精度推断需求 |

 **5. 变分推断的应用**

变分推断在许多领域得到了广泛应用，包括：

1. **深度学习：**
   - 变分自动编码器（VAE）：一种生成模型，使用变分推断优化隐变量分布。
   
2. **自然语言处理：**
   - 潜在狄利克雷分布（LDA）模型中的主题推断。

3. **生物信息学：**
   - 基因表达数据中的隐变量建模。

4. **时间序列分析：**
   - 隐马尔可夫模型（HMM）的参数估计。

---

 
