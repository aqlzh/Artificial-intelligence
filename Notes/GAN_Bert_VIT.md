## 一、GAN  阅读



**GAN**



generative Adversarial  Nets



###  1、标题 + 作者



机器学习中有两大类

- 分辨模型：判断数据的类别或者预测实数值
- 生成模型（generative）：生成数据



adversarial：对抗



Nets：networks



一作 Ian J.Goodfellow ,也是《深度学习》（花书）的作者



### 2、摘要



写法简洁，重点在于讲清楚GAN



文章提出了一个新的framework（framework通常是一个比较大的模型）用来估计生成模型，通过对抗的过程，同时会训练两个模型

- <font color=red>生成模型G</font>：用来抓取整个数据的分布（生成模型就是要对整个数据的分布进行建模，使得能够生成各种分布，这里的分布就是指的生成图片、文字或者电影等，在统计学中，整个世界是通过采样不同的分布来得到的，所以如果想要生成东西，就应该抓取整个数据的分布）
- 辨别模型D：用来估计样本到底是从真正的数据生成出来的还是来自生成模型生成出来的，(警察)

生成模型尽量想让辨别模型犯错（生成模型一般是尽量使数据分布接近，但是这个地方有所不同，它是想让辨别模型犯错）



这个framework对应的是minmax two-player game（博弈论中一个很有名的两人对抗游戏）



在任何函数空间的 G 和 D 中存在一个独一无二的解（ G 能够将数据的真实分布找出来，如果已经把真实数据发掘出来的，辨别模型 D 就做不了什么事情了），如果 G 和 D 是MLP的话，那么整个系统就可以通过误差的反向传播来进行训练



不需要使用任何马尔科夫链或着说对一个近似的推理过程展开，相比其他方法更加简单，而且实验效果非常好



### 3、导言



深度学习是用来发现一些丰富的有层次的模型，这些模型能够对AI中各种应用的各种数据做概率分布的表示

- 深度学习不仅仅是深度神经网络，更多的是对整个数据分布的特征表示，深度神经网络只是其中的一个手段



虽然深度学习在辨别模型上取得了很大进展，但是在生成模型上做的还是比较差（难点在于在最大化似然函数的时候要对概率分布进行很多近似，这个近似的计算比较困难）



<font color=red>深度学习在生成模型上进展不大，是因为要去近似概率分布分布来计算似然函数，这篇文章的关键是不用近似似然函数而可以用别的方法来得到一个计算上更好的模型</font>



==这个框架中每一个模型都是MLP==，所以取名叫GAN。这个框架中有两类模型

- 生成模型：类似于造假的人，他的目的是去生产假币
- 辨别模型：类似于警察，他的任务是将假币找出来，和真币区分开开来

造假者和警察会不断地学习，造假者会提升自己的造假技能，警察会提升自己判别真币和假币的性能，最终希望造假者能够赢得这场博弈，也就是说警察无法区分真币和假币，这时就能生成跟真实一样的数据了



框架下面的生成模型是一个MLP，它的输入是一个随机的噪音，MLP能够把产生随机噪音的分布（通常是一个高斯分布）映射到任何想要拟合的分布中。同理，如果判别模型也是MLP的情况下，在这个框架下的特例叫做adversarial nets，因为两个模型都是基于MLP，所以在训练的时候可以直接通过误差的反向传递而不需要像使用马尔科夫链类似的算法来对一个分布进行复杂的采样，从而具有计算上的优势



### 4、相关工作



视频中所读的版本是GAN在neurips上的最终版本，它是一个比较新的版本



在搜索的时候可能会搜索到arxiv版本，它是一个比较早期的版本，作者没有将最新的版本上传到arxiv上面



以上两个版本的主要区别就是相关工作是不不一样的

- arxiv版本上面相关工作的部分其实什么都没写，写的并不相关
- neurips的版本上写了很多真正相关的工作



首先阐述了其他方法所存在的问题：之前的方法是想要构造出一个分布函数出来，然后提供一些参数让他可以学习，通过最大化这些参数的对数似然函数来做这样的坏处是采样一个分布的时候计算比较困难（尤其是维度比较高的时候）



因为这些方法计算比较困难，所以开展了generative machines的相关工作，*不再去构造这样一个分布出来，而是去学习一个模型去近似这个分布，这两种方法是有区别的*

- 前一种方法明确知道分布是什么，包里面的均值、方差等
- 后一种方法不用去构造分布，只需要一个模型去近似想要的结果就可以了，缺点是不知道最后具体的分布是什么样的，好处是计算起来比较容易



![img](https://i0.hdslb.com/bfs/note/343eb63f72af165b00a6841fa24af0e3b91ddd4d.png@640w_!web-note.webp)

- 上式中对 f 的期望求导等价于对 f 自身求导，==这也是为什么可以通过误差的反向传递对GAN进行求解==



VAEs跟GAN非常类似



通过一个辨别模型来帮助生成模型，比如说NCE也用了这样的思路，但是NCE相对来说损失函数更加复杂一点，在求解上没有GAN的性能那么好



和predictability minimization算法的区别

- 其实GAN就是predictability minimization反过来



一个真实有用的技术会在不同的领域不断被人重新发现给予新的名词，大家通常会将功劳归功于那个教会了大家这个技术的人，而不是最早发明他的人



adversarial examples和GAN的区别

- adversarial examples是说通过构造一些和真实样本很像的假样本，能够糊弄到分类器，从而测试整个算法的稳定性

### 5、模型



==这个框架最简单的应用是当生成器和辨别器都是MLP的时候==，生成器需要去学一个在数据x上的Pg分布，x中每个值的分布都是由pg这个分布来控制的



生成模型如何输出x

- 首先在一个输入分布为Pz的噪音变量 z 上定义一个先验，z可以认为是一个100维的向量，每一元素是均值为0，方差为1的高斯噪音
- ==生成模型就是把z映射成x，生成模型是MLP，他有一个可以学习的参数 θg== 



假设想要生成游戏的图片

- 第一种办法是反汇编游戏代码，然后利用代码就知道游戏是如何生成出来的，==这就类似于构造分布函数的方法，在计算上比较困难==
- 第二种办法是不管游戏程序是什么，假设用一个若干维的向量就足以表达游戏背后隐藏的逻辑，再学一个映设（MLP，MLP理论上可以拟合任何一个函数，所以可以通过构造一个差不多大小的向量，然后==利用MLP强行将z映射成x==，使得他们相似就可以了），这种方法的好处是计算比较简单，坏处是MLP不在乎背后真正的分布是什么，而是只是每次生成一个东西，看起来相似就行了



辨别器D也是一个MLP，它也有自己可以学习的参数 θd ，它的作用是将数据放进来之后输出一个标量，这个标量用来判断x到底是来自真实采样的数据还是生成出来的图片（以游戏为例，就是这个图片到底是来自游戏中的截图，还是生成器自己生成的图片），因为知道数据的来源，所以会给数据一个标号（如果来自真实的数据就是1，来自生成的数据就是0）



所以就采样一些数据来训练一个两类的分类器



<font color=red>在训练D的同时也会去训练G，G用来最小化log(1-D(G(z)))</font>

- z 代表随机噪音，放到G中就会生成图片，假设辨别器正确的话，辨别器的输出应该为0，表示是生成的数据，这个式子最终为log1等于0
- 但是如果辨别器没有做好，会输出一个大于0的数，在极端情况下输出1，即辨别器百分之百地确信生成模型所生成的辨别器来自真实的数据，即判断错误。则无论无何log（1-一个大于零小于1 的数）的最终结果就会变成一个负数，在极端情况下，log0是负无穷大
- 所以如果要训练G来最小化log(1-D(G(z)))就意味着，训练一个G使得辨别器尽量犯错，无法区分出来数据到底是来自真实数据还是生成模型所生成的数据



总结

![img](https://i0.hdslb.com/bfs/note/c90034d3d3af722239b39d2012e719092e47bb44.png@640w_!web-note.webp)



20:35

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。



<font color=red>符号 $\mathbb{E}_{z \sim p_z(z)}$ 表示的是对随机变量 \(z\) 的期望值，其中 \(z\) 是从先验分布 \(p_z(z)\) 中采样得到的。这里的 \(\mathbb{E}\) 是期望（Expectation）的缩写，而 $z \sim p_z(z)$ 表示 \(z\) 是遵循概率分布 \(p_z(z)\) 的一个随机变量</font>。



1. **随机变量 \(z\)**：
   \(z\) 通常在机器学习和统计学中用来表示一个随机变量，它可能代表数据点、特征向量、或者在生成对抗网络（GANs）中的输入噪声。

2. **概率分布 \(p_z(z)\)**：
   \(p_z(z)\) 是定义在 \(z\) 上的概率分布，描述了 \(z\) 可能取值的概率。在GAN中，这通常指的是输入到生成器的随机噪声的分布，常见的选择是高斯分布。

3. **期望 \(\mathbb{E}_{z \sim p_z(z)}\)**：
   期望值是一种度量随机变量平均取值的方法。对于离散随机变量，期望值是每个可能值乘以其发生的概率的总和。对于连续随机变量，期望值是每个可能值乘以其概率密度的积分。



数学表达：

对于离散随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \sum_{z} z \cdot p_z(z) $
其中，\( p_z(z) \) 是 \( z \) 取特定值的概率。

对于连续随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \int_{-\infty}^{\infty} z \cdot p_z(z) \, dz $
其中，\( p_z(z) \) 是 \( z \) 的概率密度函数。

在GAN中的应用：

在GAN的上下文中，$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$这一项是训练生成器 \( G \) 的一部分，其中 \( G(z) \) 是生成器生成的数据，\( D \) 是判别器。生成器 \( G \) 试图生成尽可能接近真实数据的样本，以使判别器 \( D \) 难以区分真假。这里的期望值是对所有可能的噪声 \( z \) 进行平均，以评估生成器的整体性能。

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

==总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。== 



- 目标函数如上图中公式所示，是一个两人的minimax游戏
- ==V（G，D）是一个价值函数==
- 公式右边第一项是期望，x是采样真实分布
- 公式右边第二项是期望，x是采样噪音分布
- 在D是完美的情况下，公式右边的两项应该都是等于0的
- 如果D不完美、有误分类的情况下，这两项因为log的关系，都会变成一个负数值
- 所以如果想要辨别器完美地分类这两类的话，就应该最大化D的值，最小化G，目标函数中有两个东西，一个是min，一个是max，和一般的训练步骤有所区别，一般只有一个min，或者只有一个max，这里既有min又有max，就是两个模型在相互对抗：D是尽量把数据分开，G是尽量使生成数据分不开，这个在博弈论中叫两人的minimax游戏
- 如果达到了一个均衡，就是D不能往前进步，G也不能往前进步了，就认为达到了均衡，这个均衡叫做纳什均衡





![img](https://i0.hdslb.com/bfs/note/49f4369d54a493101193b6b59f8a9453dde54ecd.png@640w_!web-note.webp)

- 上图中一共有四张图，分别表示GAN在前面三步和最后一步所做的工作
- z是一个一维的标量
- x也是一个一维的标量
- 噪音是均匀分布采样来的
- **所要真实拟合的x如图中黑色圆点所示，是一个高斯分布**
- （a）表示第一步的时候，生成器将均匀分布进行映射，图中绿色的线就是把z映射成了一个高斯分布，此时辨别器视图中蓝色的线，表现一般
- （b）表示更新辨别器，尽量把这两个东西分开，**两个高斯分布的最高点表示真实分布和噪声最有可能出现的地方，辨别器需要在真实分布的地方值为1**，在噪音分布的地方值为0，这样就可以尽量将来自真实分布的x和来自于生成器的x尽量分别开来
- （c）表示尽量更新生成器，使得能够尽量糊弄到辨别器（就是将生成器生成的高斯分布的峰值尽量左移，向真实数据的高斯分布进行靠拢），让辨别器犯错，这时候辨别器就需要尽量调整来把这两个细微的区别区别开来。
- （d）表示通过不断地调整生成器和辨别器，直到最后生成器的模型能够将来自均匀分布的随即噪音z映射成几乎跟真实分布差不多融合的高斯分布，即从真实的黑点中采样还是从生成器的绿线采样，辨别模型都是分辨不出来的（不管来自于哪个分布，辨别器对这每个值的输出都是0.5，这就是GAN最后想要的结果：生成器生成的数据和真实数据在分布上是完全分别不出来的，辨别器最后对此无能为力）





![img](https://i0.hdslb.com/bfs/note/848589b90e335f754444ab0dbcdfeb9ef1b4116e.png@640w_!web-note.webp)

- 上图表示的是算法一
- 第一行是一个for循环，每一次循环里面是做一次迭代，迭代的另一部分也是一个k步的for循环，每一步中先采样m个噪音样本，再采样m个来自真实数据的样本，组成一个两个m大小的小批量，将其放入价值函数中求梯度（就是将采样的真实样本放入辨别器，将采样的噪音放进生成器得到的生成样本放进辨别器**，放进去之后对辨别器的参数求梯度来更新辨别器**），这样子做k步，做完之后再采样m个噪音样本放进第二项中，把它对于生成器的模型的梯度算出来，然后对生成器进行更新，这样就完成了一次迭代
- 每次迭代中，==先更新辨别器，再更新生成器==
- k是一个超参数，k不能取太小，也不能取太大，需要辨别器有足够的更新但也不要更新的太好。如果没有足够好的更新，对新的数据，生成器生成的东西已经改变了，如果辨别器没有做相应的变化，那么再更新生成器来糊弄D其实意义不大；反过来讲如果将D训练到足够完美，log(1-D(G(z)))就会变成0，对0进行求导，生成模型的更新就会有困难（**如果辨别器是警察，生成器是造假者，假设造假者一生产假币，警察就将其一锅端了，造假者也就不会赚到钱，就没有能力去改进之后的工艺了；反过来讲，如果警察没有能力，造假者随便造点东西，警察也看不出来，也抓不到造假者，那么造假者也不会有动力去改进工艺，使得假钞和真钞真的长得差不多，所以最好是两方实力相当，最后大家能够一起进步）**
- **k就是一个超参数，使得D的更新和G的更新在进度上差不多**
- 外层循环迭代N次直到完成，如何判断是否收敛，这里有两项，一个是往上走（max），一个是往下走（min），有两个模型，所以如何判断收敛并不容易。整体来说，GAN的收敛是非常不稳定的，所之后有很多工作对其进行改进









在上面的公式中，等式右边的第二项存在一定的问题：在早期的时候G比较弱，生成的数据跟真实的数据差得比较远，这就很容易将D训练的特别好（D能够完美地区分开生成的数据和真实的数据），就导致log(1-D(G(z)))会变成0，它变成0的话，对他求梯度再更新G的时候，就会发现求不动了。所以在这种情况下建议在更新G的时候将目标函数改成最大化log(D(G(z)))就跟第一项差不多了，这样的话就算D能够把两个东西区分开来，但是因为是最大化的话，问题还是不大的，但是这也会带来另外一个问题，如果D(G(z))等于零的话，log(D(G(z)))是负无穷大，也会带来数值上的问题，在之后的工作中会对其进行改进



### 6、理论



当且仅当生成器学到的分布和真实数据的分布式相等的情况下，目标函数有全局的最优解



算法一确实能够求解目标函数



第一个结论：当G是固定，即生成器是固定的情况下，最优的辨别器的计算如下图公式中所示

![img](https://i0.hdslb.com/bfs/note/596ad2ec9c620805028558d7978a49070965ef1c.png@640w_!web-note.webp)

- \* 表示最优解
- Pdata表示将x放进去之后，在真实产生数据的分布中的概率是多少
- ==Pg表示将x放进去之后，生成器所拟合的分布的概率是多少==
- 分布是在0和1之间的数值，所以上式中的每一项都是大于等于0、小于等于1的，因此上式中分子、分母中所有的项都是非负的，所以整个式子右式的值是在0到1之间的
- 当Pdata和Pg是完全相等的情况下（即对每一个x，两个p给出来的结果是一样的），右式的值是1/2，即不管对什么样的x，最优的辨别器的输出概率都是1/2，<font color=red>表示这两个分布是完全分不开的</font>
- 这里可以看到D是如何训练出来的，从两个分布中分别采样出数据，用之前的目标函数训练一个二分类的分类器，这个分类器如果说给的值都是1/2，即什么值都分辨不出来，就表示这两个分布是重合的，否则的话就能够分辨出来，这个东西在统计学中非常有用，这叫做two sample test：判断两个数据是不是来自同一个分布在统计上其实有很多工具，比如说用T分布检测（在数据科学中经常使用，可以完全不管分布是什么样子的，可以无视在高维上很多统计工序不好用，就训练一个二分类的分类器，如果这个分类器能够分开这两个数据，就表示这两个数据是来自于不同分布，如果不能分开，就表示这个数据是来自同一分布的，这个技术在很多实用的技术中经常会用到它，比如说在一个训练集上训练一个模型然后把它部署到另外一个环境，然后看新的测试数据跟训练数据是不是一样的时候，就可以训练一个分类器把它分一下就行了，这样就可以避免训练一个模型部署到一个新的环境，然后新的环境和模型不匹配的问题）





期望的计算如下图所示

![img](https://i0.hdslb.com/bfs/note/6d803a867e81ecb80f5a459a5f2c52c07019135d.png@640w_!web-note.webp)



33:08



- 等式右边第一项是在Pdata上面对函数求均值
- 等式右边第二项是在Pz上面对函数求均值
- 已知x=g(z)，x是由g（z）生成出来的，假设Pg就是生成器对应的数据映射，就将g（z）替代成x，替代之后，右边第二项对z的概率求期望就变成了对x求期望，x的分布来自于生成器所对应的Pg。
- 一旦完成替代之后，第一项和第二项是可以合并了，合并之后，积分里面的东西抽象出来经过替换变量就可以得到一个关于y的函数，如果y是一个值的话，它其实是一个凸函数，取决于a、b不一样，它的形状不一样。因为它是一个凸函数，所以他会有一个最大值，因为要求最大值，所以会求导，结果是y=a/（a+b），意味着对于任何的x，最优解的D对他的输出等于y等于Pdata(x)/(Pdata(x) + Pg(x))，就证明了之前的结论

![img](https://i0.hdslb.com/bfs/note/5ae22246d6ecb609ae61062433a45d47e8c755c3.png@640w_!web-note.webp)

- 将所求到的最优解代入到上图所示的价值函数中，最大化D，就是将D*直接代进去然后展开，就能得到如上图所示的结果，就能得到之前的结论，再把得到的结果写成一个关于G的函数，因为D已经求得最优解并带入了，所以整个式子就只跟G相关，所以将他记成C(G)，到此对整个价值函数求解就只需要对C(G)进行最小化就行了，因为D的最优解已经算出来了
- 定理一是说当且仅当生成器的分布和真实数据的分布是相等的情况下，C(G)取得全局最小值的时候
- KL散度：用来衡量两个分布。如下图左侧红色公式所示，它表示的在知道p的情况下至少要多少个比特才能够将q描述出来

![img](https://i0.hdslb.com/bfs/note/de4bcc1019ef3c5ce818ce2cfebff93102ef73c1.png@640w_!web-note.webp)



37:11



- 上式中最终结果中的两项实际上就是两个KL散度如下图中的公式所示

![img](https://i0.hdslb.com/bfs/note/175320a3dd312c17a161296dc5a96f3c984886e5.png@640w_!web-note.webp)

- ==KL散度一定是大于等于零的，KL要等于0，那么p和q要相等==
- 如果C(G)要取得最小值，所以需要两个KL散度等于零，又因为p=q，所以Pdata=(Pdata+Pg)/2，所以C(G)的最优解就等价于Pdata=Pg，这就证明了D在已经取得了最优解的情况下，如果想要对G取最优解的话一定是Pg=Pdata，具体来说，对于写成这种形式的两个分布又叫做JS散度
- JS散度和KL散度的区别：<font color=red>JS散度是对称的，而KL不是对称的</font>，不能将p和q进行互换，但是对于JS散度，将p和q进行互换也是可以保持不变的，所以说它是一个对称的散度，而KL是一个不对称的散度
- 也有评论说因为GAN是一个对称的散度，所以使得它在训练上更加容易。但是也可以取一个更好的目标函数使得训练更加艰难
- 到此就证明了目标函数的选择还是很不错的





结论二是说算法一是能够优化目标函数的



当G和D有足够的容量的时候而且算法一允许在中间的每一步D是可以达到它的最优解的时候，如果对G的优化是去迭代下图所示的步骤（式中G已经换成最优解了），那么最后的Pg会收敛到Pdata

![img](https://i0.hdslb.com/bfs/note/d48abdd39dfef7525a35ecb0c8509a938f333080.png@640w_!web-note.webp)

- 将目标（价值函数）看成是一个关于Pg（模型或者分布）的函数，Pg其实是一个函数，那么目标函数就是一个关于函数的函数
- 一个函数的输入可以是标量或者是向量
- 这里目标函数是一个函数的函数：输入不再是一个值，而是一个值加上了计算（等于是说在python中写一个函数，本来是接收一个x，x是一个vector，然后现在需要接收一个clousure，clousure就包括了计算和数），之前是在高维的值的空间里面做迭代，现在需要在一个函数空间里面做梯度下降
- Ex~Pg其实是关于Pg的一个很简单的函数，这个东西展开之后就是把Pg写出来，是一个积分，积分里面有一个Pg(x)，后面一项跟Pg无关，所以他其实就是一个线性函数，而且是一个凸函数
- 在每一步中把D求到最优，就是说一个凸函数的上限函数还是一个凸函数，所以这个凸函数做梯度下降的时候会得到一个最优解
- 虽然假设了每一次会对D优化到极致，但实际上在算法上只是迭代了k步，所以说这个证明并不能说算法一是工作的，但是实际上算法一跑的还是挺好的（其实算法一跑的并不好，还是挺难收敛的，经常会出现各种问题）

### 7、实验+总结



下图是生成的一些图片

![img](https://i0.hdslb.com/bfs/note/d0cd6b6e8cd4349ec8082e2411ac3497d8eb2209.png@640w_!web-note.webp)

- 数字生成的还行，但是后面的图片效果不太好，分辨率特别低，需要很长的时间才能生成稍微能看的图片



总结

- 坏处是整个训练是比较难的，G和D需要比较好的均衡，如果没有均衡好的话会导致生成的图片比较差
- 优势是因为生成器并没有看真正样本上的数据，没有试图去拟合真实数据的特征，使得它能够生成一些比较锐利的边缘，但是这个说法在后面发现并不是这样的



未来的工作

- conditional GAN：现在生成的时候是不受控制的，随便给定一个z，然后看最终出来的是什么东西，但最好是说控制一下去偏向所想要生成的东西



### 8、评论



写作

- 总的来说，写作还是比较明确的，主要关注GAN在干什么
- 摘要中主要讲述了GAN在干什么事情
- intro非常短，首先写了一点故事性（为什么要做这个事情），然后接下来就是写GAN在干什么
- 在相关工作中，虽然第一个版本写的比较糟糕，基本上就是在说与别人不一样，但是后来的版本也基本承认了很多想法前面的人工作都已经做过了（真正伟大的工作不在乎你的那些想法在别的地方已经出现过还是没有，**关键是说你能够给大家展示用这个东西在某个应用上能够取得非常好的效果，能够让别人信服跟着你继续往下做，然后把整个领域做大，这个是伟大工作的前提）**
- 第三章讲的是GAN的目标函数以及如何做优化
- 第四章证明了为什么目标函数能得到最优解以及求解算法在一定程度上能够得到最优解
- 最后一章简单介绍了一些实验和未来的工作

这样的写法比较i清楚，想读的东西可以一路读下来



但是如果工作的开创性并不是很高的时候就一定要写清楚跟别人的区别是什么和贡献是什么





对于GAN本身这个算法而言

- 它开创了一个领域
- 从一般化的角度来及那个，它影响了之后的很多工作（不仅仅是关于GAN）：1、他是无监督学习的，不需要使用标号；2、他用一个有监督学习的损失函数来做无监督学习的，他的标号（来自于采样的还是生成的）来自于数据，用了监督学习的损失函数，所以在训练上确实会高效很多，这也是之后自监督学习（比如说BERT）的灵感的来源



### 9、shuhuai008  讲解

![image-20241001222738993](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241001222738993.png)



1. **P(x ; θg)**: 这里，$P(x ; \theta_g)$表示条件概率分布，表示数据 \(x\) 在给定参数 $\theta_g$ 下的概率。这个分布可以是高斯分布、伯努利分布、或其他形式，取决于具体上下文中的模型。
   - **\(x\)** 是观测到的或生成的数据。
   - $\theta_g$通常是模型的参数（可能是均值、方差等），它控制生成数据的分布。

2. **Z ~ Pz(Z)**: 这个表示 Z 是随机变量，并服从 \(P_Z(Z)\) 的概率分布。
   - **\(Z\)** 是一个随机变量，它可能是潜在变量或者从先验分布中抽取的样本。
   - **\(P_Z(Z)\)** 通常是 Z 的概率分布，可能是标准正态分布、均匀分布等。



这个表达式可能出现在生成模型（例如**变分自编码器 (VAE)** 或 **GANs**）中。具体含义可以是：

- $Z \sim P_Z(Z)$ 表示从先验分布 \(P_Z(Z)\) 中抽取潜在变量 \(Z\)。
- 然后使用生成模型根据 \(Z\) 和模型参数 \(\theta_g\) 生成观测数据 \(x\)，即 \(P(x ; \theta_g)\)。

因此，整个表达式可以解释为：**从潜在分布 \(P_Z(Z)\) 中采样 \(Z\)，并通过生成分布 \(P(x ; \theta_g)\) 来生成观测数据 \(x\)。**

这种方法通常用于深度生成模型（如VAE）中，来描述从潜在空间生成数据的过程。

如果你有更详细的背景或任务说明，可以更具体地讨论这个概念。

![image-20241002093928249](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002093928249.png)

![image-20241002103134188](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002103134188.png)



## 二、Bert 阅读

**BERT**



pre-training of deep bidirectional transformers for language understanding



自然语言中中近三年最重要的文章



在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能



但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，<font color=red>在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练</font>



BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃



### 1、标题 + 作者



**pre-training**：在一个数据集上训练好一个模型，这个模型主要的目的是用在另外一个任务上面，所以如果另外一个任务叫training的话，那么在大的数据集上训练的这个任务（模型）就叫做pre-training，即training之前的任务



**deep**：更深的神经网络



**bidirectional**：双向的



**transformers**：



**language understanding**：transformer主要是用在机器翻译这个小任务上，这里使用的是一个更加广义的词，就是对语言的理解



这篇文章是关于BERT模型，它是一个深的双向的transformer，是用来做预训练的，针对的是一般的语言的理解任务



作者来自**Google AI**语言团队



### 2、摘要



BERT是一个新的语言表示模型，BERT的名字来自于：

- Bidirectional
- Encoder
- Representation
- Transformer

它的意思是transformer这个模型双向的编码器表示，这四个词跟标题是不一样的



**它的想法是基于ELMo**

- ELMo来自于芝麻街中人物的名字，芝麻街是美国的一个少儿英语学习节目
- BERT是芝麻街中另外一个主人公的名字
- 这篇文章和之前的ELMo开创了NLP的芝麻街系列文章



**BERT和最近的一些语言的表示模型有所不同**

- Peters 引用的是ELMo
- Radfor 引用的是GPT



**BERT和ELMo、GPT的区别：**

- **BERT是设计用来训练深的双向表示，使用的是没有标号的数据，再联合左右的上下文信息**
- 因为这样的设计导致训练好的BERT只用加一个额外的输出层，就可以在很多NLP的任务（比如问答、语言推理）上面得到一个不错的结果，而且不需要对任务做很多特别的架构上的改动

GPT考虑的是单向（用左边的上下文信息去预测未来），BERT同时使用了左侧和右侧的信息，它是双向的（Bidirectional）

<font color=blue>ELMO用的是一个基于RNN的架构，BERT用的是transformer，所以ELMo在用到一些下游任务的时候需要对架构做一点点调整，但是BERT相对比较简单，和GPT一样只需要改最上层就可以了</font>



分别用两句话讲清楚了BERT和GPT、ELMo的区别，在摘要的前面讲清楚了文章和哪两个工作相关，以及和这两个工作的区别是什么，在一定程度上表示了本文的工作是基于上述的两个工作之上然后做了一些改动





**BERT的好处**

- 模型概念上更加简单而且效果更好。它在11个NLP的任务上得到了新的最好的结果（包括GLUE、MultiNLI、SQvAD v1.1、SQvAD v2.0等绝对精度都有一定的提升、相对好处（具体提升了多少））



摘要整体上有两段话，一段话是跟另外两篇相关工作的区别，第二段话是说结果特别好。先写改进再写结果比别人好在什么地方



### 3、导言





第一段一般是交代论文所研究方向的上下文关系



在语言模型中，**预训练可以用来提升很多自然语言的任务**



自然语言任务包括两类

- 句子层面的任务（sentence-level）：主要是用来建模句子之间的关系，比如说对句子的情绪识别或者两个句子之间的关系
- 词元层面的任务（token-level）：包括实体命名的识别（对每个词识别是不是实体命名，比如说人名、街道名），这些任务需要输出一些细腻度的词元层面上的输出



预训练在NLP中已经流行了有一阵子了，在计算机视觉里面已经用了很多年了，同样的方法用到自然语言上面也不会很新，但是在介绍BERT的时候，很有可能会把NLP做预训练归功于BERT，BERT不是第一个提出来的而是BERT让这个方法出圈了让后面的研究者跟着做自然语言的任务



导言的第二段和之后一般是摘要的第一段的扩充版本



在使用预训练模型做特征表示的时候，一般有两类策略

- 一个策略是**基于特征**的，代表作是==ELMo==，对每一个下游的任务构造一个跟这个任务相关的神经网络，它使用的==RNN==的架构，然后将预训练好的这些表示（比如说词嵌入也好，别的东西也好）作为一个额外的特征和输入一起输入进模型中，希望这些特征已经有了比较好的表示，所以导致模型训练起来相对来说比较容易，这也是NLP中使用预训练模型最常用的做法（把学到的特征和输入一起放进去作为一个很好的特征表达）
- 另一个策略是**基于微调**的，这里举的是GPT的例子，就是把预训练好的模型放在下游任务的时候不需要改变太多，只需要改动一点就可以了。这个模型预训练好的参数会在下游的数据上再进行微调（所有的权重再根据新的数据集进行微调）



介绍别人的方法的目的通常来讲是为了铺垫自己的方法，别人哪些地方做的不好，自己的方法有所改进



上述两个途径在预训练的时候都是使用一个相同的目标函数，都是使用一个单向的语言模型（给定一些词去预测下一个词是什么东西，说一句话然后预测这句话下面的词是什么东西，属于一个预测模型，用来预测未来，所以是单向的）



第三段讲述了本文的主要想法**：现在这些技术会有局限性，特别是做预训练的表征的时候，主要的问题是标准的语言模型是单向的，这样就导致在选架构的时候会有局限性**

- 在GPT中使用的是一个从左到右的架构（在看句子的时候只能从左看到右），这样的坏处在于如果要做句子层面的分析的话，比如说要判断一个句子层面的情绪是不是对的话，从左看到右和从右看到左都是合法的，另外，就算是词元层面上的一些任务，比如QA的时候也是看完整个句子再去选答案，而不是一个一个往下走

因此如果将两个方向的信息都放进去的话，应该是能够提升这些任务的性能的



在指出了相关工作的局限性和提出了自己的想法之后，接下来就开始讲作者是如何解决这个问题的：提出了BERT，<font color=red>BERT是用来减轻之前提到的语言模型是一个单向的限制，使用的是一个带掩码的语言模型（masked language model）MLM，这个语言模型是受Cloze任务的启发</font>（引用了一篇1953年的论文）

- 这个带掩码的语言模型每一次随机地选一些资源，然后将它们盖住，目标函数就是预测被盖住的字，等价于将一个句子挖一些空完后进行完形填空
- 跟标准的语言模型从左看到右的不同之处在于：带掩码的语言模型是允许看到左右的信息的（相当于看完形填空的时候不能只看完形填空的左边，也需要看完形填空的右边），<font color=blue>这样的话它允许训练深的双向的transformer模型</font>
- 在带掩码的语言模型之外还训练了一个任务，预测下一个句子，核心思想是给定两个句子，然后判断这两个句子在原文里面是相邻的，还是随机采样得来的，这样就让模型学习了句子层面的信息



**这篇文章的贡献**

1. 展示了双向信息的重要性，**GPT只用了单向，之前有的工作只是很简单地把一个从左看到右的语言模型和一个从右看到左的语言模型简单地合并到一起，类似于双向的RNN模型（contact到一起），这个模型在双向信息的应用上更好**
2. 假设有一个比较好的预训练模型就不需要对特定任务做特定的模型改动。BERT是第一个在一系列的NLP任务上（包括在句子层面上和词元层面上的任务）都取得了最好的成绩的基于微调的模型
3. 代码和模型全部放在：https://github.com/google-research/bert



### 4、结论



最近一些实验表明，使用无监督的预训练是非常好的，这样使得资源不多（训练样本比较少的任务也能够享受深度神经网络）,本文主要的工作就是把前人的工作扩展到深的双向的架构上，使得同样的预训练模型能够处理大量的不同的自然语言任务



简单概括一下：本文之前的两个工作一个叫**ELMo**，它使用了双向的信息但是它网络架构比较老，用的是RNN，另外一个工作是**GPT**，它用的是transformer的架构，但是它只能处理单向的信息，因此本文将ELMo双向的想法和GPT的transformer架构结合起来就成为了BERT

- 具体的改动是在做语言模型的时候不是预测未来，而是变成完形填空



很多时候我们的工作就是把两个东西缝合到一起，或者把一个技术用来解决另外领域的问题，如果所得到的东西确实简单好用，别人也愿意使用，就朴实地将它写出来也没有问题



### 5、相关工作



**非监督的基于特征的一些工作**

- 词监督
- ELMo



**非监督的基于微调的一些工作**

- 代表作是GPT



**在有标号的数据上做迁移学习**

- 在NLP中有标号而且比较大的数据（包括自然语言的推理和机器翻译这两块中都有比较大的数据集）
- 然后在这些有标号的数据集上训练好了模型然后在别的任务上使用

在计算机视觉中这一块使用比较多：经常在ImageNet上训练好模型再去别的地方使用，但是在NLP这一块不是特别理想（可能一方面是因为这两个任务跟别的任务差别还是挺大的，另一方面可能是因为数据量还是远远不够的），BERT和他后面的一系列工作证明了在NLP上面使用没有标号的大量数据集训练成的模型效果比在有标号的相对来说小一点的数据集上训练的模型效果更好，同样的想法现在也在慢慢地被计算机视觉采用，就是说在大量的没有标号的图片上训练出的模型也可能比在ImageNet这个100万数据集上训练的模型可能效果更好

### 6、BERT模型



主要介绍了实现的一些细节



BERT中有两个步骤：

- **预训练**：**在预训练中，这个模型是在一个没有标号的数据集上训练的**
- **微调**：在微调的时候同样是用一个BERT模型，但是它的权重被初始化成在预训练中得到的权重，所有的权重在微调的时候都会参与训练，用的是有标号的数据



每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早预训练好的BERT模型作为初始化，但是每个下游任务都会根据自己的数据训练好自己的模型



虽然预训练和微调不是BERT独创的，在计算机视觉中用的比较多，但是作者还是做了一个简单的介绍（在写论文的时候遇到一些技术需要使用的时候，而且可能应该所有人都知道，最好不要一笔带过，论文是需要自洽的，后面的人读过来可能不太了解这些技术，但是这些技术又是论文中方法不可缺少的一部分的话，最好还是能够做一个简单的说明）





下图中左图表示预训练，右图表示微调

![img](https://i0.hdslb.com/bfs/note/576a684529ef23d69c4ad806b7c4e81c9d4f19af.png@630w_!web-note.webp)

- ==预训练的时候输入是一些没有标号的句子对==
- 这里是在一个没有标号的数据上训练出一个BERT模型，把他的权重训练好，对下游的任务来说，对每个任务创建一个同样的BERT模型，但是它的权重的初始化值来自于前面预训练训练好的权重，对于每一个任务都会有自己的有标号的数据，然后对BERT继续进行训练，这样就得到了对于某一任务的BERT版本





**模型架构**

![img](https://i0.hdslb.com/bfs/note/2850d9e7574d552db3406e49b4e946a6fef775d8.png@630w_!web-note.webp)

BERT模型就是一个多层的双向transformer编码器，而且它是直接基于原始的论文和它原始的代码，没有做改动

三个参数

- L：transformer块的个数
- H：隐藏层的大小
- A：自注意力机制中多头的头的个数

两个模型

- BERT base：它的选取是使得跟GPT模型的参数差不多，来做一个比较公平的比较
- BERT large：用来刷榜
- BERT中的模型复杂度和层数是一个线性关系，和宽度是一个平方的关系



**怎样把超参数换算成可学习参数的大小**

模型中可学习参数主要来自两块

- 嵌入层：就是一个矩阵，输入是字典的大小（假设是30k），输出等于隐藏单元的个数（假设是H）
- transformer块：==transformer中有两部分：一个是自注意力机制（它本身是没有可学习参数的，但是对多头注意力的话，他会把所有进入的K、V、Q分别做一次投影，每一次投影的维度是等于64的，因为有多个头，头的个数A乘以64得到H，所以进入的话有key、value、q，他们都有自己的投影矩阵，这些投影矩阵在每个头之间合并起来其实就是H*H的矩阵了，同样道理拿到输出之后也会做一次投影，他也是一个H*H的矩阵，所以对于一个transformer块，他的自注意力可学习的参数是H的平方乘以4），一个是后面的MLP（MLP里面需要两个全连接层，第一个层的输入是H，但是它的输出是4*H，另外一个全连接层的输入是4*H，输出是H，所以每一个矩阵的大小是H*4H，两个矩阵就是H的平方乘以8），这两部分加起来就是一个transformer块中的参数，还要乘以L（transformer块的个数）==

所以总参数的个数就是30k乘以H（这部分就是嵌入层总共可以学习的参数个数）再加上L层乘以H的平方再乘以12

![img](https://i0.hdslb.com/bfs/note/ff210a3cba166eaecd3db56e6242a725a3e45994.png@630w_!web-note.webp)





**输入和输出**

对于下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使BERT模型能够处理所有的任务，它的输入既可以是一个句子，也可以是一个句子对

- 这里的一个句子是指一段连续的文字，不一定是真正的语义上的一段句子
- 输入叫做一个序列，可以是一个句子，也可以是两个句子
- 这和之前文章里的transformer是不一样的：transformer在训练的时候，他的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是BERT只有一个编码器，所以为了使它能够处理两个句子，就需要把两个句子变成一个序列





**序列的构成**

<font color=green>这里使用的切词的方法是WordPiece</font>，核心思想是：

- 假设按照空格切词的话，一个词作为一个token，因为数据量相对比较大，所以会导致词典大小特别大，可能是百万级别的，那么根据之前算模型参数的方法，如果是百万级别的话，就导致整个可学习参数都在嵌入层上面
- WordPiece是说假设一个词在整个里面出现的概率不大的话，那么应该把它切开看它的一个子序列，它的某一个子序列很有可能是一个词根，这个词很有可能出现的概率比较大话，那么就只保留这个子序列就行了。这样的话，可以把一个相对来说比较长的词切成很多一段一段的片段，而且这些片段是经常出现的，这样的话就可以用一个相对来说比较小的词典就能够表示一个比较大的文本了

切好词之后如何将两个句子放在一起

- 序列的第一个词永远是一个特殊的记号[CLS]，CLS表示classification，这个词的作用是BERT希望最后的输出代表的是整个序列的信息（比如说整个句子层面的信息），因为BERT使用的是transformer的编码器，所以它的自注意力层中每一个词都会去看输出入中所有词的关系，就算是词放在第一的位置，它也是有办法能够看到之后的所有词

---

**[CLS]**：用于分类任务的特殊标记，出现在输入序列的第一个位置。



在 BERT 中，**[CLS]** 是一个专门为==分类任务设计的标记==。当输入文本经过 BERT 编码器时，BERT 会生成每个词（token）对应的隐藏层向量，**[CLS]** 的输出向量被认为是对整个输入序列（句子或段落）的综合表示。这个表示向量可以用于处理下游任务，如文本分类、情感分析、问答系统等。



​	•	**位置**：在 BERT 的输入序列中，**[CLS]** 始终位于第一个位置，无论输入的是一个句子还是两个句子。

​	•	**输出向量**：在 BERT 模型的最后一层，**[CLS]** 对应的向量被用作输入序列的全局表示，用于下游任务的决策。这个向量可以被送入一个全连接层进行分类或其他任务。



<font color=red>**[CLS]** 对应的输出向量被视为整个输入序列的综合表示</font>

---

把两个句子合在一起，但是因为要做句子层面的分类，所以需要区分开来这两个句子，这里有两个办法：

1. 在每一个句子后面放一个特殊的词：[SEP]表示separate
2. 学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子

下图中红线画出来的粉色方框表示输入的序列，[CLS]是第一个特殊的记号表示分类，中间用一个特殊的记号[SEP]分隔开，每一个token进入BERT得到这个token的embedding表示（对BERT来讲，就是输入一个序列，然后得到另外一个序列），最后transformer块的输出就表示这个词元的BERT表示，最后再添加额外的输出层来得到想要的结果

![img](https://i0.hdslb.com/bfs/note/4135badc3a7e7d87d7a74b989c37d29da545e14b.png@630w_!web-note.webp)





对于每一个词元进入BERT的向量表示，它是这个词元本身的embedding加上它在哪一个句子的embedding再加上位置的embedding，如下图所示

![img](https://i0.hdslb.com/bfs/note/42f0d13adc6530b7cae7e2e5018affdef470729b.png@630w_!web-note.webp)

- 上图演示的是BERT的嵌入层的做法，即由一个词元的序列得到一个向量的序列，这个向量的序列会进入transformer块
- 上图中每一个方块是一个词元
- token embedding：这是一个正常的embedding层，对每一个词元输出它对应的向量
- segment embedding：表示是第一句话还是第二句话
- position embedding：输入的大小是这个序列的最大长度，它的输入就是每个词元这个序列中的位置信息（从零开始），由此得到对应的位置的向量
- 最终就是每个词元本身的嵌入加上在第几个句子的嵌入再加上在句子中间的位置嵌入
- 在transformer中，位置信息是手动构造出来的一个矩阵，但是在BERT中不管是属于哪个句子，还是具体的位置，它对应的向量表示都是通过学习得来的





**预训练和微调的不同之处**

在预训练的时候，主要有两个东西比较关键

- 目标函数
- 用来做预训练的数据



**带掩码的语言模型**

对于输入的词元序列，如果词元序列是由WordPiece生成的话，那么它有15%的概率会随机替换成掩码，但是对于特殊的词元（第一个词元和中间的分割词元不做替换），如果输入序列长度是1000的话，那么就要预测150个词

这里也会存在问题：因为在做掩码的时候会把词元替换成一个特殊的token（[MASK]），在训练的时候大概会看到15%的词元，但是在微调的时候是没有的，因为在微调的时候不用这个目标函数，所以没有mask这个东西，导致在预训练和微调的时候所看到的数据会有多不同

- 解决方法：对这15%的被选中作为掩码的词有80%的概率是真的将它替换成这个特殊的掩码符号（[MASK]），还有10%的概率将它替换成一个随机的词元（其实是加入了一些噪音），最后有10%的概率什么都不干，就把它存在那里用来做预测（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/397b25bd813702907a728a46ba47747f263c8c97.png@630w_!web-note.webp)





**预训练中的第二个任务就是预测下一个句子**

在QA和自然语言推理中都是句子对，如果让它学习一些句子层面的信息也不错，具体来说，一个输入序列里面有两个句子：a和b，有50的概率b在原文中间真的是在a的后面，还有50%的概率b就是随机从别的地方选取出来的句子，这就意味着有50%的样本是正例（两个句子是相邻的关系），50%的样本是负例（两个句子没有太大的关系），加入这个目标函数能够极大地提升在QA和自然语言推理的效果（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/8e4c09357f2fb5173215cc5b91710b19e6dbfdbc.png@630w_!web-note.webp)

- 上图中高亮部分的##：在原文中 flightless 是一个词，但是由于这个词出现的概率不高，所以在WordPiece中把它砍成了两个词 flight 和 less ，他们都是比较常见的词，##表示后面的词在原文中其实是跟在前面那个词后面的意思





**预训练数据**

![img](https://i0.hdslb.com/bfs/note/df1a2fac07b73131eccdc47b80795d118ead5ce0.png@630w_!web-note.webp)

使用了两个数据集

- BooksCorpus
- English Wikipedia

应该使用文本层面的数据集，即数据集里面是一篇一篇的文章而不是一些随机打乱的句子，因为transformer确实能够处理比较长的序列，所以对于整个文本序列作为数据集效果会更好一些





**用BERT做微调**

BERT和一些基于编码器解码器的架构有什么不同

- transformer是编码器解码器架构
- 因为把整个句子对都放在一起放进去了，所以自注意力能够在两端之间相互能够看到，但是在编码器解码器这个架构中，编码器一般是看不到解码器的东西的，所以BERT在这一块会更好一点，但是实际上也付出了一定的代价（不能像transformer一样能够做机器翻译）



在做下游任务的时候会根据任务设计任务相关的输入和输出，好处是模型其实不需要做大的改动，主要是怎么样把输入改成所要的那个句子对

- 如果真的有两个句子的话就是句子a和b
- 如果只有一个句子的话，比如说要做一个句子的分类，b就没有了

然后根据下游的任务要求，要么是拿到第一个词元对应的输出做分类或者是拿到对应的词元的输出做所想要的输出，不管怎么样都是在最后加一个输出层，然后用一个softnax得到想要的标号



跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU跑一个小时就可以了，使用GPU的话多跑几个小时也行



### 7、实验



介绍了BERT怎么样用在各个下游任务上



**GLUE**

它里面包含了多个数据集，是一个句子层面的任务

BERT就是把第一个特殊词元[CLS]的最后的向量拿出来，然后学习一个输出层w，放进去之后用softmax就能得到标号，这就变成了一个很正常的多分类问题了

下图表示了在这个分类任务上的结果

![img](https://i0.hdslb.com/bfs/note/30a0222074f444f72f042dfdeb0f05e43c3e3f9d.png@630w_!web-note.webp)

- average表示在所有数据集上的平均值，它表示精度，越高越好
- 可以发现就算是BERT就算是在base跟GPT可学习参数差不多的情况下，也还是能够有比较大的提升



**SQuAD v1.1**

斯坦福的一个QA数据集

QA任务是说给定一段话，然后问一个问题，需要在这段话中找出问题的答案（类似于阅读理解），答案在给定的那段话中，只需要把答案对应的小的片段找出来就可以了（找到这个片段的开始和结尾）

- 就是对每个词元进行判断，看是不是答案的开头或者答案的结尾

具体来说就是学两个向量S和E，分别对应这个词元是答案开始的概率和答案最后的概率，它对每个词元（也就是第二句话中每个词元）的S和Ti相乘，然后再做softmax，就会得到这个段中每一个词元是答案开始的概率，公式如下图所示，同理也可以得出是答案末尾的概率

![img](https://i0.hdslb.com/bfs/note/7cca66ff2bb525232c965370ccf17888bd7ace21.png@630w_!web-note.webp)

- Ti表示第 i 个输入词元对应的最后一个隐藏向量



**在做微调的时候的参数设置**

- 使用了3个epoch，扫描了3遍数据
- 学习率是5e-5
- batchsize是32

用BERT做微调的时候结果非常不稳定，同样的参数、同样的数据集，训练十遍，可能会得到不同的结果。最后发现3其实是不够的，可能多学习几遍会好一点



BERT用的优化器是adam的不完全版，当BERT要训练很长时间的时候是没有影响的，但是如果BERT只训练一小段时间的话，它可能会带来影响（将这个优化器换成adam的正常版就可以解决这个问题了）



**SQuAD v2.0**



**SWAG数据集**

它用来判断两个句子之间的关系

跟之前的训练没有太多区别，BERT的结果比别的模型要好很多





对这些不同的数据集，BERT基本上只要把这些数据集表示成所要的句子对的形式，最后拿到一个对应的输出然后再加一个输出层就可以了，所以BERT对整个NLP领域的贡献还是非常大的，大量的任务可以用一个相对来说比较简单的架构，不需要改太多的东西就能够完成了





**ablation study**



介绍了BERT中每一块最后对结果的贡献

![img](https://i0.hdslb.com/bfs/note/ec4e09e283db95be14d7c2037af29101264d5d16.png@630w_!web-note.webp)

- No NSP：假设去掉对下一个句子的预测
- LTR & No NSP：使用一个从左看到右的单向的语言模型（而不是用带掩码的语言模型），然后去掉对下一个句子的预测
- \+ BiLSTM：在上面加一个双向的LSTM

从结果来看，去掉任何一部分，结果都会打折扣





**模型大小的影响**



BERT base中有1亿的可学习参数

BERT large中有3亿可学习的参数

相对于之前的transformer，可学习参数数量的提升还是比较大的

当模型变得越来越大的时候，效果会越来越好，这是第一个展示将模型变得特别大的时候对语言模型有较大提升的工作

虽然现在GPT3已经做到1000亿甚至在向万亿级别发展，但是在三年前，BERT确实是开创性地将一个模型推到如此之大，引发了之后的模型大战





假设不用BERT做微调而是把BERT的特征作为一个静态特征输进去会怎样

结论是效果确实没有微调好，所有用BERT的话应该用微调



**8、评论**



**写作**

- 先写了BERT和GPT的区别
- 然后介绍了BERT模型
- 接下来是在各个实验上的设置
- 最后对比结果，结果非常好

这篇文章认为本文的最大贡献就是双向性（写文章最好能有一个明确的卖点，有得有失，都应该写出来）

- 但是今天来看，这篇文章的贡献不仅仅只有双向性，还有其它东西
- 从写作上来说，至少要说选择双向性所带来的不好的地方是什么，做一个选择，会得到一些东西，也会失去一些东西：和GPT比，BERT用的是编码器，GPT用的是解码器，得到了一些好处，但是也有坏处（比如做机器翻译和文本摘要比较困难，做生成类的东西就没那么方便了）
- 分类问题在NLP中更加常见，所以NLP的研究者更喜欢用BERT，会更容易一些



BERT所提供的是一个完整的解决问题的思路，符合了大家对于深度学习模型的期望：在一个很大的数据集上训练好一个很深很宽的模型，这个模型拿出来之后可以用在很多小问题上，通过微调可以全面提升这些小数据上的性能

### 8、推荐拓展阅读



- [Bert输入部分详细解读_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ey4y1874y/?p=2&spm_id_from=pageDriver&vd_source=81e5007efea018d7c2e8c28374fcdf34)



- [图解BERT_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1uF411Y7F7/?spm_id_from=333.337.search-card.all.click&vd_source=81e5007efea018d7c2e8c28374fcdf34)





NSP  任务  用于处理两个句子之间的任务 Next Sentence Prediction

MLM：Mask Language Model



## 三、VIT  阅读



**Vit**



**an image is worth 16\*16 words:transformers for image recognition at scale**



过去一年在计算机视觉领域影响力最大的工作

- <font color=red>它挑战了自从2012年AlexNet提出以来卷积神经网络在计算机视觉里绝对统治的地位</font>
- **结论：如果在足够多的数据上做预训练，也可以不需要卷积神经网路，直接使用标准的transformer也能够把视觉问题解决的很好**
- *它打破了CV和NLP在模型上的壁垒，开启了CV的一个新时代*



papers with code

- 可以查询现在某个领域或者说某个数据集表现最好的一些方法有哪些
- 图像分类在ImageNet数据集上排名靠前的全是基于Vision Transformer

![img](https://i0.hdslb.com/bfs/note/c23dff43fa425da9d6961166754c100c3fad3c80.png@712w_!web-note.webp)

- 对于目标检测任务在COCO数据集上，排名靠前都都是基于Swin Transformer（Swin Transformer是ICCV 21的最佳论文，可以把它想象成一个多尺度的Vit（Vision Transformer））
- 在其他领域（语义分割、实例分割、视频、医疗、遥感），基本上可以说Vision Transformer将整个视觉领域中所有的任务都刷了个遍





另一篇论文

**Intriguing Properties of Vision Transformer**

Vision Transformer一些有趣的特性



下图中展示了一些在卷积神经网络CNN中工作得不太好但是用Vision Transformer都能处理的很好的例子

![img](https://i0.hdslb.com/bfs/note/23434d4d1b30827a946f257d7c9851ba7262b40d.png@712w_!web-note.webp)

- a表示的是遮挡，在这么严重的遮挡情况下，不管是卷积神经网络，人眼也很难观察出图中所示的是一只鸟
- b表示数据分布上有所偏移，这里对图片做了一次纹理去除的操作，所以图片看起来比较魔幻
- **c表示在鸟头的位置加了一个对抗性的patch**
- d表示将图片打散了之后做排列组合

上述例子中，卷积神经网络很难判断到底是一个什么物体，**但是对于所有的这些例子Vision Transformer都能够处理的很好**



### 1、标题 + 作者



一张图片等价于很多16*16大小的单词

- 为什么是16*16的单词？将图片看成是很多的patch，假如把图片分割成很多方格的形式，每一个方格的大小都是16*16，那么这张图片就相当于是很多16*16的patch组成的整体



使用transformer去做大规模的图像识别



作者团队来自于google research和google brain team



### 2、摘要



虽然说transformer已经是NLP（自然语言处理）领域的一个标准：BERT模型、GPT3或者是T5模型，但是用transformer来做CV还是很有限的



在视觉领域，<font color=red>自注意力要么是跟卷积神经网络一起使用，要么用来把某一些卷积神经网络中的卷积替换成自注意力，但是还是保持整体的结构不变</font>

- 这里的整体结构是指：比如说对于一个残差网络（Res50），它有4个stage：res2、res3、res4、res5，上面说的整体结构不变指的就是这个stage是不变的，它只是去取代每一个stage、每一个block的操作



这篇文章证明了这种对于卷积神经网络的依赖是完全不必要的，一个纯的Vision Transformer直接作用于一系列图像块的时候，也是可以在图像分类任务上表现得非常好的，尤其是当在大规模的数据上面做预训练然后迁移到中小型数据集上面使用的时候，Vision Transformer能够获得跟最好的卷积神经网络相媲美的结果



这里将ImageNet、CIFAR-100、VATB当作中小型数据集

- 其实ImageNet对于很多人来说都已经是很大的数据集了



Transformer的另外一个好处：它只需要更少的训练资源，而且表现还特别好

- 作者这里指的少的训练资源是指2500天TPUv3的天数
- 这里的少只是跟更耗卡的模型去做对比（类似于一个小目标）



### 3、引言



自注意力机制的网络，尤其是Transformer，已经是自然语言中的必选模型了，现在比较主流的方式，就是先去一个大规模的数据集上去做预训练，然后再在一些特定领域的小数据集上面做微调（这个是在BERT的文章中提出来的）



得益于transformer的计算高效性和可扩展性，现在已经可以训练超过1000亿参数的模型了，比如说GPT3



随着模型和数据集的增长，目前还没有发现任何性能饱和的现象

- 很多时候不是一味地扩大数据集或者说扩大模型就能够获得更好的效果的，==尤其是当扩大模型的时候很容易碰到过拟合的问题，但是对于transformer来说目前还没有观测到这个瓶颈==
- 最近微软和英伟达又联合推出了一个超级大的语言生成模型Megatron-Turing，它已经有5300亿参数了，还能在各个任务上继续大幅度提升性能，没有任何性能饱和的现象





回顾transformer

06:17



- transformer中最主要的操作就是自注意力操作，自注意力操作就是每个元素都要跟每个元素进行互动，两两互相的，然后算得一个attention（自注意力的图），用这个自注意力的图去做加权平均，最后得到输出
- 因为在做自注意力的时候是两两互相的，这个计算复杂度是跟序列的长度呈平方倍的。
- 目前一般在自然语言处理中，硬件能支持的序列长度一般也就是几百或者是上千（比如说BERT的序列长度也就是512）







**将transformer运用到视觉领域的<font size=6>难处</font>**



首先要解决的是如何把一个2D的图片变成一个1D的序列（或者说变成一个集合）。**最直观的方式就是把每个像素点当成元素，将图片拉直放进transformer里，看起来比较简单，但是实现起来复杂度较高**。

- 一般来说在视觉中训练分类任务的时候图片的输入大小大概是224x224，如果将图片中的每一个像素点都直接当成元素来看待的话,他的序列长度就是224x24=50176个像素点，也就是序列的长度，这个大小就相当于是BERT序列长度的100倍，这还仅仅是分类任务，对于检测和分割，现在很多模型的输入都已经变成600x600或者800x800或者更大，计算复杂度更高，所以在视觉领域，卷积神经网络还是占主导地位的，比如AlexNet或者是ResNet



所以现在很多工作就是在研究如**何将自注意力用到机器视觉中：一些工作是说把卷积神经网络和自注意力混到一起用；另外一些工作就是整个将卷积神经网络换掉，全部用自注意力。这些方法其实都是在干一个事情：因为序列长度太长，所以导致没有办法将transformer用到视觉中，所以就想办法降低序列长度**



Wang et al.,2018：既然用像素点当输入导致序列长度太长，就可以不用图片当transformer的直接输入，可以把网络中间的特征图当作transformer的输入

- 假如用残差网络Res50，==其实在它的最后一个stage，到res4的时候的featuremap的size其实就只有14*14了==，再把它拉平其实就只有196个元素了，即这个序列元素就只有196了，这就在一个可以接受的范围内了。所以就通过用特征图当作transformer输入的方式来降低序列的长度



Wang et al.,2019;Wang et al.,2020a（Stand-Alone Attention&Axial Attention，孤立自注意力和轴自注意力）

- **孤立自注意力**：之所以视觉计算的复杂度高是来源于使用整张图，所以不使用整张图，就用一个local window（局部的小窗口），这里的复杂度是可以控制的（通过控制这个窗口的大小，来让计算复杂度在可接受的范围之内）。==这就类似于卷积操作（卷积也是在一个局部的窗口中操作的）==
- **轴自注意力**：之所以视觉计算的复杂度高是因为序列长度N=H*W，是一个2D的矩阵，将图片的这个2D的矩阵想办法拆成2个1D的向量，所以先在高度的维度上做一次self-attention（自注意力），然后再在宽度的维度上再去做一次自注意力，相当于把一个在2D矩阵上进行的自注意力操作变成了两个1D的顺序的操作，这样大幅度降低了计算的复杂度



最近的一些模型，这种方式虽然理论上是非常高效的，但事实上因为这个自注意力操作都是一些比较特殊的自注意力操作，所以说无法在现在的硬件上进行加速，所以就导致很难训练出一个大模型，所以截止到目前为止，孤立自注意力和轴自注意力的模型都还没有做到很大，跟百亿、千亿级别的大transformer模型比还是差的很远，因此在大规模的图像识别上，传统的残差网络还是效果最好的



所以，自注意力早已经在计算机视觉里有所应用，而且已经有完全用自注意力去取代卷积操作的工作了，所以本文换了一个角度来讲故事



本文是被transformer在NLP领域的可扩展性所启发，本文想要做的就是直接应用一个标准的transformer直接作用于图片，尽量做少的修改（不做任何针对视觉任务的特定改变），看看这样的transformer能不能在视觉领域中扩展得很大很好



但是如果直接使用transformer，还是要解决序列长度的问题

- vision transformer将一张图片打成了很多的patch，每一个patch是16*16
- 假如图片的大小是224x224，则sequence lenth（序列长度）就是N=224x224=50176，如果换成patch，一个patch相当于一个元素的话，有效的长宽就变成了224/16=14，所以最后的序列长度就变成了N=14*14=196，所以现在图片就只有196个元素了，196对于普通的transformer来说是可以接受的
- 然后将每一个patch当作一个元素，通过一个fc layer（全连接层）就会得到一个linear embedding，这些就会当作输入传给transformer，这时候一张图片就变成了一个一个的图片块了，可以将这些图片块当成是NLP中的单词，一个句子中有多少单词就相当于是一张图片中有多少个patch，这就是题目中所提到的一张图片等价于很多16*16的单词



<font color=red>本文训练vision transformer使用的是==有监督的训练==</font>

- <font color=red>为什么要突出有监督？因为对于NLP来说，transformer基本上都是用无监督的方式训练的，要么是用language modeling，要么是用mask language modeling，都是用的无监督的训练方式但是对于视觉来说，大部分的基线（baseline）网络还都是用的有监督的训练方式去训练的</font>



到此可以发现，本文确实是把视觉当成自然语言处理的任务去做的，尤其是中间的模型就是使用的transformer encoder，跟BERT完全一样，这篇文章的目的是说使用一套简洁的框架，transformer也能在视觉中起到很好的效果



这么简单的想法，之前其实也有人想到过去做，本文在相关工作中已经做了介绍，跟本文的工作最像的是一篇ICLR 2020的paper

- 这篇论文是从输入图片中抽取2*2的图片patch
- 为什么是2x2？因为这篇论文的作者只在CIFAR-10数据集上做了实验，而CIFAR-10这个数据集上的图片都是32x32的，所以只需要抽取2*2的patch就足够了，16*16的patch太大了
- 在抽取好patch之后，就在上面做self-attention

从技术上而言他就是Vision Transformer，但是本文的作者认为二者的区别在于本文的工作证明了如果在大规模的数据集上做预训练的话（和NLP一样，在大规模的语料库上做预训练），那么就能让一个标准的Transformer，不用在视觉上做任何的更改或者特殊的改动，就能取得比现在最好的卷积神经网络差不多或者还好的结果，同时本文的作者还指出之前的ICLR的这篇论文用的是很小的2x2的patch，所以让他们的模型只能处理那些小的图片，而Vision Transformer是能够处理224*224这种图片的



所以这篇文章的主要目的就是说，Transformer在Vision领域能够扩展的有多好，就是在超级大数据集和超级大模型两方的加持下，Transformer到底能不能取代卷积神经网络的地位



一般引言的最后就是将最想说的结论或者最想表示的结果放出来，这样读者不用看完整篇论文就能知道文章的贡献有多大



本文在引言的最后说在中型大小的数据集上（比如说ImageNet）上训练的时候，如果不加比较强的约束，Vit的模型其实跟同等大小的残差网络相比要弱一点

- 作者对此的解释是：这个看起来不太好的结果其实是可以预期的，因为transformer跟卷积神经网路相比，**它缺少了一些卷积神经网络所带有的归纳偏置**
- 这里的归纳偏置其实是指一种先验知识或者说是一种提前做好的假设



对于卷积神经网络来说，常说的有两个inductive bias（归纳偏置）：

- locality：因为卷积神经网络是以滑动窗口的形式一点一点地在图片上进行卷积的，所以假设图片上相邻的区域会有相邻的特征，靠得越近的东西相关性越强
- translation equivariance（平移等变性或平移同变性）：f(g(x))=g(f(x))，就是说不论是先做g这个函数，还是先做f这个函数，最后的结果是不变的。这里可以把f理解成卷积，把g理解成平移操作，意思是说无论是先做平移还是先做卷积，最后的结果都是一样的（因为在卷积神经网络中，卷积核就相当于是一个模板，不论图片中同样的物体移动到哪里，只要是同样的输入进来，然后遇到同样的卷积核，那么输出永远是一样的）



一旦神经网络有了这两个归纳偏置之后，他就拥有了很多的先验信息，所以只需要相对较少的数据来学习一个相对比较好的模型，但是对于transformer来说，它没有这些先验信息，所以它对视觉的感知全部需要从这些数据中自己学习

- 为了验证这个假设，作者在更大的数据集（14M-300M）上做了预训练，这里的14M是ImageNet 22k数据集，300M是google自己的JFT 300M数据集，然后发现大规模的预训练要比归纳偏置好



Vision Transformer只要在有足够的数据做预训练的情况下，就能在下游任务上取得很好的迁移学习效果。具体来说，就是当在ImageNet 21k上或者在JFT 300M上训练，Vit能够获得跟现在最好的残差神经网络相近或者说更好的结果，如下图所示

![img](https://i0.hdslb.com/bfs/note/dd370b2e1b3a909ea2b1ee3e936ff380c360bdc4.png@700w_!web-note.webp)

- VTAB也是作者团队所提出来的一个数据集，融合了19个数据集，主要是用来检测模型的稳健性，从侧面也反映出了Vision Transformer的稳健性也是相当不错的



总的来说，引言写的简洁明了

- 第一段先说因为Transformer在NLP中扩展的很好，越大的数据或者越大的模型，最后performance会一直上升，没有饱和的现象，然后提出：如果将Transformer使用到视觉中，会不会产生同样的效果
- 第二段开始讲前人的工作，讲清楚了自己的工作和前人工作的区别：之前的工作要么就是把卷积神经网络和自注意力结合起来，要么就是用自注意力去取代卷积神经网络，但是从来没有工作直接将transformer用到视觉领域中来，而且也都没有获得很好的扩展效果
- 第三段讲Vision Transformer就是用了一个标准的Transformer模型，只需要对图片进行预处理（把图片打成块），然后送到transformer中就可以了，而不需要做其他的改动，这样可以彻底地把一个视觉问题理解成是一个NLP问题，就打破了CV和NLP领域的壁垒
- 最后两段展示了结果，只要在足够多的数据做预训练的情况下，Vision Transformer能够在很多数据集上取得很好的效果



### 4、结论



这篇论文的工作是直接拿NLP领域中标准的Transformer来做计算机视觉的问题，跟之前用自注意力的那些工作的区别在于：

- **除了在刚开始抽图像块的时候，还有位置编码用了一些图像特有的归纳偏置**

除此之外就再也没有引入任何图像特有的归纳偏置了，这样的好处就是不需要对Vision领域有多少了解，可以直接把图片理解成一个序列的图像块，就跟一个句子中有很多单词一样，然后就可以直接拿NLP中一个标准的Transformer来做图像分类了



当这个简单而且扩展性很好的策略和大规模预训练结合起来的时候效果出奇的好：Vision Transformer在很多图像分类的benchmark上超过了之前最好的方法，而且训练起来还相对便宜



目前还没有解决的问题（对未来的展望）

如何用transformer来做cv

第一个问题：Vit不能只做分类，还有检测和分割

- DETR：去年目标检测的一个力作，相当于是改变了整个目标检测之前的框架



鉴于Vit和DETR良好的表现，所以作者说拿Vision Transformer做视觉的其他问题应该是没有问题的

- 事实上，在Vit出现短短的一个半月之后，2020年12月检测这块就出来了一个叫Vit-FRCNN的工作，就已经将Vit用到检测上面了
- 图像分割这一块也是一样的，同年12月就有一篇SETR的paper将Vit用到分割里了
- 紧接着3个月之后Swin Transformer横空出世，它将多尺度的设计融合到了Transformer中，更加适合做视觉的问题了，真正证明了Transformer是能够当成一个视觉领域的通用骨干网络



另外一个未来的工作方向就是说要去探索一下==自监督的预训练方案==，因为在NLP领域，所有的大的transformer全都是用自监督的方式训练的，Vit这篇paper也做了一些初始实验，证明了用这种自监督的训练方式也是可行的，但是跟有监督的训练比起来还是有不小的差距的



最后作者说，继续将Vision Transformer变得更大，有可能会带来更好的结果

- 过了半年，同样的作者团队又出了一篇paper叫做Scaling Vision Transformer，就是将Transformer变得很大，提出了一个Vit-G,将ImageNet图像分类的准确率提高到了90以上了





 ### 5、相关工作





transformer在NLP领域的应用

- 自从2017年transformer提出做机器翻译以后，基本上transformer就是很多NLP任务中表现最好的方法。
- 现在大规模的transformer模型一般都是先在一个大规模的语料库上做预训练，然后再在目标任务上做一些细小的微调，这当中有两系列比较出名的工作：BERT和GPT。BERT是用一个denoising的自监督方式（**其实就是完形填空，将一个句子中某些词划掉，再将这些词预测出来**）；GPT用的是language modeling（已经有一个句子，然后去预测下一个词是什么，也就是next word prediction，预测下一个词）做自监督。这两个人物其实都是人为定的，语料是固定的，句子也是完整的，只是人为的去划掉其中的某些部分或者把最后的词拿掉，然后去做完形填空或者是预测下一个词，所以这叫自监督的训练方式



自注意力在视觉中的应用

- 视觉中如果想简单地在图片上使用自注意力，最简单的方式就是将每一个像素点当成是一个元素，让他们两两做自注意力就好了，但是这个是平方复杂度，所以很难应用到真实的图片输入尺寸上。像现在分类任务的224*224，一个transformer都很难处理，更不用提人眼看的比较清晰的图片了，一般是1k或者4k的画质，它们的序列长度都是上百万，直接在像素层面使用transformer的话不太现实，所以如果想用transformer就一定得做一些近似
- 复杂度高是因为用了整张图，所以序列长度长，那么可以不用整张图，就用local neighborhood（一个小窗口）来做自注意力，那么序列长度就大大降低了，最后的计算复杂度也就降低了
- **另外也可以使用Sparse Transformer，就是只对一些稀疏的点去做自注意力，所以只是一个全局注意力的近似**
- 还有一些方法就是将自注意力用到大小不同的block上，或者说在极端的情况下使用轴注意力（先在横轴上做自注意力，然后再在纵轴上做自注意力），序列长度也是大大减小的
- 这些特制的自注意力结构其实在计算机视觉上的结果都不错，表现都是没问题的，但是它们需要很复杂的工程去加速算子，虽然在CPU或者GPU上跑得很快或者说让训练一个大模型成为可能



跟本文工作最相似的是一篇ICLR2020的论文，区别在于Vision Transformer使用了更大的patch更大的数据集



在计算机视觉领域还有很多工作是把卷积神经网络和自注意力结合起来的，这类工作相当多，而且基本涵盖了视觉里的很多任务（检测、分类、视频、多模态等）



还有一个工作和本文的工作很相近，叫image GPT

- GPT是用在NLP中的，是一个生成性的模型
- image GPT也是一个生成性模型，也是用无监督的方式去训练的，它和Vit相近的地方在于它也用了transformer
- image GPT最终所能达到的效果：如果将训练好的模型做微调或者就把它当成一个特征提取器，它在ImageNet上的最高的分类准确率也只能到72，Vit最终的结果已经有88.5了，远高于72
- 但是这个结果也是最近一篇paper叫做MAE爆火的原因。*因为在BEiT和MAE这类工作之前生成式网络在视觉领域很多任务上是没有办法跟判别式网络相比的，判别式网络往往要比生成式网络的结果高很多，但是MAE做到了，它在ImageNet-1k数据集上训练，用一个生成式的模型，比之前判别式的模型效果好很多，而且不光是在分类任务上，最近发现在目标检测上的迁移学习的效果也非常好*



**Vit其实还跟另外一系列工作是有关系的，用比ImageNet更大的数据集去做预训练，这种使用额外数据的方式，一般有助于达到特别好的效果**

- 比如2017年介绍JFT 300数据集的那篇paper研究了卷积神经网络的效果是怎么随着数据集的增大而提高的
- 还有一些论文是研究了在更大的数据集（比如说ImageNet-21k和JFT 300M）上做预训练的时候迁移学习的效果会怎样，就是迁移到ImageNet或者CIFAR-100上的效果如何

这篇论文也是聚焦于ImageNet-21k和JFT 300M，但是训练的并不是一个残差网络，而失去训练transformer



本文的相关工作写的非常彻底，而且列举了很多跟本文工作最相近的，比如说ICLR 2020的论文、iGPT还有之前研究大数据集的BiT等



写相关工作这个章节的目的就是让读者知道在你的工作之前别人做了哪些工作，你和他们的区别在哪里。写清楚之后其实对论文本身是非常有利的，并不会降低论文的创新性，反而让整个文章变得更加简单易懂





### 6、ViT模型



在模型的设计上是尽可能按照最原始的transformer来做的，这样做的好处就是可以直接把NLP中比较成功的Transformer架构拿过来用，而不用再去对模型进行改动，而且因为transformer因为在NLP领域已经火了很久了，它有一些写的非常高效的实现，同样ViT也可以直接拿来使用



下图是模型的总览图，模型的总览图对论文来说是非常重要的，画的好的模型总览图能够让读者在不读论文的情况下，仅仅通过看图就能够知道整篇论文的大致内容

![img](https://i0.hdslb.com/bfs/note/01dd08af4754c81fc11f34a820acd55a43351153.png@700w_!web-note.webp)

- 首先给定一张图，先将这张图打成了很多的patch（如上图左下角所示），这里是将图打成了九宫格
- ==然后再将这些patch变成了一个序列，每个patch通过线性投射层的操作得到一个特征（就是本文中提到的patch embedding）==
- 自注意力是所有元素之间两两做交互，所以本身并不存在顺序的问题，但是对于图片来说，图片本身是一个整体，这个九宫格是有自己的顺序的，如果顺序颠倒了就不是原来的图片了。所以类似于NLP，给patch embedding加上了一个position embedding，==等价于加上了一个位置编码==
- 在加上这个位置编码信息之后，<font color=red>整体的token就既包含了图片块原本有的图像信息，又包含了这个图片块的所在位置信息</font>
- 在得到了这个token之后，接下来就跟NLP中完全一样了，直接将它们输入进一个Transformer encoder，然后Transformer encoder就会得到很多输出
- 这么多输出，应该拿哪个输出去做分类？这里借鉴了BERT，BERT中有一个extra learnable embedding，它是一个特殊字符CLS（分类字符），所以这里也添加了一个特殊的字符，用*代替，而且它也是有position embedding，它的位置信息永远是0，如下图红色圆圈所示

![img](https://i0.hdslb.com/bfs/note/9ac3c99c417479e91b4b7c6cfd3d1a9791a7de67.png@700w_!web-note.webp)

- 因为**所有的token都在跟其它token做交互信息**，所以作者相信，class embedding能够从别的序列后面的embedding中学到有用的信息，从而只需要根据class embedding的输出做最后的判断就可以了
- MLP Head其实就是一个通用的分类头
- 最后用交叉熵函数进行模型的训练



模型中的Transformer encoder是一个标准的Transformer，具体的结构如下图右图所示

![img](https://i0.hdslb.com/bfs/note/e428897f1841695df81dde4d425ea910ac02f366.png@700w_!web-note.webp)

- Transformer的输入是一些patch
- 一个Transformer block叠加了L次



整体上来看Vision Transformer的架构还是相当简洁的，它的特殊之处就在于如何把一个图片变成一系列的token



**具体的模型的前向过程**

- 假如说有一个224x224x3的图片X，如果使用16x16的patch size大小，就会得到196个图像块，每一个图像块的维度就是16x16x3=768，到此就把原先224x224x3的图片变成了196个patch，每个patch的维度是768
- 接下来就要将这些patch输入一个线性投射层，这个线性投射层其实就是一个全连接层（在文章中使用E表示），这个全连接层的维度是768x768，第二个768就是文章中的D，D是可以变的，如果transformer变得更大了，D也可以相应的变得更大，第一个768是从前面图像的patch算来的（16x16x3），它是不变的。
- 经过了线性投射就得到了patch embedding（X*E），它是一个196x768的矩阵（X是196x768，E是768x768），意思就是现在有196个token，每个token向量的维度是768
- 到目前为止就已经成功地将一个vision的问题变成了一个NLP的问题了，输入就是一系列1d的token，而不再是一张2d的图片了
- 除了图片本身带来的token以外，这里面加了一个额外的cls token，它是一个特殊的字符，只有一个token，它的维度也是768，这样可以方便和后面图像的信息直接进行拼接。所以最后整体进入Transformer的序列的长度是197*768（196+1：196个图像块对应的token和一个特殊字符cls token）
- 最后还要加上图像块的位置编码信息，这里是将图片打成了九宫格，所以位置编码信息是1到9，但是这只是一个序号，并不是真正使用的位置编码，==具体的做法是通过一个表（表中的每一行就代表了这些1到9的序号，每一行就是一个向量，向量的维度是768，这个向量也是可以学的==）得到位置信息，然后将这些位置信息加到所有的token中（注意这里是加，而不是拼接，序号1到9也只是示意一下，实际上应该是1到196），所以加上位置编码信息之后，这个序列还是197*768
- 到此就做完了整个图片的预处理，包括加上特殊的字符cls和位置编码信息，也就是说transformer输入的embedded patches就是一个197*768的tensor
- 这个tensor先过一个layer norm，出来之后还是197*768
- 然后做多头自注意力，这里就变成了三份：k、q、v，每一个都是197x768，这里因为做的是多头自注意力，所以其实最后的维度并不是768，假设现在使用的是VIsion Transformer的base版本，即多头使用了12个头，那么最后的维度就变成了768/12=64，也就是说这里的k、q、v变成了197*64，但是有12个头，有12个对应的k、q、v做自注意力操作，最后再将12个头的输出直接拼接起来，这样64拼接出来之后又变成了768，所以多头自注意力出来的结果经过拼接还是197*768
- 然后再过一层layer norm，还是197*768
- 然后再过一层MLP，这里会把维度先对应地放大，一般是放大4倍，所以就是197*3072
- 然后再缩小投射回去，再变成197*768，就输出了

![img](https://i0.hdslb.com/bfs/note/be3d91c3c79025f2700aa445a9dbf097bffcdaf0.png@700w_!web-note.webp)

- 以上就是一个Transformer block的前向传播的过程，进去之前是197*768，出来还是197*768，这个序列的长度和每个token对应的维度大小都是一样的，所以就可以在一个Transformer block上不停地往上叠加Transformer block，最后有L层Transformer block的模型就构成了Transformer encoder





3.1 Vision Transformer



38:22



![img](https://i0.hdslb.com/bfs/note/4e85307dcadfc5ca5356d974ab11768f4aa4b348.png@700w_!web-note.webp)

- Transformer从头到尾都是使用D当作向量的长度的，都是768，这个维度是不变的
- 对于位置编码信息，本文用的是标准的可以学习的1d position embedding，它也是BERT使用的位置编码。作者也尝试了了别的编码形式，比如说2d aware（它是一个能处理2d信息的位置编码），但是最后发现结果其实都差不多，没有什么区别





**消融实验（附录）**

针对特殊的class token还有位置编码，作者还做了详细的消融实验，因为对于Vision Transformer来说，**怎么对图片进行预处理以及怎样对图片最后的输出进行后处理是很关键的**，因为毕竟中间的模型就是一个标准的Transformer

1、class token

因为在本文中，想要跟原始的Transformer尽可能地保持一致，所以也使用了class token，因为class token在NLP的分类任务中也有用到（也是当作一个全局的对句子的理解的特征），本文中的class token是将它当作一个图像的整体特征，拿到这个token的输出以后，就在后面接一个MLP（MLP中是用tanh当作非线性的激活函数来做分类的预测）

- 这个class token的设计是完全从NLP借鉴过来的，之前在视觉领域不是这么做的，比如说有一个残差网络Res50，在最后一个stage出来的是一个14*14的feature map，然后在这个feature map之上其实是做了一个叫做gap（**global average pooling，全局平均池化**）的操作，池化以后的特征其实就已经拉直了，就是一个向量了，这个时候就可以把这个向量理解成一个全局的图片特征，然后再拿这个特征去做分类



对于Transformer来说，如果有一个Transformer模型，进去有n个元素，出来也有n个元素，为什么不能直接在n个输出上做全局平均池化得到一个最后的特征，而非要在前面加上一个class token，最后用class token的输出做分类？

- 通过实验，作者最后的结论是：这两种方式都可以，就是说可以通过全局平均池化得到一个全局特征然后去做分类，也可以用一个class token去做。本文所有的实验都是用class token去做的，主要的目的是跟原始的Transformer尽可能地保持一致（stay as close as possible），作者不想人觉得某些效果好可能是因为某些trick或者某些针对cv的改动而带来的，作者就是想证明，一个标准的Transformer照样可以做视觉

两种方法的效果对比如下图所示

![img](https://i0.hdslb.com/bfs/note/9a97a02920ebeea844abe15d4de7f8e6f418cb41.png@700w_!web-note.webp)

- 绿线表示全局平均池化
- 蓝线表示class token
- 可以发现到最后绿线和蓝线的效果是差不多的，但是作者指出绿线和蓝线所使用的学习率是不一样的，如果直接将蓝线的学习率拿过来使用得到的效果可能如橙线所示，也就是说需要进行好好调参



2、位置编码

作者也做了很多的消融实验，主要是三种

- 1d：就是NLP中常用的位置编码，也就是本文从头到尾都在使用的位置编码
- 2d：比如1d中是把一个图片打成九宫格，用的是1到9的数来表示图像块，2d就是使用11、12、13、21等来表示图像块，这样就跟视觉问题更加贴近，因为它有了整体的结构信息。具体的做法就是，原有的1d的位置编码的维度是d，现在因为横坐标、纵坐标都需要去表示，横坐标有D/2的维度，纵坐标也有D/2的维度，就是说分别有一个D/2的向量去表述横坐标和纵坐标，最后将这两个D/2的向量拼接到一起就又得到了一个长度为D的向量，把这个向量叫做2d的位置编码
- relative positional embedding（相对位置编码）：在1d的位置编码中，两个patch之间的距离既可以用绝对的距离来表示，又可以用它们之间的相对距离来表示（文中所提到的offset），这样也可以认为是一种表示图像块之间位置信息的方式

但是这个消融实验最后的结果也是：三种表示方法的效果差不多，如下图所示

![img](https://i0.hdslb.com/bfs/note/4de5b129fc2ee760c7d8c039e489e358a51d6477.png@700w_!web-note.webp)

- No Pos表示不加任何的位置编码，效果不太好，但也不算特别差。transformer根本没有感知图片位置的能力，在没有位置编码的情况下，还能够达到61的效果其实已经相当不错了
- 对比以上三种位置编码的形式发现，所有的performance都是64，没有任何区别
- 对此作者给出了他认为合理的解释，他所做的Vision Transformer是直接在图像块上做的，而不是在原来的像素块上做的，因为图像块很小，14x14，而不是全局的那种224x224，所以在排列组合这种小块或者想要知道这些小块之间相对位置信息的时候还是相对比较容易的，所以使用任意的位置编码都无所谓





通过以上的消融实验可以看出，class token也可以使用全局平均池化替换，最后1d的位置信息编码方式也可以用2d或者相对位置编码去替换，但是为了尽可能对标准的transformer不做太多改动，所以本文中的vision transformer还是使用的是class token和1d的位置信息编码方式





transformer encoder

transformer在现在看来是一个比较标准的操作了，作者对于transformer（或者说多头注意力机制）的解释放在附录中了





作者用整体的公式将整个过程总结了一下，如下图中的公式所示

![img](https://i0.hdslb.com/bfs/note/8b80f5899da957faadc23d70edc2ab2154fc9b4c.png@700w_!web-note.webp)

- X表示图像块的patch，一共有n个patch
- E表示线性投影的全连接层，得到一些patch embedding
- 得到patch embedding之后，在它前面拼接一个class embedding（Xclass），因为需要用它做最后的输出
- 一旦得到所有的tokens，就需要对这些token进行位置编码，所以将位置编码信息Epos也加进去
- Z0就是整个transformer的输入
- 接下来就是一个循环，对于每个transformer block来说，里面都有两个操作：一个是多头自注意力，一个是MLP。在做这两个操作之前，都要先经过layer norm，每一层出来的结果都要再去用一个残差连接
- ZL’就是每一个多头自注意力出来的结果
- ZL就是每一个transformer block整体做完之后出来的结果
- L层循环结束之后将ZL（最后一层的输出）的第一个位置上的ZL0，也就是class token所对应的输出当作整体图像的特征，然后去做最后的分类任务





**归纳偏置**

vision transformer相比于CNN而言要**少很多图像特有的归纳偏置**，比如在CNN中，locality（局部性）和translate equivariance（平移等变性）是在模型的每一层中都有体现的，这个先验知识相当于贯穿整个模型的始终



但是对于ViT来说，只有MLP layer是局部而且平移等变性的，但是自注意力层是全局的，这种图片的2d信息ViT基本上没怎么使用（就只有刚开始将图片切成patch的时候和加位置编码的时候用到了，除此之外，就再也没有用任何针对视觉问题的归纳偏置了）



而且位置编码其实也是刚开始随机初始化的，并没有携带任何2d的信息，所有关于图像块之间的距离信息、场景信息等，都需要从头开始学习



这里也是对后面的结果做了一个铺垫：vision transformer没有用太多的归纳偏置，所以说在中小数据集上做预训练的时候效果不如卷积神经网络是可以理解的





混合模型

既然transformer全局建模的能力比较强，卷积神经网络又比较data efficient（不需要太多的训练数据），所以搞出了一个混合的网络，前面是卷积神经网络，后面是transformer



作者对此做了实验：

- 原先是假设有一个图片，将它打成16*16的patch，得到了196个元素，这196个元素和全连接层做一次操作，最后得到patch embedding
- 现在不将图片打成块了，就按照卷积神经网络的方式去进行处理，将一整张图输入一个CNN，比如说Res50，最后出来一个14*14的特征图，这个特征图拉直了以后恰好也是196个元素，然后用新的到的196个元素去和全连接层做操作得到新的patch embedding

以上就是两种不同的对图片进行预处理的方式

- 一种是将图片打成patch，然后直接经过全连接层
- 另外一种就是经过一个CNN

因为这两种方式得到的序列的长度都是196，所以后续的操作都是一样的，都是直接输入一个transformer，最后再做分类





**遇到更大尺寸图片的时候如何做微调**

之前有工作说如果在微调的时候，能用比较大的图像尺寸（不是用224x224，而是用256x256，甚至更大的320*320，就会得到更好的结果）就能够得到更好的效果



vision transformer也想在在更大的尺寸上做微调，但是用一个预训练好的vision transformer其实是不太好去调整输入尺寸的。当使用更大尺寸的图片的时候，如果将patch size保持一致，但是图片扩大了，那么序列长度就增加了，所以transformer从理论上来讲是可以处理任意长度的，只要硬件允许，任意长度都可以。



但是提前预训练好的位置编码有可能就没用了，因为原来的位置编码是有明确的位置信息的意义在里面的，现在图片变大了，如果保持patch size不变的话，patch增多了，

- 这个时候位置编码该如何使用？作者发现其实做一个简单的2d的插值就可以了（使用torch官方自带的interpolate函数就可以完成了）。
- 但是这里的插值也不是想插多长就插多长，当从一个很短的序列变成一个很长的序列时，简单的插值操作会导致最终的效果下降，所以说这里的插值只是一种临时的解决方案，这也算是vision transformer在微调的时候的一个局限性
- 因为使用了图片的位置信息进行插值，所以这块的尺寸改变和抽图像块是vision transformer里唯一用到2d信息的归纳偏置的地方



### 7、实验



主要是对比了残差网络、vit和它们混合模型的表征学习能力



为了了解训练好每个模型到底需要多少数据，在不同大小的数据集上做预训练，然后在很多的数据集上做测试



当考虑到预训练的时间代价（预训练的时间长短）的时候，vision transformer表现得非常好，能在大多数数据集上取得最好的结果，同时需要更少的时间进行训练



最后作者还做了一个自监督的实验，自监督实验的结果虽然没有最好，但是还是可以，还是比较有潜力

- 时隔一年之后，MAE就证明了自监督的方式去训练ViT确实效果很好



数据集的使用方面主要是用了

- ImageNet的数据集：ImageNet-1k（最常用的有1000个类别、1300张图片）、ImageNet-21k（有21000个类别、14000张图片）
- JFT数据集：Google自己的数据集（有3亿张图片）



下游任务全部是做的分类，用的也是比较常用的数据集

- CIFAR
- Oxford Pets
- Oxford Flowers





**模型的变体**

一共有三种模型，参数如下图所示

![img](https://i0.hdslb.com/bfs/note/73acce55ebf70f9ebf01c0e70f4c53df06b6c53a.png@700w_!web-note.webp)

- Base
- Large
- Huge
- Layers：transformer block的个数
- Hidden size D：向量维度
- MLP size：
- Heads：多头自注意力中头的数量



因为本文中的模型，不光跟transformer本身有关系，还和输入有关系。当patch size大小变化的时候，模型的位置编码就不一样，所以patch size也要考虑在模型的命名里面，所以模型的命名方式就是

- vit-l 16:表示用的是一个vit large的模型，输入的patch size是16*16



transformer的序列长度其实是跟patch size成反比的，因为patch size越小，切成的块就越多，patch size越大，切成的块就越少，所以当模型用了更小的patch size的时候计算起来就会更贵，因为序列长度增加了



结果如下图所示，下表是说当它已经在大规模的数据上进行过预训练之后，在左边这一列的数据集上去做fine-tune（微调）的时候得到的表现

![img](https://i0.hdslb.com/bfs/note/9d47e062866abbd0a73c177308f19064b8d63702.png@838w_!web-note.webp)

- 上表对比了几个vit的变体和卷积神经网络（bit和noisy student）
- 和bit做对比的原因是因为bit确实是之前卷积神经网络里做得比较大的，而且也是因为他是作者团队自己本身的工作，所以正好可以拿来对比
- 和noisy student做对比是因为它是ImageNet之前表现最好的方法，它所采用的方法是用pseudo-label（伪标签）去进行self training，也就是常说的用伪标签也取得了很好的效果
- 从上表中可以看出，vit huge用比较小的patch 14*14能取得所有数据集上最好的结果。



但是因为这些数值都太接近了，仅仅相差零点几个点或者一点几个点，没有特别大的差距，所以作者觉得没有展示出vision transformer的威力，所以作者就得从另外一个角度来体现vit的优点：因为训练起来更便宜

- 作者所说的更便宜是指最大的vit huge这个模型也只需要训练2500天tpuv3天数，正好bit和noisy student也都是google的工作，也都是用tpuv3训练的，所以刚好可以拿来比较，bit用了9900天，noisy student用了一万多天，所以从这个角度上来说，vit不仅比之前bit和noisy student要训练的快，而且效果要好，所以通过这两点可以得出vit真的是比卷积神经网络要好的结论





**分析**

vision trasformer到底需要多少数据才能训练的比较好？

下图中图三是整个vision trasformer论文最重要的take home message，是作者最想让读者知道的，这张图基本上把所有的实验都快概括了

![img](https://i0.hdslb.com/bfs/note/24ec144dfcd98cfb8595688dc8a7a3285f630140.png@838w_!web-note.webp)

- 图三表示当时用不同大小的数据集的时候，比如说ImageNet是1.2m，而ImageNet-21k是14m，JFT是300m，当数据集不断增大的时候，resnet和vit到底在ImageNet的fine-tune的时候效果如何
- 图三的主要意思是说，灰色代表bit，也就是各种大小的resnet，**<font color=red size=5>最下面表示50，最上面表示152</font>**，如下图所示，他所想要展示的是在中间的灰色区域就是resnet能达到的效果范围，剩下的圆点就是各种大小不一的vision transformer

![img](https://i0.hdslb.com/bfs/note/88678bd2ad92d0f85d752cbc5e8792f666476a2e.png@838w_!web-note.webp)

- 在最小的ImageNet上做预训练时，vision transformer是完全不如resnet，vision transformer基本上所有的点都在灰色区域的下面。这说明vision transformer在中小型数据集上做预训练的时候的效果是远不如残差网络的，原因就是因为vision transformer没有使用先验知识（归纳偏置），所以它需要更大的数据去让网络学得更好
- 在ImageNet-21k上做预训练的时候，vision transformer和resnet已经是差不多了，vision transformer基本上所有的点都落在灰色区域内
- 只有当用特别大的数据集JFT-300M时，vision transformer是比bit对应的res152还要高的

总之这个图所要表达的是两个信息

- 如果想用vision transformer，那么得至少准备差不多和ImageNet-21k差不多大小的数据集，如果只有很小的数据集，还是选择使用卷积神经网络比较好
- 当已经拥有了比ImageNet-21k更大的数据集的时候，用vision transformer就能得到更好的结果，它的扩展性更好一些

其实整篇论文所讲的就是这个scaling



图四如下图右图所示，因为作者在图三中要用vision transformer跟resnet做比较，所以在训练的时候用了一些强约束（比如说dropout、weight decay、label smoothing），所以就不太好分析vision transformer模型本身的特性,所以在图四中做了linear few-shot evaluation（在拿到预训练的模型之后，直接把它当成一个特征提取器，不去fine-tune，而是直接拿这些特征做了一个just take a regression就可以了），同时作者选择了few-shot，图示中标出了5-shot，就是在ImageNet上做linear evaluation的时候，每一类随机选取了5个sample，所以这个evaluation做起来是很快的，作者用这种方式做了大量的消融实验

![img](https://i0.hdslb.com/bfs/note/50925ec9c3601353871b07864a9e1a6a91f019f9.png@838w_!web-note.webp)

- 图四中横轴表示预训练数据集的大小，这里就使用了JFT，没有用别的数据集，但是他取了一些JFT的子集：10M、30M、100M、300M，这样因为所有的数据都是从一个数据集里面得来的，就没有那么大的distribution gap，这样比较起来模型的效果就更加能体现出模型本身的特质
- 图四中的结果其实跟图三差不多，图中浅灰色的线是res50，深灰色的线是res152，当用很小的预训练的数据集的时候vision transformer是完全比不过resnet的
- 本文给出的解释是因为缺少归纳偏置和约束方法（weight decay、label smoothing等），所以就导致在10M数据集的情况下vision transformer容易过拟合，导致最后学到的特征不适合做其他任务，但是随着预训练数据集的增大，vision transformer的稳健性就提升上来了
- 但是因为这里的提升也不是很明显，作者也在最后一段写了如何用vision transformer去做这种小样本的学习是一个非常有前途的方向





由于vision transformer这篇论文之前说了，它的预训练比用卷积神经网络便宜，所以这里就需要做更多的实验来支持它的论断，因为大家对transformer的印象都是又大又贵，很难训练，下图图五中画了两个表

图五
左图的average-5就是他在五个数据集（ImageNet real、pets、flowers、CIFAR-10、CIFAR-100）上做了evaluation，然后把这个数字平均了

因为ImageNet太重要了，所以作者将ImageNet单独拎出来又画了一张表，如右图所示

但是其实这两张表的结果都差不多

蓝色的圆点表示vit

灰色的圆点表示resnet

橙色的加号表示混合模型（前面是卷积神经网络，后面是transformer）

图中大大小小的点就是各种配置下大小不一样的vision transformer的变体，或者说是resnet的变体

左右两张图中所有的模型都是在JFT 300M数据集上训练的，作者这样训练的目的不想让模型的能力受限于数据集的大小，所以说所有的模型都在最大的数据集上做预训练

上图中几个比较有意思的现象

如果拿蓝色的圆圈所表示的vit去跟灰色圆圈的resnet作比较，就会发现，在同等计算复杂度的情况下，一般transformer都是比resnet要好的，这就证明了：训练一个transformer是要比训练一个卷积神经网络要便宜的

在比较小的模型上面，混合模型的精度是非常高的，它比对应的vision transformer和resnet都要高，按道理来讲，混合模型都应该是吸收了双方的优点：既不需要太多的数据去做预训练，同时又能达到跟vision transformer一样的效果

但是当随着模型越来越大的时候，混合模型就慢慢的跟vision transformer差不多了，甚至还不如在同等计算条件下的vision transformer，为什么卷积神经网络抽出来的特征没有帮助vision transformer更好的去学习？这里作者对此也没有做过多的解释，其实怎么预处理一个图像，怎么做tokenization是个非常重要的点，之后很多论文都去研究了这个问题

如果看整体趋势的话，随着模型的不断增加，vision transformer的效果也在不停地增加，并没有饱和的现象（饱和的话一般就是增加到一个平台就不增加了），还是在不停的往上走的。但是但从这个图中来看的话，其实卷积神经网络的效果也没有饱和

分析完训练成本以后，作者也做了一些可视化，希望通过这些可视化能够分析一下vit内部的表征

vision transformer的第一层（linear projection layer，E），下图展示了E是如何embed rgb value，这里主要展示了头28个主成分，其实vision transformer学到了跟卷积神经网络很像，都是这种看起来像gabor filter，有颜色和纹理，所以作者说这些成分是可以当作基函数的，也就师叔，它们可以用来描述每一个图像块的底层的结构


位置编码是如何工作的？如下图所示，这张图描述的是位置编码的相似性，数字越大相似性越高（-1到1，cos），横纵坐标分别是对应的patch，如果是同一个坐标，自己和自己相比，相似性肯定是最高的。从图中可以发现，学到的位置编码是可以表示一些距离信息的，同时它还学习到了一些行和列的规则，每一个图像块都是同行同列的相似性更高，也就意味着虽然它是一个1d的位置编码，但是它已经学到了2d图像的距离概念，这也可以解释为什么在换成2d的位置编码以后，并没有得到效果上的提升，是因为1d已经够用了


最后作者想看一下自注意力是否起作用了，只为之所以想用transformer，就是因为自注意力的操作能够模拟长距离的关系。在NLP中，一个很长的句子里开头的一个词和结尾的一个词也能互相有关联，类比在图像里很远的两个像素点也能够做自注意力，所以作者就是想看一下自注意力到底是不是想期待的一样去工作的。下图展示的是vit large 16这个模型，vit large有24层，所以横坐标所表示的网络深度就是从0到24，图中五颜六色的点就是每一层的transformer block中多头自注意力的头，对于vit large来说一共有16个头，所以每一列其实有16个点。纵轴所表示的是mean attention distance（平均注意力的距离：假如说图上有两个点，平均注意力距离表示的就是整两个点真正的像素之间差的距离乘以他们之间的attention weights，因为自注意力是全局都在做，所以说每个像素点跟每个像素点都会有一个自注意力权重，平均注意力的距离就能反映模型到底能不能注意到两个很远的像素）。图中的规律还还是比较明显的：投机层中，有的自注意力中的头注意的距离还是挺近的，能达到20个像素，但是有的头能达到120个像素，这也就证明了自注意力真的能够在网络最底层，也就是刚开始的时候就已经能够注意到全局上的信息了，而不是像卷神经网络一样，刚开始第一层的receptive field（感受野）非常小，只能看到附近的一些像素；随着网络越来越深，网络学到的特征也会变得越来越高级，越来越具有语义信息；大概在网络的后半部分，模型的自注意力的距离已经非常远了，也就是说它已经学到了带有语义性的概念，而不是靠邻近的像素点去进行判断


为了验证上面所得到的结论，作者又画了另外一个图，如下图所示。图中是用网络中最后一层的out token所作的图，从图中可以发现，如果用输出的token的自注意力折射回原来的输入图片，可以发现模型确实是学习到了这些概念。对于全局来说，因为输出的token是融合了所有的信息（全局的特征），模型已经可以关注到与最后分类有关的图像区域






在文章的最后，作者还做了如何用自监督的方式去训练vision transformer的测试

这篇论文算上附录22页，在这么多的结果中，作者把别的结果都放到了附录里，而把自监督放到了正文中，可见它的重要性。它之所重要主要是因为在nlp领域，transformer这个模型确实起到了很大的推动作用，但另外一个真正让transformer火起来的原因其实是大规模的自监督训练，二者缺一不可。NLP中的自监督无非就是完形填空或者是预测下一个词，但是因为这篇论文主要仿照的是BERT，所以作者就想能不能也借鉴BERT这个目标函数去创建一个专属于vision的目标函数，BERT使用的就是完形填空（mask language modeling，给定一个句子，然后将一些词mask掉，然后通过一个模型，最后将它预测出来），同理，本文就仿造了一个mask patch prediction，意思就是给定一张图片，将它打成很多patch，然后将某些patch随机抹掉，通过这个模型以后，再将这些patch重建出来。

但是最后vit base 16在ImageNet只能达到80的左右的准确率，虽然相对于从头来训练vision transformer已经提高了两个点，但是跟最好的有监督的训练方式比差了4个点，所以作者将跟对比学习的结果当作是未来的工作（对比学习是去年CV圈最火的人们话题，是所有自监督学习中表现最好的，所以紧接着vit MoCo v3和DINO就出现了，这两篇论文都是用对比学习的方式去训练了一个vision transformer）





###  8、评价

这篇论文写的还是相当简洁明了的，在有这么多内容和结果的情况下，做到了有轻有重，把最重要的结果都放到了论文里，图和表也都做的一目了然

从内容上来说，可以从各个角度来进行分析、提高或者推广vision transformer

如果从任务角度来说，vision transformer只是做了分类，所以还可以拿他去做检测、分割甚至别的领域的任务

如果从改变结构的角度来讲，可以去改变刚开始的tokenization，也可以改中间的transformer block，后来就已经有人将自注意力换成了MLP，而且还是可以工作得很好（几天前，甚至有一篇论文叫做mataformer，他认为transformer真正工作的原因是transformer这个架构而不是因为某些算子，所以他就将自注意力直接换成了池化操作然后发现，用一个甚至不能学习的池化操作（文中提出了一个pool former模型）也能在视觉领域取得很好的效果），所以在模型的改进上也大有可为

如果从目标函数来讲，可以继续采用有监督，也可以尝试很多不同的自监督训练的方式

最重要的是vit打破了NLP和CV之间的鸿沟，挖了一个更大的多模态的坑，可以用它去做视频、音频，甚至还可以去做一些基于touch的信号，也就是说各种modality的信号都可以拿来使用

 



