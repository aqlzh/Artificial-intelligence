[toc]



> [深度学习复盘与论文复现B_深度学习论文复现博客-CSDN博客](https://blog.csdn.net/QuantumYou/article/details/139354339?ops_request_misc=%7B%22request%5Fid%22%3A%228B832DBE-D254-4E91-8091-D1BAFA2EDA70%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&request_id=8B832DBE-D254-4E91-8091-D1BAFA2EDA70&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-139354339-null-null.nonecase&utm_term=Convolutional Neural Networks&spm=1018.2226.3001.4450) CNN

## 论文基本知识



### 1、卷积核

在卷积神经网络（CNN）中，**滤波器（Filter）**是核心组件，它负责从输入数据（如图像）中提取不同层次的特征。滤波器也称为**卷积核（Kernel）**，通过卷积操作来捕捉图像中的局部模式，如边缘、纹理、形状等。



1. **大小（Size）**：滤波器通常是一个较小的矩阵，大小通常为 \(3 \times 3\)、\(5 \times 5\)、\(7 \times 7\) 等，大小依赖于具体的任务。
2. **数量（Number of Filters）**：每一层中可以使用多个滤波器，每个滤波器在输入数据上滑动（即卷积），从中提取不同的特征。更多的滤波器可以提取更多的特征。
3. **权重（Weights）**：滤波器的元素（权重）在训练过程中通过反向传播算法进行调整，使得CNN能够自适应学习到最佳的特征提取方式。



滤波器的工作原理

**滤波器通过与输入数据进行卷积运算，输出一个新的特征图**。卷积操作是通过将滤波器在输入数据上滑动，并对局部区域进行加权求和来实现的。具体过程如下：

1. **卷积操作**：滤波器与输入的局部区域逐个元素相乘，然后将这些乘积求和，得到输出特征图中的一个值。
2. **滑动窗口**：滤波器以一定的步长（stride）在输入图像上滑动，卷积操作会不断在不同的局部区域进行。
3. **生成特征图**：滤波器滑过输入数据的所有区域后，将输出一个特征图，该特征图表示滤波器在整个输入数据上提取到的模式。

![](https://i-blog.csdnimg.cn/blog_migrate/fc7588939aa91c81b3ad51abfc5f4f3b.gif)

卷积操作的数学公式如下：

$$
f(x, y) = \sum_{i=1}^{n}\sum_{j=1}^{m} X(x+i, y+j) \cdot W(i, j)
$$


其中：
- \(X\) 是输入数据，\(W\) 是滤波器。
- \(f(x, y)\) 是输出特征图在位置 \(x, y\) 的值。
- \(n \times m\) 是滤波器的大小。



滤波器的作用

1. **低层特征提取**：在卷积网络的前几层，滤波器通常负责提取简单的特征，如边缘、角点等。这些滤波器通常能识别出特定方向的边缘或纹理。
   
2. **高层特征提取**：随着网络层数的加深，滤波器可以提取更高级别的特征，如形状、对象的一部分等。

3. **感受野（Receptive Field）**：滤波器的大小决定了它的“感受野”，即每次卷积操作所覆盖的输入区域。随着层数增加，感受野会变大，因此滤波器能在深层网络中捕获更全局的特征。



总结

1. **滤波器**是CNN中提取特征的核心组件，卷积操作通过滤波器在输入数据上滑动，生成特征图。
2. <font color=red>**滤波器的大小**和**数量**是超参数，直接影响到模型的特征提取能力</font>
3. **多层卷积**网络能够逐步提取从低级到高级的特征，使得CNN在图像识别、分类等任务上表现优异。

滤波器的调整和选择直接影响到CNN的性能和效果，通常需要在实验中调优。



### 2、Inception

Inception模块的主要目的是通过在同一层中应用不同大小的卷积核和池化操作，提取图像的多尺度特征，从而提高网络的表达能力。



- 在Google Net  中的运用

### 3、BN

Batch Normalization（批量归一化）是一种在训练深度神经网络时常用的技术，旨在提高训练速度、稳定性和性能。它通过规范化（归一化）神经网络中间层的输入来工作，从而减少了所谓的“内部协变量偏移”（internal covariate shift），即网络中间层的输入分布随时间变化的情况。

内部协变量偏移是指在训练过程中，由于每层的参数更新，神经网络中间层的激活值的分布可能会发生变化。这可能导致训练过程中的梯度问题，比如梯度消失或梯度爆炸。

Batch Normalization 的核心思想是在网络的中间某些层（通常是卷积层或全连接层）中插入一个归一化层，对每个小批量数据的激活值进行归一化处理。归一化层会学习到两个参数，γ（gamma）和β（beta），它们允许对归一化后的数据进行缩放和偏移，以保持模型的表达能力。

Batch Normalization 的具体操作步骤如下：

1. **归一化**：对于每个小批量数据，计算其均值和方差，并使用这些统计量来归一化数据。
   $\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
   其中，\(x^{(k)}\) 是第 \(k\) 个数据点，\(\mu_B\) 是小批量数据的均值，\(\sigma_B^2\) 是小批量数据的方差，\(\epsilon\) 是一个很小的常数，用来防止除以零。

2. **缩放和偏移**：使用可学习的参数 \(\gamma\) 和 \(\beta\) 对归一化后的数据进行缩放和偏移。
   $y^{(k)} = \gamma \hat{x}^{(k)} + \beta $
   其中，\(y^{(k)}\) 是归一化层的输出。

3. **反向传播**：在训练过程中，通过反向传播算法更新 \(\gamma\) 和 \(\beta\) 的值。

Batch Normalization 的优点包括：

- **加速训练**：使得网络可以更快地收敛。
- **允许更高的学习率**：由于减少了梯度消失的问题，可以使用更大的学习率。
- **减少初始化依赖**：对权重的初始化不那么敏感。
- **作为正则化**：可以减少模型对 Dropout 的依赖，因为它本身就有一定的正则化效果。

Batch Normalization 在各种深度学习框架中都有实现，如 TensorFlow、PyTorch 等，并且已经成为许多现代神经网络架构的标准组件。



### 4、VAE



**变分自编码器（Variational Autoencoder, VAE）** 是一种生成模型，由 Kingma 和 Welling 于 2013 年提出。与传统的自编码器不同，==VAE 不仅可以进行数据的压缩与重建，还能够生成新的数据样本==。其核心思想是将数据编码为一个概率分布，然后通过该分布进行采样，从而生成新数据。



VAE 的基本概念



VAE 是一种基于概率的生成模型，结合了自编码器的特性和贝叶斯推断。其架构包括两个主要部分：

1. **编码器（Encoder）**：将输入数据映射到一个隐变量空间（通常为高斯分布）。
2. **解码器（Decoder）**：从隐变量空间中采样，通过解码器生成重建的输出。

VAE 的生成过程可以分为以下几个步骤：

1. **编码（Encoding）**：给定输入数据 \( x \)，编码器将其映射为潜在空间中的均值 \( \mu(x) \) 和标准差 \( \sigma(x) \)，并假设隐变量 \( z \) 服从高斯分布：
   
   $z \sim \mathcal{N}(\mu(x), \sigma(x))$
   
2. **采样（Sampling）**：从编码器输出的概率分布中采样隐变量 \( z \)。

3. **解码（Decoding）**：通过解码器将隐变量 \( z \) 映射回原始数据空间，生成重建的数据 \( \hat{x} \)。

VAE 的关键点：重参数技巧（Reparameterization Trick）

为了能够通过梯度下降进行训练，VAE 引入了**重参数化技巧**（Reparameterization Trick）。因为直接从高斯分布中采样 \( z \) 不能进行反向传播，重参数化技巧将采样过程拆解为一个确定性函数和一个随机变量的组合：

$z = \mu(x) + \sigma(x) \odot \epsilon$

其中 $ \epsilon \sim \mathcal{N}(0, 1) $ 是一个标准正态分布的噪声。这种方法使得 VAE 可以通过标准反向传播算法训练。



VAE 的损失函数



VAE 的损失函数包含两部分：

1. **重建损失（Reconstruction Loss）**：衡量重建数据 \( \hat{x} \) 与原始数据 \( x \) 之间的差异，通常使用均方误差或交叉熵损失。

   $\text{Reconstruction Loss} = \mathbb{E}_{q(z|x)} [\log p(x|z)]$

2. **KL 散度（KL Divergence）**：<font color=red>衡量隐变量分布 \( q(z|x) \) 与先验分布 \( p(z) \) 之间的差异</font>。通常，先验分布 \( p(z) \) 设为标准正态分布 $ \mathcal{N}(0, 1) $

   

   $D_{\text{KL}}(q(z|x) \| p(z)) = \frac{1}{2} \sum_{i=1}^{d} \left( 1 + \log(\sigma_i^2) - \mu_i^2 - \sigma_i^2 \right)
   $

因此，VAE 的总损失函数为：
$
\mathcal{L} = \text{Reconstruction Loss} + D_{\text{KL}}(q(z|x) \| p(z))
$

通过最小化这一损失函数，VAE 可以同时优化编码器和解码器，使得模型不仅能够重建输入数据，还能够从潜在空间中生成新样本。



应用

1. **图像生成**：VAE 可以从潜在空间中采样隐变量，**并通过解码器生成类似于训练数据的新图像**。
2. **数据降维**：VAE 的编码器部分可以用于将高维数据降维，同时保证潜在变量有良好的分布结构。
3. **异常检测**：由于 VAE 学习了数据的生成过程，它能够识别与训练数据分布不同的异常数据。

VAE 的优缺点

- **优点**：
  - 能够生成新数据，并且生成的数据具有很强的连贯性。
  - ==通过 KL 散度，VAE 保证了潜在空间中的点与先验分布一致，使得生成的样本在隐变量空间中是连续的==。

- **缺点**：
  - 生成的图像质量通常不如 GAN（生成对抗网络）高。
  - KL 散度的权重较难平衡，有时会出现“KL 瓶颈”问题，即模型过度依赖于重建损失，而忽略潜在变量的正则化。

总结

变分自编码器（VAE）是自编码器和生成模型的结合，它能够学习数据的分布并生成新的样本。VAE 的关键在于其通过重参数化技巧，<font color=red>使用编码器和解码器的组合来学习潜在空间中的概率分布</font>。尽管生成质量不如 GAN，但 VAE 在连贯性、理论优雅性和训练稳定性方面具有明显优势。





### 5、SGD

**随机梯度下降（Stochastic Gradient Descent, SGD）**是一种用于优化机器学习模型的常用方法，尤其在深度学习和大规模数据集训练中表现突出。它是一种基于梯度下降的优化算法，<font color=blue>区别在于每次更新参数时只使用一个或少量的样本，而不是整个数据集</font>。



工作原理：

1. **初始化参数**：首先，模型的参数随机初始化。
2. **随机选择样本**：==从训练数据集中随机选择一个样本或一小批样本（称为mini-batch）==。
3. **计算损失和梯度**：基于选定的样本，计算损失函数的值，并对模型参数求梯度。
4. **更新参数**：根据损失函数的梯度，用以下公式更新参数：
   $
   \theta = \theta - \eta \cdot \nabla L(\theta)
   $
   其中，\(\theta\) 是模型的参数，\(\eta\) 是学习率，\(\nabla L(\theta)\) 是当前样本的梯度。
5. **重复迭代**：重复步骤2-4，直到模型的参数收敛或达到设定的迭代次数。



特点与优点：

- **速度快**：相比批量梯度下降（Batch Gradient Descent），SGD每次仅使用一部分数据进行参数更新，能够快速收敛。
- **在线学习**：适用于流式数据或者数据规模非常大的场景，可以在数据逐渐到达的过程中实时更新模型。
- **避免局部最优**：由于引入了噪声，SGD可以帮助模型跳出局部最优点，找到更好的全局最优。



缺点：

- **更新不稳定**：由于每次仅使用少量样本更新参数，更新路径会有较大的波动，可能导致收敛较慢或难以找到最优解。
- **调参困难**：学习率的选择非常关键，过大可能导致不收敛，过小则收敛过慢。

<font color=red>常见改进</font>：

- **Mini-batch SGD**：通过每次使用多个样本（小批量）来更新参数，可以减少梯度的波动，同时保持SGD的高效性。
- **动量（Momentum）**：引入动量机制，以减少更新过程中的震荡并加速收敛。
- **自适应学习率**：像Adam、RMSProp这样的算法，动态调整学习率，以加速收敛。

SGD广泛用于神经网络训练等大规模机器学习任务中，是深度学习优化中的核心工具之一。



### 6、Identity mapping

**Identity Mapping**（恒等映射）是指在函数或网络层中输出保持与输入完全相同的一种映射方式。数学上，它可以表示为：
$
f(x) = x
$
其中，输入 \( x \) 与输出 \( x \) 完全一致，不进行任何修改。



在神经网络中的作用：

在深度学习中，**identity mapping** 通常出现在深度残差网络（ResNet）等架构中。==ResNet通过引入“残差连接”解决了深度神经网络中训练困难、梯度消失等问题==。残差连接实现的就是一种 **identity mapping**，即让网络层的某些部分直接将输入复制到输出，以便保留原始信息。

为什么有用？

1. **防止梯度消失**：在深层网络中，梯度会随着层数增多而逐渐衰减，导致模型训练困难。通过 identity mapping，网络可以轻松地跨层传递信息，避免梯度消失问题。
2. **简化优化问题**：在ResNet中，网络不需要学习所有层的复杂映射，只需要学习与输入的残差部分。换句话说，即使某些层不对输出做任何处理，模型也能保持有效。



ResNet中的残差块结构：

残差块可以被表示为：
$
y = f(x) + x
$
其中 \( f(x) \) 是一个非线性变换（卷积、ReLU等），而 \( x \) 是输入。通过这种设计，若学习的 \( f(x) \) 是0，网络也可以通过 identity mapping 直接将输入传递到输出，从而使得深层网络的训练更加稳定。



总结：

**Identity Mapping** 是一种保持输入与输出相同的映射方式，在深度学习中，它通过“跳跃连接”帮助缓解深层网络的训练难题，尤其是在残差网络中起到了至关重要的作用。



### 7、Bottleneck Design

**Bottleneck Design** 是在深度神经网络（特别是残差网络，ResNet）中常用的一种架构设计，其目的是减少网络的计算量和参数，同时保持模型的表示能力。



“Bottleneck” 是一种通过缩减网络内部维度来减少计算复杂度的设计。具体来说，**bottleneck block** 通常采用一个三层的结构，==通过先缩减维度，再恢复维度的方式进行卷积运算==：

1. **1x1卷积（降维）**：先用一个 1x1 的卷积核来减少输入通道的数量，降低计算量。
2. **3x3卷积（核心处理）**：再进行标准的 3x3 卷积操作，保持特征的空间结构。
3. **1x1卷积（升维）**：最后再通过 1x1 的卷积核把通道数还原到原始维度。

整个过程类似于“压缩-处理-解压”，中间的 3x3 卷积操作可以在低维空间中完成，从而显著减少计算复杂度。



公式：

假设输入通道数为 \( C \)，bottleneck block 的设计可以表示为：
- **降维（1x1卷积）：** 将通道数从 \( C \) 减少到一个较小的数（如 \( C/4 \)）。
- **3x3卷积：** 在低维空间进行卷积操作。
- **升维（1x1卷积）：** 将通道数恢复到原始的 \( C \)。

为什么使用Bottleneck设计？

1. **减少计算成本**：普通卷积操作随着通道数的增加，计算成本会成倍增长。bottleneck通过降低中间维度，使得计算量大大减少，尤其是在处理高维输入时。
   
2. **参数减少**：这种设计不仅减少了计算，还显著降低了模型的参数量，从而加快训练速度，同时减少过拟合风险。

3. **保持表示能力**：尽管中间维度被压缩，但由于有降维和升维的操作，网络依然能够有效地保持特征表达能力。

应用：

Bottleneck Design 在深度残差网络（ResNet）中广泛使用，特别是在 ResNet-50、ResNet-101、ResNet-152 这些较深的模型中，bottleneck block 用于构建残差模块。这种设计使得网络可以在增加深度的同时，避免模型过于庞大，提升训练和推理的效率。



### 8、NCE



在机器学习中，**NCE（Noise Contrastive Estimation）** 是一种用于**概率模型**的高效学习方法，尤其在**自然语言处理**和**深度学习**中得到广泛应用。<font color=red>NCE 的主要思想是将**密度估计问题**转化为一个**分类问题**，通过对比**真实样本**和**噪声样本**来学习模型的参数</font>。



NCE 的主要思想

NCE 的核心理念是：与其直接计算数据的精确概率分布，不如通过引入噪声分布，将密度估计简化为一个**二分类问题**，即区分“**真实样本**”和“**噪声样本**”。

- **真实样本**：来自训练数据的样本，符合模型的真实分布。
- **噪声样本**：由某个已知的简单分布（例如均匀分布）生成的样本。

通过这个对比，NCE 避免了计算复杂的**归一化常数**，从而提高了计算效率。



NCE 的应用场景

NCE 经常用于以下场景：

1. **词向量学习**：如在 Word2Vec 中，NCE 被用来高效地训练语言模型，以降低计算复杂度。
2. **语言模型**：NCE 通过最大化对真实数据与噪声样本的区分，帮助语言模型学习词的概率分布。
3. **生成模型**：在生成模型的训练中，NCE 可以作为一种近似方法来优化难以计算的似然估计。



NCE 的好处

- **减少计算复杂度**：==相比于传统的最大似然估计（MLE），NCE 不需要计算复杂的归一化常数==。
- **高效训练**：尤其在处理大规模数据或词汇量时，NCE 能显著减少训练时间。
- **灵活性**：NCE 允许引入不同的噪声分布，使其可以适应不同类型的模型和数据。

NCE 本质上是通过噪声对比来简化概率模型的训练过程，并且已经成为深度学习中高效概率估计的重要工具。





### 9、KL 散度和JS 散度



==KL 散度 (Kullback-Leibler Divergence)==



KL 散度（Kullback-Leibler Divergence）是度量两个概率分布之间差异的非对称度量。它描述了一个**真实分布 P** 和一个**近似分布 Q** 之间的差异。KL 散度不是真正的“距离”，因为它不满足对称性，也不满足三角不等式。

KL 散度的定义为：
$
D_{KL}(P \| Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
$
或者在连续情况下：
$
D_{KL}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx
$
其中：

- \( P(x) \) 是真实的概率分布。
- \( Q(x) \) 是近似的概率分布。
- \( \log \frac{P(x)}{Q(x)} \) 是两个分布之间的对数比例（它衡量每一个点上这两个分布的差异）。

**直观理解**：

- KL 散度衡量的是：使用分布 \( Q \) 来“替代”分布 \( P \) 所导致的信息损失。
- 如果 \( P(x) = Q(x) \) 对所有 \( x \) 都成立，则 \( D_{KL}(P \| Q) = 0 \)，表示两个分布完全一致。
- \( D_{KL}(P \| Q) \) 一般为正数，且 \( D_{KL}(P \| Q) \neq D_{KL}(Q \| P) \)，因此 KL 散度是不对称的。

**应用**：

1. **信息论**：KL 散度最早出现在信息论中，作为两种概率分布（或事件）的“相对熵”。
2. **机器学习**：在生成模型中，特别是变分自动编码器（VAE）中，KL 散度用于衡量潜在变量分布与预设分布（如标准正态分布）的差异。
3. **概率模型**：在贝叶斯推理中，KL 散度常用于衡量后验分布与先验分布之间的差异。



==JS 散度 (Jensen-Shannon Divergence)==



JS 散度（Jensen-Shannon Divergence）是 KL 散度的对称化版本，解决了 KL 散度的不对称问题。它通过引入两个分布的“中间分布”来衡量两者的差异，是一种对称的、有限的散度度量。

JS 散度的定义为：
$
D_{JS}(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M)
$
其中 $M = \frac{1}{2}(P + Q) $ 是 \( P \) 和 \( Q \) 的中间分布。

JS 散度的特点：
- **对称性**：$D_{JS}(P \| Q) = D_{JS}(Q \| P) $，这是相对于 KL 散度的一大优点。
- **有限值**：JS 散度是有限的，且其值在 \( [0, 1] \) 之间。KL 散度在某些情况下会趋向无穷大，而 JS 散度则受限于 1。
- **平滑**：通过在两个分布之间引入一个中间分布 \( M \)，JS 散度对异常值的敏感性较低。

**直观理解**：

- JS 散度的本质是衡量 \( P \) 和 \( Q \) 之间的平均散度，即两者相对于它们“均值”分布 \( M \) 的距离之和。
- 如果两个分布 \( P \) 和 \( Q \) 完全一致，那么 JS 散度为 0；如果它们完全不同，JS 散度则接近 1。

**应用**：

1. **文本分析与分类**：JS 散度经常用于自然语言处理和文本分析中，来衡量两个文本的词频分布之间的差异。
2. **聚类分析**：在聚类任务中，JS 散度可以用来评估不同簇的分布是否相似。
3. **生成模型评估**：在 GAN（生成对抗网络）中，JS 散度用于衡量生成的分布与真实数据分布之间的差异。



KL 散度与 JS 散度的区别

1. **对称性**：KL 散度不对称，JS 散度对称。

2. **范围**：KL 散度的值可以是正无穷，而 JS 散度的值始终是有限的，且通常在 \( [0, 1] \) 之间。

3. **应用场景**：KL 散度通常用于机器学习中的概率推断和生成模型；JS 散度则更常用于衡量两种分布的相似度，特别是在文本、图像等任务中。

   

   总结

- **KL 散度**：衡量两个概率分布之间的信息差异，主要关注的是信息损失。它对分布不对称，因此 \( D_{KL}(P \| Q) \neq D_{KL}(Q \| P) \)。
- **JS 散度**：是一种对称、平滑、有限的散度度量，适用于衡量两者的对称差异，尤其在生成模型和分类任务中应用广泛。

两者都是衡量概率分布差异的重要工具，根据具体任务的需求，可以选择合适的散度度量。

## 一、AlexNet  阅读

> 文章链接 [ImageNet classification with deep convolutional neural networks (acm.org)](https://dl.acm.org/doi/pdf/10.1145/3065386)

### 1、introduction

**第一段**

一篇论文的第一段通常是讲个故事

- ==做什么研究==
- 哪个方向
- 这个方向有什么东西
- ==为什么很重要==



**第二段**

- 描述了怎么做神经网络，这里只介绍了CNN

- 写论文的时候，<font color=red>千万不要只说自己这个领域这个小方向大概怎么样，还要提到别的方向怎么样</font>



**第三段**

CNN虽然很好，但是很难训练，但是现在有GPU了，GPU算力能够跟上，所以能够训练很大的东西，**而且数据集够大，确实能够训练比较大的CNN**



前三段基本描述了

- 我做了什么东西
- 为什么能做



**第四段**

paper的贡献

- 训练了一个最大的的神经网络，然后取得了特别好的结果
- 实现了GPU上性能很高的一个2D的卷积
- 网络有一些新的和不常见的一些特性，能够提升性能，降低模型的训练时间
- 使用了什么过拟合的方法使得模型变得更好
- 模型具有5个卷积层，3个全连接层，发现深度很重要，移掉任何一层都不行



结果很好，但是还是有新东西在里面的，如果就结果很好，没有新东西，大概是不会称为奠基作



### 2、the dataset



大概描述了一下所用的数据集



重点是最后一段：ImageNet中图片的分辨率是不一样的，因此将每张图片变成了一个256*256的图片：

- **将图片的短边减少到256，长边是保证高宽比不变的情况下也往下降**，长边如果依然多出来的话，如果多于256的话，<font color=red>就以中心为界将两边裁掉</font>，裁成一个256*256的图片
- 没有做任何的预处理，只是对图片进行了裁剪
- 网络是在raw RGB Value上训练的
- 当时做计算机视觉都是将特征抽出来，抽SIFT也好，抽别的特征也好（imagenet数据集也提供了一个==SIFT==版本的特征），这篇文章说不要抽特征，直接是在原始的Pixels上做了
- <font color=blue>在之后的工作里面基本上主要是**end to end（端到端）**：及那个原始的图片或者文本直接进去，不做任何的特征提取，神经网络能够帮助你完成这部分工作</font>





### 3、the architecture



讲整个网络的架构

- relu非线性激活函数
- 使用了多GPU进行训练
- 正则化、归一化
- overlapping pooling
- 总体架构





![img](https://i0.hdslb.com/bfs/note/fec00a250446e40b26248c49b4e86e1d215d562b.png@630w_!web-note.webp)

- 方框表示每一层的输入和输出的数据的大小
- 输入的图片是一个高宽分别为224*224的3通道RGB图片
- **第一层卷积：卷积的窗口是11*11，有48个输出通道，stride等于4**
- 有两个GPU，GPU1和GPU0都有自己的卷积核参数

![img](https://i0.hdslb.com/bfs/note/787fdba5c593948017ec66d0bc130dc6187dcac3.png@630w_!web-note.webp)

- 第一个卷积层在两个GPU上各有一个
- 第二个卷积层是在每个GPU把当前的卷积结果拿过来（GPU0的第二个卷积层读的是GPU0的第一个卷积层的卷积结果，GPU0和GPU1之间没有任何通讯）
- 到第三个卷积层的时候，GPU还是每个GPU上有自己的卷积核，但是每个卷积核会同时将第二个卷积层中GPU0和GPU1的卷积结果作为输入，两个GPU之间会通讯一次
- 第4、5个卷积层之间没有任何通讯
- 每个卷积层的通道数是不一样的，通道数有所增加，高和宽也有所变化
- 高宽慢慢变小、深度慢慢增加，随着深度的增加，慢慢地将空间信息压缩，直到最后每一个像素能够代表前面一大块的像素，然后再将通道数慢慢增加，可以认为每个通道数是去看一种特定的模式（例如192个通道可以简单地认为，能够识别图片中的192种不同的模式）
- 慢慢将空间信息压缩，语义空间慢慢增加，到最后卷积完之后，进入全连接层
- 全连接层中又出现了GPU之间的通讯，全连接层的输入是每个GPU第五个卷积的输出合并起来做全连接

![img](https://i0.hdslb.com/bfs/note/ccc807a47db1ad370b23c9552cc75f226c50f89e.png@630w_!web-note.webp)

- 最后进入分类层的时候，变成了一个4096长的向量，每一块来自两个GPU，每片是2048，最后拼起来，所以一张图片会表示成一个4096维的向量，最后用一个线性分类做链接
- 深度学习的主要作用是将一张输入的图片，通过**卷积、池化、全连接**等一系列操作，将他压缩成一个长为4096的向量，这个向量能够将中间的语义信息都表示出来（将一个人能够看懂的像素通过一系列的特征提取变成了一个长为4096的机器能够看懂的东西，这个东西可以用来做搜索、分类等）
- 整个机器学习都可以认为是一个知识的压缩过程，不管是图片、语音还是文字或者视频，通过一个模型最后压缩成一个向量，然后机器去识别这个向量，然后在上面做各种事情
- 模型并行（model parallel）：现在在计算机视觉里面用的不多，但是在自然语言处理方面又成为主流了（将模型切开进行训练）



### 4、reducing overfitting



第四章讲述了如何降低过拟合



数据增强（data augmentation）

- 把一些图片人工地变大
- 在图片中随机地抠出一部分区域，做一张新的图片
- 把整个RGB的颜色通道channel上做一些改变，这里使用的是一个**PCA（主成分分析）**的方法，颜色会有不同，因此每次图片跟原始图片是有一定的不同的





dropout

- 随机的把一些隐藏层的输出变成用50%的概率设为0，每一次都是把一些东西设置为0，所以模型也就发生了变化，每次得到一个新的模型，但是这些模型之间权重是共享的除了设置成0的，非0的东西都是一样的，这样就等价于做了模型融合
- 后来大家发现dropout其实也不是在做模型融合，==更多的dropout就是一个正则项（dropout在现行模型上等价于一个L2正则项）==
- 这里将dropout用在了前面的两个全连接层上面
- 文章说没有dropout的话，overfitting会非常严重，有dropout的话，训练会比别人慢两倍
- 现在CNN的设计通常不会使用那么大的全连接层，所以dropout也不那么重要，而且GPU、内存也没那么吃紧了
- dropout在全连接上还是很有用的，在RNN和Attension中使用的非常多



### 5、details of learning



讲述了模型是如何训练的

- 使用SGD（随机梯度下降）来进行训练，SGD调参相对来说可能会比较难调，后来发现SGD里面的噪音对模型的泛化性其实是有好处的，所以现在深度学习中普遍使用SGD对模型进行训练。在这个文章之后SGD基本上在机器学习界成为了最主流的一个优化算法
- 批量大小是128
- momentum是0.9
- weight decay是0.0005，<font color=red>也就是L2正则项，但是这个东西不是加在模型上，而是加在优化算法上</font>，虽然他们两个是等价关系，但是因为深度学习的学习，所以大家现在基本上把这个东西叫做weight decay了
- momentum也是因为这篇文章之后用的特别多，虽然在2010年的时候有大量的加速算法，里面有很fancy的各种加速SGD算法，但是现在看起来似乎用一个简单的momentum也是不错的
- momentum实际上是，当优化的表面非常不平滑的时候，冲量使得不要被当下的梯度过多的误导，可以保持一个冲量从过去那个方向，沿着一个比较平缓的方向往前走，这样子不容易陷入到局部最优解
- 权重用的是一个均值为0，方差为0.01的高斯随机变量来初始化（0.01对很多网络都是可以的，但是如果特别深的时候需要更多优化，但是对于一些相对简单的神经网络，0.01是一个不错的选项）
- 现在就算是比较大的那些BERT，也就是用了0.02作为随机的初始值的方差
- 在第二层、第四层和第五层的卷积层把初始的偏移量初始化成1，剩下的全部初始化成0
- 每个层使用同样的学习率，从0.01开始，然后呢如果验证误差不往下降了，就手动的将他乘以0.1，就是降低十倍
- ResNet中，每训练120轮，学习率每30轮就下降0.1另外一种主流的做法就是，前面可以做得更长一点，必须能够60轮或者是100轮，然后再在后面下降
- 在Alex之后的很多训练里面，都是做规则性地将学习率往下下降十倍，这是一个非常主流的做法，但是现在很少用了，现在使用更加平滑的曲线来降低学习率，比如果用一个cos的函数比较平缓地往下降。一开始的选择也很重要，如果选的太大可能会发生爆炸，如果太小又有可能训练不动，所以现在主流的做法是学习率从0开始再慢慢上升，慢慢下降

![img](https://i0.hdslb.com/bfs/note/c490536f8d63015d57bf2564fd9e249d6e3f3aa8.png@630w_!web-note.webp)

- 模型训练了90个epoch，然后每一遍用的是ImageNet完整的120万张图片，需要5-6天在两个GTX GPU上训练





### 6、result



- 有时候结果可能不重要

- 有些东西可能还不是很理解，可以去看文章所引用的文章





## 二、ResNet阅读

- xavier初始化

### 1、introduction

- 深度神经网络好在可以加很多层把网络变得特别深，==然后不同程度的层会得到不同等级的feature，比如低级的视觉特征或者是高级的语义特征==



提出问题：随着网络越来越深，<font color=red>梯度就会出现爆炸或者消失</font>

- 解决他的办法就是：1、*在初始化的时候要做好一点，就是权重在随机初始化的时候，权重不要特别大也不要特别小*。2、在中间加入一些normalization，包括BN（batch normalization）可以使得校验每个层之间的那些输出和他的梯度的均值和方差相对来说比较深的网络是可以训练的，避免有一些层特别大，有一些层特别小。使用了这些技术之后是能够训练（能够收敛），虽然现在能够收敛了，但是当网络变深的时候，性能其实是变差的（精度会变差）
- 文章提出出现精度变差的问题不是因为层数变多了，模型变复杂了导致的过拟合，而是因为训练误差也变高了（overfitting是说训练误差变得很低，但是测试误差变得很高），训练误差和测试误差都变高了，所以他不是overfitting。虽然网络是收敛的，但是好像没有训练出一个好的结果





深入讲述了**深度增加了之后精度也会变差** 

- 考虑一个比较浅一点的网络和他对应的比较深的版本（在浅的网络中再多加一些层进去），如果浅的网络效果还不错的话，深的网络是不应该变差的：深的网络新加的那些层，总是可以把这些层学习的变成一个**identity mapping**（输入是x，输出也是x，等价于可以把一些权重学成比如说简单的n分之一，是的输入和输出是一一对应的），但是实际情况是，虽然理论上权重是可以学习成这样，但是实际上做不到：假设让SGD去优化，深层学到一个跟那些浅层网络精度比较好的一样的结果，上面的层变成identity（相对于浅层神经网络，深层神经网络中多加的那些层全部变成identity），这样的话精度不应该会变差，应该是跟浅层神经网络是一样的，但是实际上SGD找不到这种最优解
- 这篇文章提出显式地构造出一个identity mapping，使得深层的神经网络不会变的比相对较浅的神经网络更差，它将其称为deep residual learning framework
- 要学的东西叫做H（x），假设现在已经有了一个浅的神经网络，他的输出是x，然后要在这个浅的神经网络上面再新加一些层，让它变得更深。新加的那些层不要直接去学H（x），而是应该去学H（x）-x，x是原始的浅层神经网络已经学到的一些东西，新加的层不要重新去学习，而是去学习学到的东西和真实的东西之间的残差，最后整个神经网络的输出等价于浅层神经网络的输出x和新加的神经网络学习残差的输出之和，将优化目标从H（x）转变成为了H（x）-x

![img](https://i0.hdslb.com/bfs/note/0a418ebf24535ae9494157b84c95460d67c4f11a.png@706w_!web-note.webp)

- 上图中最下面的红色方框表示所要学习的H（x）
- 蓝色方框表示原始的浅层神经网络
- 红色阴影方框表示新加的层
- o表示最终整个神经网络的输出
- 这样的好处是：只是加了一个东西进来，没有任何可以学的参数，不会增加任何的模型复杂度，也不会使计算变得更加复杂，而且这个网络跟之前一样，也是可以训练的，没有任何改变





非常深的residual nets非常容易优化，但是如果不添加残差连接的话，效果就会很差。越深的网络，精度就越高



<font color=red>introduction是摘要的扩充版本，也是对整个工作比较完整的描述</font>



### 2、related work



一篇文章要成为经典，不见得一定要提出原创性的东西，很可能就是把之前的一些东西很巧妙的放在一起，能解决一个现在大家比较关心难的问题





残差连接如何处理输入和输出的形状是不同的情况：

- 第一个方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加
- 第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做1*1的卷积层，这个卷积层的特点是在空间维度上不做任何东西，主要是在通道维度上做改变。所以只要选取一个1*1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上





### 3、Deep Residual Learning

implementation中讲了实验的一些细节

- 把短边随机的采样到256和480（AlexNet是直接将短边变成256，而这里是随机的）。随机放的比较大的好处是做随机切割，切割成224*224的时候，随机性会更多一点
- 将每一个pixel的均值都减掉了
- 使用了颜色的增强（AlexNet上用的是PCA，现在我们所使用的是比较简单的RGB上面的，调节各个地方的亮度、饱和度等）
- 使用了BN（batch normalization）
- 所有的权重全部是跟另外一个paper中的一样（作者自己的另外一篇文章）。注意写论文的时候，尽量能够让别人不要去查找别的文献就能够知道你所做的事情
- 批量大小是56，学习率是0.1，然后每一次当错误率比较平的时候除以10
- 模型训练了60*10^4个批量。建议最好不要写这种iteration，因为他跟批量大小是相关的，如果变了一个批量大小，他就会发生改变，所以现在一般会说迭代了多少遍数据，相对来说稳定一点
- 这里没有使用dropout，因为没有全连接层，所以dropout没有太大作用
- 在测试的时候使用了标准的10个crop testing（给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后再每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。
- 采样的时候是在不同的分辨率上去做采样，这样在测试的时候做的工作量比较多，但是在实际过程中使用比较少







### 4、experiments



- 如何评估ImagNet
- 各个不同版本的ResNet是如何设计的

首先阐述了ImageNet

描述了plain networks

没有带残差的时候，使用了一个18层和34层

![img](https://i0.hdslb.com/bfs/note/bcecbccc1aeaeb5620412f501396378706cbd175.png@686w_!web-note.webp)

- 上表是整个ResNet不同架构之间的构成信息（5个版本）
- 第一个7*7的卷积是一样的
- 接下来的pooling层也是一样的
- 最后的全连接层也是一样的（最后是一个全局的pooling然后再加一个1000的全连接层做输出）
- 不同的架构之间，主要是中间部分不一样，也就是那些复制的卷积层是不同的
- conv2.x：x表示里面有很多不同的层（块）
- 【3*3,64】:46是通道数
- 模型的结构为什么取成表中的结构，论文中并没有细讲，这些超参数是作者自己调出来的，实际上这些参数可以通过一些网络架构的自动选取
- **flops**：整个网络要计算多少个浮点数运算。卷积层的浮点运算等价于输入的高乘以宽乘以通道数乘以输出通道数再乘以核的窗口的高和宽



表1：ImageNet的架构

表1详细列出了用于ImageNet分类任务的不同残差网络（ResNet）的架构。这些架构通过使用不同数量的残差模块来构建不同深度的网络。以下是表1中的主要内容：

- **layer name**: 层的名称，表示网络中不同层次的名称。
- **output size**: 该层输出的特征图尺寸。
- **18-layer, 34-layer, 50-layer, 101-layer, 152-layer**: 这些列分别展示了不同深度的残差网络架构，包括18层、34层、50层、101层和152层。

每个层级包含的详细信息如下：

- **卷积层**: 表示为 `3x3 conv, 64` 意味着这是一个3x3的卷积核，有64个滤波器的卷积层。
- **步幅**: 如 `/2` 表示该层操作会将宽度和高度缩小一半。
- **池化层**: 表示为 `3x3 max pool, stride 2` 表示使用3x3的最大池化，步幅为2。
- **线性层**: 如 `1x1, 256` 表示1x1的卷积核用于改变特征图的深度，这里是将特征图深度从64变为256。
- **平均池化层**: `1x1 average pool` 表示1x1的平均池化层。
- **全连接层**: `fc 1000` 表示1000个神经元的全连接层。

**FLOPs**: 表示每层操作的浮点运算次数，用于衡量计算复杂度。



图4：在ImageNet上的训练情况

图4展示了18层和34层的普通网络（plain networks）与残差网络（ResNet）在ImageNet数据集上的训练和验证误差对比。

- **左侧**: 展示了18层和34层普通网络的训练过程。图中细线表示训练误差，粗线表示验证误差。可以观察到，随着网络深度的增加，34层网络的训练误差和验证误差都比18层网络要高，这表明普通网络在增加深度时会遇到优化难题。

- **右侧**: 展示了18层和34层残差网络的训练过程。同样，细线表示训练误差，粗线表示验证误差。与普通网络相反，34层残差网络不仅训练误差比18层残差网络要低，而且验证误差也更低。这说明残差网络能够通过增加深度来提高准确率，并且解决了普通网络遇到的优化难题。

总结

表1和图4共同展示了残差网络架构的细节以及它们在ImageNet数据集上的性能表现。通过引入残差学习，可以有效地训练更深的网络，并在图像识别任务中取得更好的性能。



![img](https://i0.hdslb.com/bfs/note/316285db5ae7f1c4dcab40937540681a5b0a7f04.png@686w_!web-note.webp)

- 上图中比较了18层和34层在==有残差连接和没有残差连接的结果==
- 左图中，红色曲线表示34的验证精度（或者说是测试精度）
- 左图中，粉色曲线表示的是34的训练精度
- 一开始训练精度是要比测试精度高的，因为在一开始的时候使用了大量的数据增强，使得寻来你误差相对来说是比较大的，而在测试的时候没有做数据增强，噪音比较低，所以一开始的测试误差是比较低的
- 图中曲线的数值部分是由于学习率的下降，每一次乘以0.1，对整个曲线来说下降就比较明显。为什么现在不使用乘0.1这种方法：在什么时候乘时机不好掌控，如果乘的太早，会后期收敛无力，晚一点乘的话，一开始找的方向更准一点，对后期来说是比较好的
- 上图主要是想说明在有残差连接的时候，34比28要好；另外对于34来说，有残差连接会好很多；其次，有了残差连接以后，收敛速度会快很多，核心思想是说，在所有的超参数都一定的情况下，有残差的连接收敛会快，而且后期会好





<font color=red>输入输出形状不一样的时候怎样做残差连接</font>

- 填零
- 投影
- 所有的连接都做投影：就算输入输出的形状是一样的，一样可以在连接的时候做个1*1的卷积，但是输入和输出通道数是一样的，做一次投影



对比以上三种方案

![img](https://i0.hdslb.com/bfs/note/8c75c28d1b2ec0943678d40962aa0c5101d4fd4f.png@686w_!web-note.webp)

- A表示填0
- B表示在不同的时候做投影
- C表示全部做投影
- B和C的表现差不多，但是还是要比A好一点
- B和C虽然差不多，但是计算复杂度更高，B对计算量的增加比较少，作者采用了B





怎样构建更深的ResNet

如果要做50或者50层以上的，会引入bottleneck design

![img](https://i0.hdslb.com/bfs/note/9a6493ed10b6a1cb12739837870d10c86c030419.png@686w_!web-note.webp)

- 左图是之前的设计，当通道数是64位的时候，通道数不会发生改变

- 如果要做到比较深的话，可以学到更多的模式，可以把通道数变得更大，右图从64变到了256

- 当通道数变得更大的时候，计算复杂度成平方关系增加，这里通过1个1*1的卷积，将256维投影回到64维，然后再做通道数不变的卷积，然后再投影回256（将输入和输出的通道数进行匹配，便于进行对比）。等价于先对特征维度降一次维，在降一次维的上面再做一个空间上的东西，然后再投影回去

- 虽然通道数是之前的4倍，但是在这种设计之下，二者的算法复杂度是差不多的

  32:25

  

>每一个卷积核的通道数增加到四倍，为了保持**输出通道数**不变，卷积核数量也变为四倍，一共十六倍
>
>ImageNet标号的错误率本来挺高的，估计有1%



CIFAR-10是一个很小的数据集，跑起来相对来说比较容易，32*32，五万个样本，10类的数据集





在整个残差连接，如果后面新加上的层不能让模型变得更好的时候，因为有残差连接的存在，新加的那些层应该是不会学到任何东西，应该都是靠近0的，这样就等价于就算是训练了1000层的ResNet，但是可能就前100层有用，后面的900层基本上因为没有什么东西可以学的，基本上就不会动了





这篇文章没有结论





==**mAP**：目标检测上最常见的一个精度，锚框的平均精度，越高越好==





**为什么ResNet训练起来比较快？**

- 一方面是因为梯度上保持的比较好，新加一些层的话，加的层越多，梯度的乘法就越多，因为梯度比较小，一般是在0附近的高斯分布，所以就会导致在很深的时候就会比较小（梯度消失）。虽然batch normalization或者其他东西能够对这种状况进行改善，但是实际上相对来说还是比较小，但是如果加了一个ResNet的话，它的好处就是在原有的基础上加上了浅层网络的梯度，深层的网络梯度很小没有关系，浅层网络可以进行训练，变成了加法，一个小的数加上一个大的数，相对来说梯度还是会比较大的。也就是说，不管后面新加的层数有多少，前面浅层网络的梯度始终是有用的，这就是从误差反向传播的角度来解释为什么训练的比较快

![img](https://i0.hdslb.com/bfs/note/9793eefee9774a679bd59f35543ba79ce40cd3b5.png@686w_!web-note.webp)

- 在CIFAR上面加到了1000层以上，没有做任何特别的regularization，然后效果很好，overfitting有一点点但是不大。SGD收敛是没有意义的，SGD的收敛就是训练不动了，收敛是最好收敛在比较好的地方。做深的时候，用简单的机器训练根本就跑不动，根本就不会得到比较好的结果，所以只看收敛的话意义不大，但是在加了残差连接的情况下，因为梯度比较大，所以就没那么容易收敛，所以导致一直能够往前（SGD的精髓就是能够一直能跑的动，如果哪一天跑不动了，梯度没了就完了，就会卡在一个地方出不去了，所以它的精髓就在于需要梯度够大，要一直能够跑，因为有噪音的存在，所以慢慢的他总是会收敛的，所以只要保证梯度一直够大，其实到最后的结果就会比较好）

![img](https://i0.hdslb.com/bfs/note/4f84d817570adb88364b9dad35af6db13710a6a4.png@686w_!web-note.webp)





为什么ResNet在CIFAR-10那么小的数据集上他的过拟合不那么明显？

open question

虽然模型很深，参数很多，但是因为模型是这么构造的，所以使得他内在的模型复杂度其实不是很高，也就是说，很有可能加了残差链接之后，使得模型的复杂度降低了，一旦模型的复杂度降低了，其实过拟合就没那么严重了

- 所谓的模型复杂度降低了不是说不能够表示别的东西了，而是能够找到一个不那么复杂的模型去拟合数据，就如作者所说，不加残差连接的时候，理论上也能够学出一个有一个identity的东西（不要后面的东西），但是实际上做不到，因为没有引导整个网络这么走的话，其实理论上的结果它根本过不去，所以一定是得手动的把这个结果加进去，使得它更容易训练出一个简单的模型来拟合数据的情况下，等价于把模型的复杂度降低了





这篇文章的residual和gradient boosting是不一样的

- gradient boosting是在标号上做residual
- 这篇文章是在feature维度上





## 三、Transformer 阅读

**transformer**



- 最近三年以内深度学习里面最重要的文章之一
- 可以认为是开创了继MLP、CNN、RNN之后的第四大类模型

### 1、标题：Attention is all you need



作者后面打上星号表示同等贡献

![img](https://i0.hdslb.com/bfs/note/308006683fd345e35b1f028664a22a545a1d01c4.png@630w_!web-note.webp)



### 2、摘要



- 在主流的序列转录模型里面，主要是依赖于比较复杂的循环或者是卷积神经网络，一般是使用encoder和decoder的架构

- 序列转录模型：给定一个序列，然后生成另外一个序列，比如机器翻译



在性能最好的模型之中，通常也会在编码器和解码器之间使用注意力机制



这篇文章提出了一个新的简单的架构（simple，之前都倾向于写成novel），这个模型仅仅依赖于注意力机制，而没有用之前的循环或者卷积



做了两个机器翻译的实验，显示这个模型在性能上特别好，可以并行度更好然后使用更少的时间来训练



模型达到了28.4的BLEU

- BLEU score：机器翻译里面常用的一个衡量标准



这篇文章一开始写的时候是针对机器翻译这个小任务写的，但是随着之后BERT、GPT把这个架构用在更多的语言处理的任务上的时候，整个工作就出圈了，最近用在了图片和video上面，几乎什么东西都能用



### 3、导言



==transformer这个模型是第一个仅仅使用注意力机制做序列转录的模型，将之前的循环层全部换成了multi-headed self-attetion==



在机器翻译的这个任务上面，transformer能够训练的比其他的架构都要快很多，而且在实际的结果上确实是效果更好



作者对于这种纯基于注意力机制的模型感到非常激动，作者想要把它用在文本以外的别的数据上面，使得生成不那么时序化也是另外的一个研究方向



这篇文章所有的代码放在tensor2tensor这个库里面，作者将整个代码放在了结论的最后（如果有代码的话，通常会把这个代码放在摘要的最后一句话，因为现在神经网络的文章里面细节通常是比较多的，简简单单的一篇文章很难把所有的细节都讲清楚，所以最好第一时间公布代码，让别人能够很方便地复现文章，然后这样就能够扩大文章的影响力）



### 4、结论



这里的导言写的比较短，基本可以认为是前面摘要的前面一半的扩充



在时序模型里面，2017最常用的是RNN、LSTM、GRU，有两个比较主流的模型：

- 一个叫做语言模型
- 另一个是当输出结构化信息比较多的时候会使用**编码器和解码器架构**



**RNN的特点**

- RNN中给定一个序列，它的计算是把这个序列从左到右一步一步往前做，假设序列是一个句子的话，就是一个一个词，对第t个词会计算出一个输出叫做ht（也叫做它的隐藏状态），ht是由前面一个词的隐藏状态（h（t-1））和当前第t个词本身决定的，这样就可以把前面学到的历史信息通过h（t-1）放到当下，然后和当前的词做一些计算，然后得到输出



**RNN如何有效处理时序信息的关键**：他将之前的信息全部放在隐藏状态里，然后一个一个放下去。他的问题也来自于这里：

1. <font color=red>他是一个一步一步计算（时序）的过程，比较难以并行，在时间上难以并行，使得在计算上性能比较差</font>
2. 历史信息是一步一步地往后传递的，如果时序比较长的话，那么很早期的时序信息在后面的时候可能丢掉，如果不想丢掉的话，==可能需要ht要比较大，如果要做比较大的ht，在每一个时间步都得把他存下来，导致内存开销比较大==。



**attention在RNN上的应用**

- 在这篇文章之前，attention已经成功地用在编码器和解码器里面了，主要是用在怎么样把编码器的东西有效地传给解码器



这篇文章提出来的transformer是一个新的模型，不再使用之前被大家使用的循环神经层，而是纯基于注意力机制，并行度比较高，这样的话它能够在比较短的时间之内做到比之前更好的结果



NeurIPS是一个篇幅比较短的会议，单列八页



### 5、相关工作



如何使用卷积神经网络替换循环神经网络，减少时序的计算



主要问题：用卷积神经网络的话，对于比较长的序列难以建模

- ==卷积做计算的时候每次是去看一个比较小的窗口，如果两个像素相隔比较远的话，需要用很多层卷积堆积起来==，最后才能够把这两个隔得比较远的像素融合起来。但是如果是使用transformer里面的注意力机制的话，每一次都能够看到所有的像素，使用一层就能看到整个序列
- 卷积的好处是可以做多个输出通道，<font color=blue>一个输出通道可以认为是识别不一样的模式，作者也想要实现这种效果，所以提出了一个叫做multi-headed attention（多头注意力机制），用于模拟卷积神经网络多输出通道的一个效果</font>



自注意力机制



memory networks



### 6、模型



序列模型里面比较好的是编码器和解码器的架构

- 编码器：将一个长为n的x1到xn的输入，编码器会把它表示成为一个也是长为n但是其中每一个zt对应xt的向量的表示，这就是编码器的输出，就是将原始的输入变成机器学习可以理解的一系列向量

- 解码器：拿到编码器的输出，然后生成一个长为m的一个序列（n和m是不一样长的，可以一样，也可以不一样）。他和编码器最大的不同之处在于，在解码器中，词是一个一个生成的（**因为对于编码器来讲，很可能是一次性能看全整个句子，但是在解码的时候只能一个一个的生成**），即自回归（auto-regressive）,在这里面输入又是输出，在过去时刻的输出也会作为当前时刻的输入

  18:13

- transformer是使用了编码器和解码器的架构，具体来讲它是将自注意力和point-wise、fully connected layers一个一个堆在一起

![img](https://i0.hdslb.com/bfs/note/0e3d20df953dae975726187ccb0f650bdd0ffa7a.png@630w_!web-note.webp)

- 写论文的话，有一张比较漂亮的能够把整个全局话清楚的图是非常重要的

![img](https://i0.hdslb.com/bfs/note/b0776a09bed44621f07c49727b13b23b5068864a.png@630w_!web-note.webp)

- 上图是一个编码器和解码器的架构

![img](https://i0.hdslb.com/bfs/note/810dcd1555f3667928075a49d562ecb201d75090.png@630w_!web-note.webp)

- 左边圈出来的部分是编码器，右边圈出来的部分是解码器
- 左下角的input是编码器的输入，如果是中文翻英文的话，输入就是中文的句子
- 右下角的outputs是解码器的输入，解码器在做预测的时候是没有输入的，<font color=red>实际上就是解码器在之前时刻的一些输出作为输入</font>
- shifted right就是一个一个往右移
- input embedding：嵌入层。进来的是一个一个的词，需要将它们表示成向量
- poositional encoding：
- Nx：N代表层数

![img](https://i0.hdslb.com/bfs/note/e51891cb1c295c0dd3e694e67e8bf482e2ee8201.png@630w_!web-note.webp)

- 红色圆圈圈出来的部分可以看作*transformer block*
- multi-headed attention
- feed forward：前馈神经网络
- add&norm：**add表示残差的连接**

![img](https://i0.hdslb.com/bfs/note/1a2a1c716c249a06927cef2dd67c068bd2e4ea26.png@630w_!web-note.webp)

- 编码器的输出会作为解码器的输入

![img](https://i0.hdslb.com/bfs/note/4a4253d3db880e386c30502213c33ba0c8e11761.png@630w_!web-note.webp)

- 解码器和编码器有点像，红色圆圈圈出来的部分是一样的，但是多了下面的masked multi-headed attention（多头注意力机制）

![img](https://i0.hdslb.com/bfs/note/7451688c71b3902744ff608362c07c0c4248b62b.png@630w_!web-note.webp)

- 解码器可以认为是红色圆圈中的三部分组成的一个块重复n次
- 解码器的输出进入一个输出层，然后做一个softmax就能得到最终的输出

![img](https://i0.hdslb.com/bfs/note/53b444cad5e6bf92cc4583be019995733d10aac8.png@630w_!web-note.webp)

- 红色方括号所扩起来的部分就是标准的神经网络的做法
- 上图所示的确是一个标准的编码器解码器的架构，只是说中间的每一块有所不同，然后是编码器和解码器之间的连接有所不同





**具体模块的实现**



**编码器**



编码器是用一个n等于6的一个完全一样的层，下图中的红色阴影部分算作一层，然后重复6次

- 每一个layer中会有2个sub-layer
- 第一个sub-layer叫做multi-headed self-attention
- 第二个sub-layer是一个simple position-wise fully connected feed-forward network，说白了就是一个MLP
- 对于每一个子层采用了一个残差连接，最后再使用layer normalization

![img](https://i0.hdslb.com/bfs/note/8135b4566e409c3da99494834984b8b26c2128d5.png@630w_!web-note.webp)

- 子层的公式如下图中黄色高亮部分所示

![img](https://i0.hdslb.com/bfs/note/aecfc2f2f4340d1f4b542272d3b39c32ed3a579e.png@630w_!web-note.webp)

- sublayer（x）：输入进来以后先进入子层
- x+sublayer（x）：因为是残差连接，所以将输入和输出加在一起，最后进入他的layernorm
- 因为残差连接需要输入和输出是一样大小的，如果大小不一样的话，需要做投影，为了简单起见，讲么一个层的输出维度变成512（固定长度表示是的模型相对来说是比较简单的，调参的话只需要调一个参数就行了，就是模型的输出维度，另外一个参数是要复制多少块n）





### batch norm对比layer norm

- 与 Batch Normalization 不同的是，Layer Normalization 是<font color=red>在单个样本的维度上进行归一化的，因此更加适合处理变长的序列数据，比如自然语言处理任务中的文本</font>。

考虑一个最简单的二维输入的情况，在二维输入的情况下输入是一个矩阵，**每一行是一个样本，每一列是一个特征**

![img](https://i0.hdslb.com/bfs/note/7e9f1aed6a40e400cb368bdab253ee1a70ea3d2e.png@630w_!web-note.webp)

batch norm所干的事情就是每一次将每一列（特征）在它的一个小mini-batch里面的均值变成零，方差变成1

- 把这个向量本身的均值减掉，然后再除以他的方差就可以了
- 在计算均值的时候，是在每个小批量里面（一条向量里面算出他的均值和方差）
- 在训练的时候可以做小批量，在预测的时候会把全局的均值算出来
- 在预测的时候会把全局的均值算出来，整个数据扫一遍之后，在所有数据上平均的均值方差存起来，在预测的时候再使用

![img](https://i0.hdslb.com/bfs/note/4295a86121344721f7c408f8f68f07fabed9e5b6.png@630w_!web-note.webp)

- batchnorm还会学 lambda和beta 出来：可以把向量通过学习放成一个方差为任意某个值，均值为任意某个值的东西



**layernorm和batpchnorm在很多时候几乎是一样的**

- 对于一个同样的二维输入来说（最简单的情况），layer norm对每个样本做normalization而不是对每个特征做normalization（之前是将每个列的均值变为0，方差变为1，现在是把每一个行变成均值为0，方差为1，这里的每一个行表示一个样本，<font color=red>所以可以认为layernorm就是把整个数据转置一下放到batchnorm里面出来的结果，再转置回去）</font>
- 在transformer或者正常的RNN里面，输入是一个三维的东西，输入的是一个序列的样本，每一个样本其实里面有很多元素（比如一个句子里面有n个词，每个词有个向量hebatch的话，就是一个3D的东西）。最大的正方形表示batch（样本），但是列不再是特征了，而是序列的长度（sequence），对每个sequence（每个词）都有自己的向量，即feature

![img](https://i0.hdslb.com/bfs/note/3dacd4d421cb79e1730f1fab0379416e0b436db3.png@630w_!web-note.webp)

- 如果还是用batchnorm的话，每次是取一根特征，然后把他的每个样本里面的所有元素，以及他的整个batch取出来，如下图立方体中蓝色正方形所示，然后把他的均值变为0方差变成1，就相当于是切一块下来然后拉成一个向量，然后再进行运算

![img](https://i0.hdslb.com/bfs/note/4f45e9a27d369ab5e70da71b8f23e5b7271c5665.png@630w_!web-note.webp)

- 如果是laynorm的话，就是对每个样本进行横切，如上图立方体中橘黄色正方形所示 



切法不一样会带来不同的效果，为什么layernorm用的多一点？

- 在时序序列模型中，每个样本的长度可能会发生变化，如下图中红色阴影所示，没有的东西一般是会放零进去

![img](https://i0.hdslb.com/bfs/note/3054b67ce539f6bfdc90a1f08935d1aaaebd6e5d.png@630w_!web-note.webp)

- 如果是用batchnorm的话，切出来的效果如下图中所示，其余地方补零

![img](https://i0.hdslb.com/bfs/note/330c7dbd93d170182fc29136ca76d42ad6753d64.png@630w_!web-note.webp)

- 如果是layernorm的话，切出来的效果如下图所示

![img](https://i0.hdslb.com/bfs/note/7cabc373f27ccd8bb867abc27ef85518c2d8179a.png@630w_!web-note.webp)

- 这里主要的问题是在算均值和方差上面，对于batchnorm来说，会对上图中切出来的阶梯形的部分进行求解（只有这部分是有效值，其他地方因为是补零，所以其实没有太多作用），如果样本长度变化比较大的时候，每次做小批量的时候，算出来的均值和方差的抖动相对来说是比较大的
- 另外，在做预测的时候要把全局的均值和方差记录下来，这个全局的均值和方差如果碰到一个新的预测样本，如果碰到一个特别长的，因为在训练的时候没有见过这种长度的，那么在之前计算的均值和方差可能就不那么好用了。
- 相反，对于layernorm相对来说没有太多的问题，因为他是按照每个样本来进行均值和方差的计算，同时也不需要存下一个全局的均值和方差（不管样本的长短，均值和方差的计算都是以样本为单位的），这样的话相对来讲更稳定一些





**解码器**



解码器跟编码器很像，跟编码器一样是由（n=6）个同样的层构成的，每个层里面跟编码器一样有两个子层



解码器和编码器的不同之处在于解码器里面用了第三个子层，他同样是多头的注意力机制，跟编码器一样同样用了残差连接，用了layernorm



解码器中做的是自回归，也就是说当前的输出的输入集是上面一些时刻的输出，**意味着在做预测的时候不能看到之后的那些时刻的输出**

- 在注意力机制中，每一次能够看到完整的输入，这里要避免这个情况的发生，也就是说在解码器训练的时候，在预测第t个时刻的输出时候不应该看到t时刻以后的那些输入，<font color=blue>他的做法是通过一个带掩码的注意力机制，如下图中的masked所示，这也是与解码器其他地方的不同之处，这个masked的作用是保证输入进来的时候，在t时间是不会看到t时间以后的那些输入，从而保证训练和预测的时候行为是一致的</font>

![img](https://i0.hdslb.com/bfs/note/dfe1b6afe0df3580ac9da156feb63028e340e173.png@630w_!web-note.webp)





**子层**



注意力层



注意力：注意力函数是一个将 query 和一些 key-value对 映射成输出的函数

- 里面所有的query、key-value、输出都是向量

- 输出是value的加权和，所以输出的维度和value的维度是一样的

- 对于每一个value的权重，他是value对应的key和query的相似度（compatibility function，不同的注意力机制有不同的算法，不同的相似度函数导致不一样的注意力的版本）计算得来的

  34:45



transformer中使用到的注意力机制：

scaled dot-product attention

- **query和key长度是等长的，都等于dk（可以不等长的，不等长有别的计算方法）**
- value的长度等于dv（输出的长度也应该是dv）
- 具体的计算方法：对每一个query和key最内积，然后将其作为相似度（如果两个向量做内积的话，如果这两个向量的 long 是一样的，那么内积的值（余弦值）越大，就表示这两个向量的相似度就越高，如果内积等于零（两个向量正交），就是说这两个向量没有相似度），**算出来之后再除以根号dk（即向量的长度**），然后再用softmax来得到权重。因为对于一个query，假设给n个key、value对的话，就会算出n个值，因为这个query会跟每个key做内积，算出来之后再放进softmax就会得到n个非负的和为1的权重（对于权重来说，非负、和为1是比较好的权重），然后将这些权重作用在value上面，就能得到输出了。
- 在实际中不能一个一个做运算，运算起来比较慢，文章提出query可以写成矩阵，可能不只是一个query，也可能有n个query，query的个数和key value的个数可能是不一样的，但是长度一定是一样的，这样才能做内积
- 给定query和key这两个矩阵，相乘就会得到一个n*m的矩阵，如下图所示，他的每一行（如图中蓝色的线所示），就是一个query对所有key的内积值，再除以根号dk后做softmax（对每一行做softmax，行与行之间是独立的），这样就能得到权重，然后再乘V（V是一个m行dv列的矩阵），得到一个n*dv的矩阵（这个矩阵的每一行就是所需要的输出）

![img](https://i0.hdslb.com/bfs/note/041d075f36729f3cf7dad1ea1b6412c724e5068c.png@630w_!web-note.webp)

- 对于key、value对和n个query，可以通过两次矩阵乘法来做整个计算，key和value在实际中对应的就是序列，这样就等价于是在并行地计算里面的每个元素（矩阵乘法便于并行）





文中所提出的注意力机制和其他注意力机制的区别

一般有两种比较常见的注意力机制

- 加型注意力机制：可以处理query和key不等长的情况
- 点积注意力机制：点积注意力机制和文中所提出的注意力机制是一样的（唯一的区别就是文中所提出来的注意力机制多除以了一个根号dk，这个根号dk就是命名中提到的scaled）

这两种注意力机制其实都差不多，文章选用的是点积注意力机制，==因为实现起来比较简单，而且效果比较好，两次矩阵乘法就能算好==

这里为什么要除以根号dk？

- 当dk不是很大的时候除不除都没关系，但是当dk比较大的时候，也就是两个向量长度比较长的时候，做点积的时候值可能比较大也可能比较小
- 当值比较大的时候，向量之间相对的差距就会变大，就导致值最大的那个值进行softmax操作后就会更接近1，剩下的值就会更靠近于0，值就会向两极分化，当出现这种情况后，在算梯度的时候，梯度会比较小（softmax最后的结果是所希望的预测值置信的地方尽量靠近1，不置信的地方尽量靠近0，这样就差不多收敛了，梯度就会变得比较小，就会跑不动）
- 在transformer里面一般用的dk比较大（512），所以除以根号dk是一个不错的选择





整个注意力机制的计算过程如下图左图所示

![img](https://i0.hdslb.com/bfs/note/5873f1135c233f4bf9ca53377f2d444a52e99ea4.png@630w_!web-note.webp)

- Q代表query矩阵
- K代表key矩阵
- mask主要是为了避免在第t时刻的时候看到以后时间的东西，具体来说，假设query和key是等长的，长度都为n，而且在时间上是能对应起来的，对第t时刻的qt在做计算的时候，应该只是看到k1一直到k（t-1），而不应该看到kt和它之后的东西，因为kt在当前时刻还没有。
- 但是在做注意力机制的时候，会发现其实qt在跟所有k里面的东西全部做运算，从k1一直算到kn，只要保证在计算权重的时候，**不要用到后面的东西就可以了**
- mask是说对于qt和kt之后的用于计算的那些值，**把他们替换成非常大的负数**，这些大的负数在进入softmax做指数的时候就会变成0，所以导致softmax之后出来对应的权重都会变成0，而kt之前所对应的值会有权重
- 这样在计算输出的时候就只用到了v对应的v1一直到v（t-1）的结果，而vt后面的东西并没有用到
- 所以mask的效果是在训练的时候，让第t个时间的query只看到对应的前面的那一些key、value对，使得在做预测的时候能够进行一一对应





multi-head

与其做一个单个的注意力函数，不如把整个query、key、value投影到一个低维，**投影h次，然后做h次的注意力函数，再将每一个函数并在一起，再投影回来得到最终的输出，如下图右图所示**

![img](https://i0.hdslb.com/bfs/note/235b323c873517333e90e3da0329e7aab3ff5da6.png@630w_!web-note.webp)

- 原始的value、key、query进入一个线性层（线性层将其投影到比较低的维度），然后再做一个scaled dot-product attention（如上图左图所示），做h次，得到h个输出，再把这些输出向量全部合并到一起，最后做一次线性的投影，然后回到multi-head attetion



为什么要做多头注意力机制？

- dot-product的注意力中没有什么可以学的参数，具体函数就是内积。有时候为了识别不一样的模式，希望有一些不一样的计算像素的办法
- 如果是用加型attention的话，里面其实是有一个权重可以学习到的，但是本文使用的是内积，它的做法是先投影到低维，这个投影的w是可以学的，也就是说，有h次机会希望可以学到不一样的投影方法，使得再投影进去的度量空间中能够匹配不同模式所需要的相似函数，然后最后把所得到的东西再做一次投影（这里有点像在卷积神经网络里面有多个输出通道的感觉）
- 具体的计算（公式如下图）：在multi-head的情况下，还是以前的Q、K、V，但是输出已经是不同头的输出做contact运算再投影到一个WO里面，对每一个头，就是把Q、K、V通过不同的可以学习的WQ、WK、WV投影到dv上面，再做注意力函数，然后再出来就可以了

![img](https://i0.hdslb.com/bfs/note/524ad9abd746aa1f05c549e42bb0ba567128b273.png@630w_!web-note.webp)

- 实际上h是等于8的，就是用8个头
- 注意力的时候，因为有残差连接的存在，使得输入和输出的维度是一样的，所以他投影的时候，投影的就是输出的维度除以h（因为输出维度是512，除以8之后，就是每一次把它投影到64维的维度，然后在这个维度上面计算注意力函数，最后再投影回来）
- 虽然公式中看起来有很多小矩阵的乘法，实际上在实现的时候也可以通过一次矩阵的乘法来实现（可以作为一个练习题来练习如何实现）





在transformer模型中是如何实现注意力的？

三种实现情况

![img](https://i0.hdslb.com/bfs/note/e5eabdd645598be803f6fe2e83b53866807351b0.png@630w_!web-note.webp)

- 上图中三个阴影表示三个注意力层，这三个注意力层各不相同



第一个注意力曾的使用：

![img](https://i0.hdslb.com/bfs/note/adda262c75f3b2b6214c7cd8027aaca973aa9139.png@630w_!web-note.webp)

- 上图中红色圈出来的部分是编码器的注意力层。编码器的输入（假设句子长度是n的话，他的输入其实是n个长为d的向量，每一个输入的词对应的是一个长为d的向量，一共有n个）

- 这个注意力层有三个输入，分别表示的是key、value、query。这里一根线复制成三根线表示同样一个东西，既作为key，也作为value和query，这个东西叫做自注意力机制，就是说key、value和query其实是一个东西，就是他自己本身

- 这里输入了n个query，每个query会得到一个输出，最终会得到n个输出，而且这个输出和value因为长度是一样的，所以输出的维度其实也是d，即输入和输出的大小其实是一样的，输出长也为n。

- 输出其实就是value的加权和，权重来自query和key

  49:48

- 假设不考虑多头和有投影的情况，输出其实是输入的加权和，权重来自于自己本身跟各个向量之间的相似度。如果有多头的话，因为有投影，会学习出h个不一样的距离空间出来，使得得出来的东西会有点不一样



第二个注意力层的使用：

![img](https://i0.hdslb.com/bfs/note/eb85c3be8b1398c59e37455e34eecf875cbdbbb6.png@630w_!web-note.webp)

- 如上图中红色圆圈圈出的部分所示
- 解码器也是一样的，也是同一个东西复制了三次
- 解码器的输入也是一样的，只是长度可能变成了m，维度其实也是一样的，所以它跟编码器一样的也是自注意力，唯一不一样的是里面有一个mask（mask的作用：在解码器计算query对应的输出的时候，不应该看到第t时刻后面的东西，意味着后面的东西要设为0）





第三个注意力层：

![img](https://i0.hdslb.com/bfs/note/372e3f43acc43abc457f882067609ef56ecc7255.png@630w_!web-note.webp)

- 如上图中红线箭头所指的部分

- 它不再是自注意力了，key和value来自编码器的输出，query来自解码器下一个attention的输入

- 编码器最后一层的输出就是n个长为d的向量

- 解码器的masked attention的输出是m个长为d的向量

- 编码器的输出作为key和value传入到这个注意力层中，解码器的下一层输出作为query传入到这个注意力层中，意味着对于解码器的每一个输出，作为query要计算出一个所要的输出，这个输出是来自于value的一个加权和（来自于编码器的输出的加权和，权重的粗细程度取决于query与编码器输出的相似度，如果相似度比较高的话，权重就会大一点，相反，如果相似度比较低的话，权重就会小一点）

- 这个attention中所要做的其实就是有效地把编码器里面的输出根据需要截取出来。例如

  53:20

- attention如何在编码器和解码器之间传递信息的时候起作用：根据在解码器输入的不同，根据当前的向量在编码器的输出里面挑选感兴趣的东西，也就是去注意感兴趣的东西，那些不那么感兴趣的东西就可以忽略掉





feed forward：



其实就是一个fully connected feed-forward network，就是一个MLP，但是不同之处在于他是applied to each position seperately and identically（就是把同一个MLP对每个词作用一次，即position-wise，说白了就是MLP只是作用在最后一个维度）

- position：输入序列中有很多个词，每一个词就是一个点，这些点就是position
- 具体公式如下图所示，xW1+b1就是一个线性层，max就是relu激活层，最后再有一个线性层

![img](https://i0.hdslb.com/bfs/note/aa19f938abdb89ea54e7f388d97a379c2c80b1f1.png@630w_!web-note.webp)

- 在注意力层的输入（每一个query对应的输出）的长为512，x就是一个512的向量，W1会把512投影成2048（等价于将他的维度扩大了4倍），以为最有有残差连接，所以还需要投影回去，所以W2又把2048投影回512
- 这其实就是一个单隐藏层的MLP，中间的隐藏层将输入扩大4倍，最后输出的时候又回到输入的大小（如果用pytorch来实现的话其实就是把两个线性层放在一起，而不需要改任何参数，因为pytorch在输入是3d的时候，默认就是在最后一个维度做计算）





整个transformer是如何抽取序列信息，然后把这些序列信息加工成最后所想要的语义空间向量？



56:53



首先考虑一个最简单的情况（没有残差连接、attention也是单头、没有投影），如下图所示

![img](https://i0.hdslb.com/bfs/note/288e6174d7c9e420fd77c7cda728e3c15ae09297.png@630w_!web-note.webp)

输入就是长为n的向量，在进入attention之后，就会得到同样长度的输出，最简单的attention其实就是对输入进行加权求和，加权和之后进入MLP，每个MLP对每一个输入的点做运算会得到输出，最后就得到整个transformer块的输出（输入和输出的大小都是一样的）

- 在整个过程中attetion所起到的作用就是把整个序列里面的信息抓取出来做一次汇聚（aggregation），因为已经抓取序列中感兴趣的信息，所以在做投影、MLP、映射成为更想要的语义空间的时候，因为加权之后的结果已经包含了序列种的信息，所以每个MLP只要再每个点独立进行运算就行了，因为序列信息已经汇聚完成，所以在做MLP的时候是可以分开做的



作为对比，RNN（没有隐藏层的MLP，纯线性）的实现过程

RNN的输入也是向量，对于第一个点也是做一个线性层

对于下一个点，如何利用序列信息，还是用之前的MLP（权重跟之前是一样的），但是时序信息（下图中绿色曲线表示之前的信息，蓝色曲线表示当前的信息）方面，是将上一个时刻的输出放回来作为输入的一部分与第二个点一起作为输入，这样就完成了信息的传递



RNN和transformer都是用一个线性层或者MLP来进行语义空间的转换，但是不同之处在于传递序列信息的方式：RNN十八上一时刻信息的输出传入下一时候做输入，但是在transformer中是通过attention层全局地拉取整个序列里面的信息，然后再用MLP做语义转换

- 他们的关注点都是如何有效地使用序列信息





embeding

因为输入是一个一个的词（或者叫词源，token），需要将其映射成向量。embeding的作用就是给任何一个词，学习一个长为d的向量来表示它（本文中d等于512）

- 编码器的输入需要embeding
- 解码器的输入也需要embeding
- 在softmax前面的线性也需要embeding

本文中这3个embeding是一样的权重，这样子训练起来会简单一点

另外还将权重乘了根号d：维度一大的话，权重值就会变小，之后要加上positional encoding，他不会随着长度变长把他的long固定住，因此乘上了根号d之后，使得他们在scale差不多

01:01:15







positional encoding

attention不会有时序信息，输出是value的加权和，权重是query和key之间的距离，和序列信息无关（也就是说给定一句话，把顺序任意打乱之后，attetion出来的结果都是一样的，顺序会变，但是值不会变，这样是存在问题的，所以需要把时序信息加入进来）

RNN是如何添加时序信息的？RNN将上一个时刻的输出作为下一个时刻的输入来传递历史信息

attention是在输入里面加入时序信息，将输入词所在的位置信息加入到输入里面（positional encoding），具体公式如下图所示

![img](https://i0.hdslb.com/bfs/note/c7647aebdd77684ce356c7e89d8ea81e28e0b913.png@630w_!web-note.webp)



01:03:23



- 在计算机中，数字是用一定长度的向量来表示的
- 词在嵌入层会表示成一个长为d的向量，同样用一个长为d的向量来表示数字，也就是词的位置，这些数字的不同计算方法是用周期不一样的sin和cos函数的值来算出来的，所以说任何一个值可以用长为d的向量来表示，然后这个长为d的记录了时序信息的向量和嵌入层相加，就完成了将时序信息加进数据中的操作，如下图中的红色线团所示，因为是cos和sin的函数所以是在+1和-1之间抖动的，所以乘了一个根号d，使得每个数字也是差不多在正负1的数值区间里面

![img](https://i0.hdslb.com/bfs/note/d23b1523d97fb0363e9e71f8a27f5d4aa3d41728.png@630w_!web-note.webp)









**4、为什么要用自注意力？**

相对于使用循环层或者卷积层，使用自注意力有多好

下表比较了四种不一样的层

![img](https://i0.hdslb.com/bfs/note/9c7cf0185f83d58bc0105949ec377ad7c8d8af49.png@630w_!web-note.webp)

- 第一个是自注意力层
- 第二个是循环层
- 第三个是卷积层
- 第四个是构造出来的受限的自注意力
- 第一列是计算复杂度，越低越好
- 第二列是顺序的计算，越少越好。指的是在算layer的时候，下一步计算必须要等前面多少步计算完成。相当于是说非并行度
- 第三列说的是信息从一个数据点走到另一个数据点要走多远，越短越好
- n是序列的长度
- d是向量的长度
- 整个自注意力就是几个矩阵做运算，其中一个矩阵运算时query矩阵（n行，n个query，列数是d，也就是维度是d）乘以key矩阵（也是n*d），两个矩阵相乘，算法复杂度就是n平方乘以d，别的矩阵运算复杂度也是一样的。因为只是牵涉到矩阵的运算，矩阵中可以认为并行度是比较高的，所以是o（1），最大长度是说从一个点的信息想跳到另一个点要走多少步，在attention里面，一个query可以跟所有的key做运算，而且输出是所有value的加权和，所以query跟任何一个很远的key、value对只要一次就能过来，所以长度是比较短的
- 对循环层来说，如果序列是乘了n的话就一个一个做运算，每个里面主要的计算就是n*n的矩阵的dense layer然后再乘以长为d的输入，所以是n平方，然后要做n次，所以是n*d平方
- 对比自注意力和RNN的计算复杂度，其实是有一定的区别的，取决于n大还是d大。在本文中d是512，n也差不多是几百，现在比较大的模型d可以做到2048甚至更大，n相对来说也会做的比较多，所以现在看起来其实这两种的计算复杂度是差不多的（n和d在差不多的数据上面），但是在循环的时候因为要一步一步做运算，当前时刻的值需要等待前面完成，所以导致了是一个长为n的序列化的操作，在并行上比较吃亏。在最初点的那个历史信息到最后一个点的时候需要走过n步才能过去，所以循环的最长距离是o（n），即RNN对特别长的序列的时候做的不够好，因为信息从一开始走，走着走着就走丢了，而不是像attention一样直接一步就能到
- 卷积在序列上的具体做法是用一个1d的卷积，所以它的kernel就是k，n是长度，d就是输入的通道数和输出的通道数，所以这里是k乘n乘d的平方。k一般不大（一般是3或者5，所以一般可以认为是常数），所以导致卷积的复杂度和RNN的复杂度差不多，但是卷积的好处在于只用卷积就完成了，并行度很高，所以卷积做起来通常比RNN要快一点，另外卷积每一次一个点是由一个长为k的窗口来看，所以信息在k距离内是能够一次完成传递的，如果超过k的话，传递信息就需要通过多层一层一层上去，但是由于是log的操作，所以也不会有太大的麻烦
- 最后一个层是在做注意力的时候，query只跟最近的r个邻居做运算，这样就不用算n平方了，但是问题在于这样的话，两个比较长的远一点的点需要走几步才能过来
- 一般来说使用attetion主要是关心对于特别长的序列是否能将整个信息揉的比较好一点所以在实际过程中，带限制的自注意力使用的不是特别多，一般都是用最原始的版本，而不做受限，所以基本就是考虑前三种层
- 实际中，当序列的长度和整个模型的宽度差不多而且深度都一样的话的时候，基本上前三个模型的算法复杂度都是差不多的，attention和卷积相对来说计算会好一点，attention在信息的糅合性上会好一点，所以用了self-attention看上去对长数据处理更好一些，但是实际上attention对整个模型做了更少的假设，导致需要更多的数据和更大的模型才能训练出来跟RNN和CNN同样的效果，所以导致现在基于transformer的模型都特别大

### 7、实验



训练的一些设置



训练数据集和batching

使用了两个任务：

- 英语翻德语标准的WMT 2014的数据，里面有4.5w个句子，使用了byte-pair encoding（bpe，不管是英语还是德语，一个词里面有很多种变化，如果直接把每个词做成一个token的话，会导致字典里面的东西会比较多，而且一个动词可能有好几种不同的变化形式，在做成不一样的词的时候，它们之间的区别模型是不知道的，bpe相对来说就是把词根给提出来，这样的好处是整个字典比较小），这里使用的是37000个token的字典，他是在英语和德语之间共享的，也就是说不再为英语构造一个字典也不再为德语构造一个字典，这样的好处是整个编码器和解码器的embeding就可以使用同一个东西，而且整个模型变得更加简单，也就是之前说的编码器和解码器的embeding是权重共享的
- 英语翻法语：使用了一个更大的数据集



硬件和schedule

- 训练使用了8个P100的GPU，后来使用tpu（tpu更适合做大的矩阵乘法）
- base模型使用小一点的参数，每一个batch训练的时间是0.4秒，一共训练了10w步，一共在8台gpu上训练了12个小时（基本上一个模型训练12个小时也是一个不错的性能）
- 这个大的模型，一个batch训练需要一秒钟，一共训练了30W步，最后一台机器训练了3.5天，其实也是一个可以承受的范围





在训练器上使用的是Adam

学习率是根据以下公式计算出来的：学习律是根据模型宽度的-0.5次方（就是说当模型越宽的时候，学习的向量越长的时候，学习率会低一点）

![img](https://i0.hdslb.com/bfs/note/de02350ab647617c8db3ff593db62d2e2ff8af3e.png@630w_!web-note.webp)

- 存在一个warmup，就是从一个小的值慢慢地爬到一个高的值，爬到之后，再根据步数按照0.5次方衰减，最后warmup是4000
- 学习率几乎是不用调的：第一，adam对学习率不敏感；学习率已经把模型考虑进来了，schedule也已经算是不错的schedule了，所以学习率是不需要调的





正则化

总共使用了三个正则化

residual dropout：对每一个子层（包括多头的注意力层和之后的MLP），在每个层的输出上，在进入残差连接之前和进入到layernorm之前，使用dropout（dropout率为0.1，也就是把输出的10%的那些元素值乘0.1，剩下的值乘1.1）。另外在输入加上词嵌入再加上position encoding的时候，也用了一个dropout。也就是说，基本上对于每一个带权重的层，在输出上都使用了dropout，虽然dropout率不是特别高，但是使用了大量的dropout层来对模型做正则化





label smoothing（inception v3）

使用softmax去学东西的时候，正确的标号是1，错误的标号是0，对于正确的label的softmax值，让他去逼近于1，但是softmax的值是很难去逼近于1的，因为他里面是指数，比较soft（需要输出接近无限大的时候，才能逼近于1），这样使得训练比较困难。

一般的做法是不要搞成特别难的0和1，可以把1的值往下降一点，比如降成0.9，本文中是直接降成了0.1，就是说对于正确的那个词，只需要softmax的输出（置信度）到0.1就可以了，而不需要特别高，剩下的值就可以是0.9除以字典大小，这里会损失perplexity（log lost做指数），基本上可以认为是模型的不确信度（正确答案只要10%是对的就行了，不确信度会增加，所以这个值会变高），但是在模型不那么确信的情况下会提升精度和BLUE的分数（这两个才是真正所要关心的点）





下表表示的是不同的超参数之间的对比

![img](https://i0.hdslb.com/bfs/note/ffd017934f4674fdc9a2fb7ad5610768e2e45d32.png@630w_!web-note.webp)

- n表示要堆多少层

- d表示模型的宽度，即token进来要表示成一个多长的向量

- dff表示MLP中间隐藏层的输出的大小

- h表示注意力层的头的个数

- dk、dv分别是一个头里面key和value的维度

- Pdrop表示dropout的丢弃率

- els表示最后label smoothing的时候要学的label的真实值是多少

- train steps表示要训练多少个batch

  01:19:54

- 整个模型参数相对来说还是比较简单的，基本上能调的就是上面的这些超参数剩下的东西基本上都是可以按比例算过来的，这也是transformer的一个好处，虽然看上出模型比较复杂，但是实际上没有太多可以调的东西，这个设计对后面的人来说相对更加方便一点



### 8、评价



写作

- 非常简洁，每一句上基本上在讲一个事情
- 没有使用太多的写作技巧
- 这种写法并不是很推荐，因为对一篇文章来说，需要讲一个故事来让读者有代入感，能够说服读者
- 一般说可以选择把东西减少一点，甚至把一些不那么重要的东西放在附录里面，但是在正文的时候，最好还是讲个故事：为什么做这些事情以及设计的理念、对整个文章的一些思考，这让会使得文章更加有深度



transformer模型

- 现在不仅仅是用在机器翻译上，也能够用在几乎所有的NLP任务上面，在bert，gpt，后续能够训练很大的预训练模型，能够极大提升所有NLP任务的性能，类似于CNN对整个计算机视觉的改变：能够训练一个大的CNN模型，使得别的任务也能够从中受益，CNN使得整个计算机视觉的研究者提供了一个同样的框架，使得只要学会CNN就行了，而不需要去管以前跟任务相关的海量专业知识（比如特征提取、任务建模等）
- 对于transformer来说，之前需要做各种各样的数据文本的预处理，然后根据NLP的任务设计不也一样的架构，现在不需要了，使用了整个transforner架构就能够在各个任务上取得非常好的成绩，而且它的预训练模型也让大家的训练变得更加简单
- 现在transformer不仅仅是用在自然语言上面，也在图片、语音、video上面取得了很大的进展
- 之前计算机视觉的研究者使用CNN，而在语言处理使用RNN，在别的方面用别的模型，现在发现同样一个模型能够在所有领域都能用，让大家的语言的一样了，任何一个领域的研究做的一些突破能够很快地在别的领域被使用，能够极大地减少一个新的技术在机器学习里面各个领域被应用的时间
- 人对世界的感知是多模态的：图片、文字、语音，现在transformer能够把这些所有不同的数据给融合起来，因为大家都用一个同样的架构抽取特征的话，就可以抽取到一个同样的语义空间，使得我们可以用文本、图片、语音、视频等训练更好更大的模型
- 虽然transformer这些模型取得了非常好的实验性的结果，但是对它的理解还是处于比较初级的阶段。
- 虽然标题说只需要attention就够了，但是最新的研究表明，attention只是在transformer里面起到把整个序列的信息聚合起来的作用，但是后面的MLP以及残差连接是缺一不可的，如果把这些东西去掉的话，attention基本上什么东西都训练不出来所以模型也不只是说只需要attention就够了
- attention根本就不会对数据的顺序做建模，为什么能够打赢RNN呢？RNN能够显示建模的序列信息理论上应该比MLP效果更好现在大家觉得它使用了一个更广泛的归纳偏置，使得他能够处理一些更一般化的信息，这也是为什么说attetion并没有做任何空间上的一些假设，它也能够跟CNN甚至是比CNN取得更好的结果，但是他的代价是因为他的假设更加一般，所以他对数据里面抓取信息的能力变差了，以至于说需要使用更多的数据、更大的模型，才能训练出想要的效果，这也是为什么现在transformer模型越来越大
- attention也给了研究者一些鼓励，原来CNN和RNN之外也会有新的模型能打败他们。现在也有一些工作说，就用MLP或者就用一些更简单的架构也能够在图片或者文本上面取得很好的结果
- 未来肯定会有很多新的架构出现，让整个领域更加有意思一些



## 四、GAN  阅读



**GAN**



generative Adversarial  Nets



###  1、标题 + 作者



机器学习中有两大类

- 分辨模型：判断数据的类别或者预测实数值
- 生成模型（generative）：生成数据



adversarial：对抗



Nets：networks



一作 Ian J.Goodfellow ,也是《深度学习》（花书）的作者



### 2、摘要



写法简洁，重点在于讲清楚GAN



文章提出了一个新的framework（framework通常是一个比较大的模型）用来估计生成模型，通过对抗的过程，同时会训练两个模型

- <font color=red>生成模型G</font>：用来抓取整个数据的分布（生成模型就是要对整个数据的分布进行建模，使得能够生成各种分布，这里的分布就是指的生成图片、文字或者电影等，在统计学中，整个世界是通过采样不同的分布来得到的，所以如果想要生成东西，就应该抓取整个数据的分布）
- 辨别模型D：用来估计样本到底是从真正的数据生成出来的还是来自生成模型生成出来的，(警察)

生成模型尽量想让辨别模型犯错（生成模型一般是尽量使数据分布接近，但是这个地方有所不同，它是想让辨别模型犯错）



这个framework对应的是minmax two-player game（博弈论中一个很有名的两人对抗游戏）



在任何函数空间的 G 和 D 中存在一个独一无二的解（ G 能够将数据的真实分布找出来，如果已经把真实数据发掘出来的，辨别模型 D 就做不了什么事情了），如果 G 和 D 是MLP的话，那么整个系统就可以通过误差的反向传播来进行训练



不需要使用任何马尔科夫链或着说对一个近似的推理过程展开，相比其他方法更加简单，而且实验效果非常好



### 3、导言



深度学习是用来发现一些丰富的有层次的模型，这些模型能够对AI中各种应用的各种数据做概率分布的表示

- 深度学习不仅仅是深度神经网络，更多的是对整个数据分布的特征表示，深度神经网络只是其中的一个手段



虽然深度学习在辨别模型上取得了很大进展，但是在生成模型上做的还是比较差（难点在于在最大化似然函数的时候要对概率分布进行很多近似，这个近似的计算比较困难）



<font color=red>深度学习在生成模型上进展不大，是因为要去近似概率分布分布来计算似然函数，这篇文章的关键是不用近似似然函数而可以用别的方法来得到一个计算上更好的模型</font>



==这个框架中每一个模型都是MLP==，所以取名叫GAN。这个框架中有两类模型

- 生成模型：类似于造假的人，他的目的是去生产假币
- 辨别模型：类似于警察，他的任务是将假币找出来，和真币区分开开来

造假者和警察会不断地学习，造假者会提升自己的造假技能，警察会提升自己判别真币和假币的性能，最终希望造假者能够赢得这场博弈，也就是说警察无法区分真币和假币，这时就能生成跟真实一样的数据了



框架下面的生成模型是一个MLP，它的输入是一个随机的噪音，MLP能够把产生随机噪音的分布（通常是一个高斯分布）映射到任何想要拟合的分布中。同理，如果判别模型也是MLP的情况下，在这个框架下的特例叫做adversarial nets，因为两个模型都是基于MLP，所以在训练的时候可以直接通过误差的反向传递而不需要像使用马尔科夫链类似的算法来对一个分布进行复杂的采样，从而具有计算上的优势



### 4、相关工作



视频中所读的版本是GAN在neurips上的最终版本，它是一个比较新的版本



在搜索的时候可能会搜索到arxiv版本，它是一个比较早期的版本，作者没有将最新的版本上传到arxiv上面



以上两个版本的主要区别就是相关工作是不不一样的

- arxiv版本上面相关工作的部分其实什么都没写，写的并不相关
- neurips的版本上写了很多真正相关的工作



首先阐述了其他方法所存在的问题：之前的方法是想要构造出一个分布函数出来，然后提供一些参数让他可以学习，通过最大化这些参数的对数似然函数来做这样的坏处是采样一个分布的时候计算比较困难（尤其是维度比较高的时候）



因为这些方法计算比较困难，所以开展了generative machines的相关工作，*不再去构造这样一个分布出来，而是去学习一个模型去近似这个分布，这两种方法是有区别的*

- 前一种方法明确知道分布是什么，包里面的均值、方差等
- 后一种方法不用去构造分布，只需要一个模型去近似想要的结果就可以了，缺点是不知道最后具体的分布是什么样的，好处是计算起来比较容易



![img](https://i0.hdslb.com/bfs/note/343eb63f72af165b00a6841fa24af0e3b91ddd4d.png@640w_!web-note.webp)

- 上式中对 f 的期望求导等价于对 f 自身求导，==这也是为什么可以通过误差的反向传递对GAN进行求解==



VAEs跟GAN非常类似



通过一个辨别模型来帮助生成模型，比如说NCE也用了这样的思路，但是NCE相对来说损失函数更加复杂一点，在求解上没有GAN的性能那么好



和predictability minimization算法的区别

- 其实GAN就是predictability minimization反过来



一个真实有用的技术会在不同的领域不断被人重新发现给予新的名词，大家通常会将功劳归功于那个教会了大家这个技术的人，而不是最早发明他的人



adversarial examples和GAN的区别

- adversarial examples是说通过构造一些和真实样本很像的假样本，能够糊弄到分类器，从而测试整个算法的稳定性

### 5、模型



==这个框架最简单的应用是当生成器和辨别器都是MLP的时候==，生成器需要去学一个在数据x上的Pg分布，x中每个值的分布都是由pg这个分布来控制的



生成模型如何输出x

- 首先在一个输入分布为Pz的噪音变量 z 上定义一个先验，z可以认为是一个100维的向量，每一元素是均值为0，方差为1的高斯噪音
- ==生成模型就是把z映射成x，生成模型是MLP，他有一个可以学习的参数 θg== 



假设想要生成游戏的图片

- 第一种办法是反汇编游戏代码，然后利用代码就知道游戏是如何生成出来的，==这就类似于构造分布函数的方法，在计算上比较困难==
- 第二种办法是不管游戏程序是什么，假设用一个若干维的向量就足以表达游戏背后隐藏的逻辑，再学一个映设（MLP，MLP理论上可以拟合任何一个函数，所以可以通过构造一个差不多大小的向量，然后==利用MLP强行将z映射成x==，使得他们相似就可以了），这种方法的好处是计算比较简单，坏处是MLP不在乎背后真正的分布是什么，而是只是每次生成一个东西，看起来相似就行了



辨别器D也是一个MLP，它也有自己可以学习的参数 θd ，它的作用是将数据放进来之后输出一个标量，这个标量用来判断x到底是来自真实采样的数据还是生成出来的图片（以游戏为例，就是这个图片到底是来自游戏中的截图，还是生成器自己生成的图片），因为知道数据的来源，所以会给数据一个标号（如果来自真实的数据就是1，来自生成的数据就是0）



所以就采样一些数据来训练一个两类的分类器



<font color=red>在训练D的同时也会去训练G，G用来最小化log(1-D(G(z)))</font>

- z 代表随机噪音，放到G中就会生成图片，假设辨别器正确的话，辨别器的输出应该为0，表示是生成的数据，这个式子最终为log1等于0
- 但是如果辨别器没有做好，会输出一个大于0的数，在极端情况下输出1，即辨别器百分之百地确信生成模型所生成的辨别器来自真实的数据，即判断错误。则无论无何log（1-一个大于零小于1 的数）的最终结果就会变成一个负数，在极端情况下，log0是负无穷大
- 所以如果要训练G来最小化log(1-D(G(z)))就意味着，训练一个G使得辨别器尽量犯错，无法区分出来数据到底是来自真实数据还是生成模型所生成的数据



总结

![img](https://i0.hdslb.com/bfs/note/c90034d3d3af722239b39d2012e719092e47bb44.png@640w_!web-note.webp)



20:35

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。



<font color=red>符号 $\mathbb{E}_{z \sim p_z(z)}$ 表示的是对随机变量 \(z\) 的期望值，其中 \(z\) 是从先验分布 \(p_z(z)\) 中采样得到的。这里的 \(\mathbb{E}\) 是期望（Expectation）的缩写，而 $z \sim p_z(z)$ 表示 \(z\) 是遵循概率分布 \(p_z(z)\) 的一个随机变量</font>。



1. **随机变量 \(z\)**：
   \(z\) 通常在机器学习和统计学中用来表示一个随机变量，它可能代表数据点、特征向量、或者在生成对抗网络（GANs）中的输入噪声。

2. **概率分布 \(p_z(z)\)**：
   \(p_z(z)\) 是定义在 \(z\) 上的概率分布，描述了 \(z\) 可能取值的概率。在GAN中，这通常指的是输入到生成器的随机噪声的分布，常见的选择是高斯分布。

3. **期望 \(\mathbb{E}_{z \sim p_z(z)}\)**：
   期望值是一种度量随机变量平均取值的方法。对于离散随机变量，期望值是每个可能值乘以其发生的概率的总和。对于连续随机变量，期望值是每个可能值乘以其概率密度的积分。



数学表达：

对于离散随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \sum_{z} z \cdot p_z(z) $
其中，\( p_z(z) \) 是 \( z \) 取特定值的概率。

对于连续随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \int_{-\infty}^{\infty} z \cdot p_z(z) \, dz $
其中，\( p_z(z) \) 是 \( z \) 的概率密度函数。

在GAN中的应用：

在GAN的上下文中，$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$这一项是训练生成器 \( G \) 的一部分，其中 \( G(z) \) 是生成器生成的数据，\( D \) 是判别器。生成器 \( G \) 试图生成尽可能接近真实数据的样本，以使判别器 \( D \) 难以区分真假。这里的期望值是对所有可能的噪声 \( z \) 进行平均，以评估生成器的整体性能。

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

==总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。== 



- 目标函数如上图中公式所示，是一个两人的minimax游戏
- ==V（G，D）是一个价值函数==
- 公式右边第一项是期望，x是采样真实分布
- 公式右边第二项是期望，x是采样噪音分布
- 在D是完美的情况下，公式右边的两项应该都是等于0的
- 如果D不完美、有误分类的情况下，这两项因为log的关系，都会变成一个负数值
- 所以如果想要辨别器完美地分类这两类的话，就应该最大化D的值，最小化G，目标函数中有两个东西，一个是min，一个是max，和一般的训练步骤有所区别，一般只有一个min，或者只有一个max，这里既有min又有max，就是两个模型在相互对抗：D是尽量把数据分开，G是尽量使生成数据分不开，这个在博弈论中叫两人的minimax游戏
- 如果达到了一个均衡，就是D不能往前进步，G也不能往前进步了，就认为达到了均衡，这个均衡叫做纳什均衡





![img](https://i0.hdslb.com/bfs/note/49f4369d54a493101193b6b59f8a9453dde54ecd.png@640w_!web-note.webp)

- 上图中一共有四张图，分别表示GAN在前面三步和最后一步所做的工作
- z是一个一维的标量
- x也是一个一维的标量
- 噪音是均匀分布采样来的
- **所要真实拟合的x如图中黑色圆点所示，是一个高斯分布**
- （a）表示第一步的时候，生成器将均匀分布进行映射，图中绿色的线就是把z映射成了一个高斯分布，此时辨别器视图中蓝色的线，表现一般
- （b）表示更新辨别器，尽量把这两个东西分开，**两个高斯分布的最高点表示真实分布和噪声最有可能出现的地方，辨别器需要在真实分布的地方值为1**，在噪音分布的地方值为0，这样就可以尽量将来自真实分布的x和来自于生成器的x尽量分别开来
- （c）表示尽量更新生成器，使得能够尽量糊弄到辨别器（就是将生成器生成的高斯分布的峰值尽量左移，向真实数据的高斯分布进行靠拢），让辨别器犯错，这时候辨别器就需要尽量调整来把这两个细微的区别区别开来。
- （d）表示通过不断地调整生成器和辨别器，直到最后生成器的模型能够将来自均匀分布的随即噪音z映射成几乎跟真实分布差不多融合的高斯分布，即从真实的黑点中采样还是从生成器的绿线采样，辨别模型都是分辨不出来的（不管来自于哪个分布，辨别器对这每个值的输出都是0.5，这就是GAN最后想要的结果：生成器生成的数据和真实数据在分布上是完全分别不出来的，辨别器最后对此无能为力）





![img](https://i0.hdslb.com/bfs/note/848589b90e335f754444ab0dbcdfeb9ef1b4116e.png@640w_!web-note.webp)

- 上图表示的是算法一
- 第一行是一个for循环，每一次循环里面是做一次迭代，迭代的另一部分也是一个k步的for循环，每一步中先采样m个噪音样本，再采样m个来自真实数据的样本，组成一个两个m大小的小批量，将其放入价值函数中求梯度（就是将采样的真实样本放入辨别器，将采样的噪音放进生成器得到的生成样本放进辨别器**，放进去之后对辨别器的参数求梯度来更新辨别器**），这样子做k步，做完之后再采样m个噪音样本放进第二项中，把它对于生成器的模型的梯度算出来，然后对生成器进行更新，这样就完成了一次迭代
- 每次迭代中，==先更新辨别器，再更新生成器==
- k是一个超参数，k不能取太小，也不能取太大，需要辨别器有足够的更新但也不要更新的太好。如果没有足够好的更新，对新的数据，生成器生成的东西已经改变了，如果辨别器没有做相应的变化，那么再更新生成器来糊弄D其实意义不大；反过来讲如果将D训练到足够完美，log(1-D(G(z)))就会变成0，对0进行求导，生成模型的更新就会有困难（**如果辨别器是警察，生成器是造假者，假设造假者一生产假币，警察就将其一锅端了，造假者也就不会赚到钱，就没有能力去改进之后的工艺了；反过来讲，如果警察没有能力，造假者随便造点东西，警察也看不出来，也抓不到造假者，那么造假者也不会有动力去改进工艺，使得假钞和真钞真的长得差不多，所以最好是两方实力相当，最后大家能够一起进步）**
- **k就是一个超参数，使得D的更新和G的更新在进度上差不多**
- 外层循环迭代N次直到完成，如何判断是否收敛，这里有两项，一个是往上走（max），一个是往下走（min），有两个模型，所以如何判断收敛并不容易。整体来说，GAN的收敛是非常不稳定的，所之后有很多工作对其进行改进









在上面的公式中，等式右边的第二项存在一定的问题：在早期的时候G比较弱，生成的数据跟真实的数据差得比较远，这就很容易将D训练的特别好（D能够完美地区分开生成的数据和真实的数据），就导致log(1-D(G(z)))会变成0，它变成0的话，对他求梯度再更新G的时候，就会发现求不动了。所以在这种情况下建议在更新G的时候将目标函数改成最大化log(D(G(z)))就跟第一项差不多了，这样的话就算D能够把两个东西区分开来，但是因为是最大化的话，问题还是不大的，但是这也会带来另外一个问题，如果D(G(z))等于零的话，log(D(G(z)))是负无穷大，也会带来数值上的问题，在之后的工作中会对其进行改进



### 6、理论



当且仅当生成器学到的分布和真实数据的分布式相等的情况下，目标函数有全局的最优解



算法一确实能够求解目标函数



第一个结论：当G是固定，即生成器是固定的情况下，最优的辨别器的计算如下图公式中所示

![img](https://i0.hdslb.com/bfs/note/596ad2ec9c620805028558d7978a49070965ef1c.png@640w_!web-note.webp)

- \* 表示最优解
- Pdata表示将x放进去之后，在真实产生数据的分布中的概率是多少
- ==Pg表示将x放进去之后，生成器所拟合的分布的概率是多少==
- 分布是在0和1之间的数值，所以上式中的每一项都是大于等于0、小于等于1的，因此上式中分子、分母中所有的项都是非负的，所以整个式子右式的值是在0到1之间的
- 当Pdata和Pg是完全相等的情况下（即对每一个x，两个p给出来的结果是一样的），右式的值是1/2，即不管对什么样的x，最优的辨别器的输出概率都是1/2，<font color=red>表示这两个分布是完全分不开的</font>
- 这里可以看到D是如何训练出来的，从两个分布中分别采样出数据，用之前的目标函数训练一个二分类的分类器，这个分类器如果说给的值都是1/2，即什么值都分辨不出来，就表示这两个分布是重合的，否则的话就能够分辨出来，这个东西在统计学中非常有用，这叫做two sample test：判断两个数据是不是来自同一个分布在统计上其实有很多工具，比如说用T分布检测（在数据科学中经常使用，可以完全不管分布是什么样子的，可以无视在高维上很多统计工序不好用，就训练一个二分类的分类器，如果这个分类器能够分开这两个数据，就表示这两个数据是来自于不同分布，如果不能分开，就表示这个数据是来自同一分布的，这个技术在很多实用的技术中经常会用到它，比如说在一个训练集上训练一个模型然后把它部署到另外一个环境，然后看新的测试数据跟训练数据是不是一样的时候，就可以训练一个分类器把它分一下就行了，这样就可以避免训练一个模型部署到一个新的环境，然后新的环境和模型不匹配的问题）





期望的计算如下图所示

![img](https://i0.hdslb.com/bfs/note/6d803a867e81ecb80f5a459a5f2c52c07019135d.png@640w_!web-note.webp)



33:08



- 等式右边第一项是在Pdata上面对函数求均值
- 等式右边第二项是在Pz上面对函数求均值
- 已知x=g(z)，x是由g（z）生成出来的，假设Pg就是生成器对应的数据映射，就将g（z）替代成x，替代之后，右边第二项对z的概率求期望就变成了对x求期望，x的分布来自于生成器所对应的Pg。
- 一旦完成替代之后，第一项和第二项是可以合并了，合并之后，积分里面的东西抽象出来经过替换变量就可以得到一个关于y的函数，如果y是一个值的话，它其实是一个凸函数，取决于a、b不一样，它的形状不一样。因为它是一个凸函数，所以他会有一个最大值，因为要求最大值，所以会求导，结果是y=a/（a+b），意味着对于任何的x，最优解的D对他的输出等于y等于Pdata(x)/(Pdata(x) + Pg(x))，就证明了之前的结论

![img](https://i0.hdslb.com/bfs/note/5ae22246d6ecb609ae61062433a45d47e8c755c3.png@640w_!web-note.webp)

- 将所求到的最优解代入到上图所示的价值函数中，最大化D，就是将D*直接代进去然后展开，就能得到如上图所示的结果，就能得到之前的结论，再把得到的结果写成一个关于G的函数，因为D已经求得最优解并带入了，所以整个式子就只跟G相关，所以将他记成C(G)，到此对整个价值函数求解就只需要对C(G)进行最小化就行了，因为D的最优解已经算出来了
- 定理一是说当且仅当生成器的分布和真实数据的分布是相等的情况下，C(G)取得全局最小值的时候
- KL散度：用来衡量两个分布。如下图左侧红色公式所示，它表示的在知道p的情况下至少要多少个比特才能够将q描述出来

![img](https://i0.hdslb.com/bfs/note/de4bcc1019ef3c5ce818ce2cfebff93102ef73c1.png@640w_!web-note.webp)



37:11



- 上式中最终结果中的两项实际上就是两个KL散度如下图中的公式所示

![img](https://i0.hdslb.com/bfs/note/175320a3dd312c17a161296dc5a96f3c984886e5.png@640w_!web-note.webp)

- ==KL散度一定是大于等于零的，KL要等于0，那么p和q要相等==
- 如果C(G)要取得最小值，所以需要两个KL散度等于零，又因为p=q，所以Pdata=(Pdata+Pg)/2，所以C(G)的最优解就等价于Pdata=Pg，这就证明了D在已经取得了最优解的情况下，如果想要对G取最优解的话一定是Pg=Pdata，具体来说，对于写成这种形式的两个分布又叫做JS散度
- JS散度和KL散度的区别：<font color=red>JS散度是对称的，而KL不是对称的</font>，不能将p和q进行互换，但是对于JS散度，将p和q进行互换也是可以保持不变的，所以说它是一个对称的散度，而KL是一个不对称的散度
- 也有评论说因为GAN是一个对称的散度，所以使得它在训练上更加容易。但是也可以取一个更好的目标函数使得训练更加艰难
- 到此就证明了目标函数的选择还是很不错的





结论二是说算法一是能够优化目标函数的



当G和D有足够的容量的时候而且算法一允许在中间的每一步D是可以达到它的最优解的时候，如果对G的优化是去迭代下图所示的步骤（式中G已经换成最优解了），那么最后的Pg会收敛到Pdata

![img](https://i0.hdslb.com/bfs/note/d48abdd39dfef7525a35ecb0c8509a938f333080.png@640w_!web-note.webp)

- 将目标（价值函数）看成是一个关于Pg（模型或者分布）的函数，Pg其实是一个函数，那么目标函数就是一个关于函数的函数
- 一个函数的输入可以是标量或者是向量
- 这里目标函数是一个函数的函数：输入不再是一个值，而是一个值加上了计算（等于是说在python中写一个函数，本来是接收一个x，x是一个vector，然后现在需要接收一个clousure，clousure就包括了计算和数），之前是在高维的值的空间里面做迭代，现在需要在一个函数空间里面做梯度下降
- Ex~Pg其实是关于Pg的一个很简单的函数，这个东西展开之后就是把Pg写出来，是一个积分，积分里面有一个Pg(x)，后面一项跟Pg无关，所以他其实就是一个线性函数，而且是一个凸函数
- 在每一步中把D求到最优，就是说一个凸函数的上限函数还是一个凸函数，所以这个凸函数做梯度下降的时候会得到一个最优解
- 虽然假设了每一次会对D优化到极致，但实际上在算法上只是迭代了k步，所以说这个证明并不能说算法一是工作的，但是实际上算法一跑的还是挺好的（其实算法一跑的并不好，还是挺难收敛的，经常会出现各种问题）

### 7、实验+总结



下图是生成的一些图片

![img](https://i0.hdslb.com/bfs/note/d0cd6b6e8cd4349ec8082e2411ac3497d8eb2209.png@640w_!web-note.webp)

- 数字生成的还行，但是后面的图片效果不太好，分辨率特别低，需要很长的时间才能生成稍微能看的图片



总结

- 坏处是整个训练是比较难的，G和D需要比较好的均衡，如果没有均衡好的话会导致生成的图片比较差
- 优势是因为生成器并没有看真正样本上的数据，没有试图去拟合真实数据的特征，使得它能够生成一些比较锐利的边缘，但是这个说法在后面发现并不是这样的



未来的工作

- conditional GAN：现在生成的时候是不受控制的，随便给定一个z，然后看最终出来的是什么东西，但最好是说控制一下去偏向所想要生成的东西



### 8、评论



写作

- 总的来说，写作还是比较明确的，主要关注GAN在干什么
- 摘要中主要讲述了GAN在干什么事情
- intro非常短，首先写了一点故事性（为什么要做这个事情），然后接下来就是写GAN在干什么
- 在相关工作中，虽然第一个版本写的比较糟糕，基本上就是在说与别人不一样，但是后来的版本也基本承认了很多想法前面的人工作都已经做过了（真正伟大的工作不在乎你的那些想法在别的地方已经出现过还是没有，**关键是说你能够给大家展示用这个东西在某个应用上能够取得非常好的效果，能够让别人信服跟着你继续往下做，然后把整个领域做大，这个是伟大工作的前提）**
- 第三章讲的是GAN的目标函数以及如何做优化
- 第四章证明了为什么目标函数能得到最优解以及求解算法在一定程度上能够得到最优解
- 最后一章简单介绍了一些实验和未来的工作

这样的写法比较i清楚，想读的东西可以一路读下来



但是如果工作的开创性并不是很高的时候就一定要写清楚跟别人的区别是什么和贡献是什么





对于GAN本身这个算法而言

- 它开创了一个领域
- 从一般化的角度来及那个，它影响了之后的很多工作（不仅仅是关于GAN）：1、他是无监督学习的，不需要使用标号；2、他用一个有监督学习的损失函数来做无监督学习的，他的标号（来自于采样的还是生成的）来自于数据，用了监督学习的损失函数，所以在训练上确实会高效很多，这也是之后自监督学习（比如说BERT）的灵感的来源



### 9、shuhuai008  讲解

![image-20241001222738993](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241001222738993.png)



1. **P(x ; θg)**: 这里，$P(x ; \theta_g)$表示条件概率分布，表示数据 \(x\) 在给定参数 $\theta_g$ 下的概率。这个分布可以是高斯分布、伯努利分布、或其他形式，取决于具体上下文中的模型。
   - **\(x\)** 是观测到的或生成的数据。
   - $\theta_g$通常是模型的参数（可能是均值、方差等），它控制生成数据的分布。
   
2. **Z ~ Pz(Z)**: 这个表示 Z 是随机变量，并服从 \(P_Z(Z)\) 的概率分布。
   - **\(Z\)** 是一个随机变量，它可能是潜在变量或者从先验分布中抽取的样本。
   - **\(P_Z(Z)\)** 通常是 Z 的概率分布，可能是标准正态分布、均匀分布等。



这个表达式可能出现在生成模型（例如**变分自编码器 (VAE)** 或 **GANs**）中。具体含义可以是：

- $Z \sim P_Z(Z)$ 表示从先验分布 \(P_Z(Z)\) 中抽取潜在变量 \(Z\)。
- 然后使用生成模型根据 \(Z\) 和模型参数 \(\theta_g\) 生成观测数据 \(x\)，即 \(P(x ; \theta_g)\)。

因此，整个表达式可以解释为：**从潜在分布 \(P_Z(Z)\) 中采样 \(Z\)，并通过生成分布 \(P(x ; \theta_g)\) 来生成观测数据 \(x\)。**

这种方法通常用于深度生成模型（如VAE）中，来描述从潜在空间生成数据的过程。

如果你有更详细的背景或任务说明，可以更具体地讨论这个概念。

![image-20241002093928249](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002093928249.png)

![image-20241002103134188](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002103134188.png)



## 五、Bert阅读

**BERT**



pre-training of deep bidirectional transformers for language understanding



自然语言中中近三年最重要的文章



在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能



但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练



BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃



### 1、标题 + 作者



**pre-training**：在一个数据集上训练好一个模型，这个模型主要的目的是用在另外一个任务上面，所以如果另外一个任务叫training的话，那么在大的数据集上训练的这个任务（模型）就叫做pre-training，即training之前的任务



**deep**：更深的神经网络



**bidirectional**：双向的



**transformers**：



**language understanding**：transformer主要是用在机器翻译这个小任务上，这里使用的是一个更加广义的词，就是对语言的理解



这篇文章是关于BERT模型，它是一个深的双向的transformer，是用来做预训练的，针对的是一般的语言的理解任务



作者来自**Google AI**语言团队



### 2、摘要



BERT是一个新的语言表示模型，BERT的名字来自于：

- Bidirectional
- Encoder
- Representation
- Transformer

它的意思是transformer这个模型双向的编码器表示，这四个词跟标题是不一样的



**它的想法是基于ELMo**

- ELMo来自于芝麻街中人物的名字，芝麻街是美国的一个少儿英语学习节目
- BERT是芝麻街中另外一个主人公的名字
- 这篇文章和之前的ELMo开创了NLP的芝麻街系列文章



**BERT和最近的一些语言的表示模型有所不同**

- Peters 引用的是ELMo
- Radfor 引用的是GPT



**BERT和ELMo、GPT的区别：**

- BERT是设计用来训练深的双向表示，使用的是没有标号的数据，再联合左右的上下文信息
- 因为这样的设计导致训练好的BERT只用加一个额外的输出层，就可以在很多NLP的任务（比如问答、语言推理）上面得到一个不错的结果，而且不需要对任务做很多特别的架构上的改动

GPT考虑的是单向（用左边的上下文信息去预测未来），BERT同时使用了左侧和右侧的信息，它是双向的（Bidirectional）

ELMO用的是一个基于RNN的架构，BERT用的是transformer，所以ELMo在用到一些下游任务的时候需要对架构做一点点调整，但是BERT相对比较简单，和GPT一样只需要改最上层就可以了



分别用两句话讲清楚了BERT和GPT、ELMo的区别，在摘要的前面讲清楚了文章和哪两个工作相关，以及和这两个工作的区别是什么，在一定程度上表示了本文的工作是基于上述的两个工作之上然后做了一些改动





**BERT的好处**

- 模型概念上更加简单而且效果更好。它在11个NLP的任务上得到了新的最好的结果（包括GLUE、MultiNLI、SQvAD v1.1、SQvAD v2.0等绝对精度都有一定的提升、相对好处（具体提升了多少））



摘要整体上有两段话，一段话是跟另外两篇相关工作的区别，第二段话是说结果特别好。先写改进再写结果比别人好在什么地方









3、导言



第一段一般是交代论文所研究方向的上下文关系



在语言模型中，预训练可以用来提升很多自然语言的任务



自然语言任务包括两类

- 句子层面的任务（sentence-level）：主要是用来建模句子之间的关系，比如说对句子的情绪识别或者两个句子之间的关系
- 词元层面的任务（token-level）：包括实体命名的识别（对每个词识别是不是实体命名，比如说人名、街道名），这些任务需要输出一些细腻度的词元层面上的输出



预训练在NLP中已经流行了有一阵子了，在计算机视觉里面已经用了很多年了，同样的方法用到自然语言上面也不会很新，但是在介绍BERT的时候，很有可能会把NLP做预训练归功于BERT，BERT不是第一个提出来的而是BERT让这个方法出圈了让后面的研究者跟着做自然语言的任务



导言的第二段和之后一般是摘要的第一段的扩充版本



在使用预训练模型做特征表示的时候，一般有两类策略

- 一个策略是**基于特征**的，代表作是ELMo，对每一个下游的任务构造一个跟这个任务相关的神经网络，它使用的RNN的架构，然后将预训练好的这些表示（比如说词嵌入也好，别的东西也好）作为一个额外的特征和输入一起输入进模型中，希望这些特征已经有了比较好的表示，所以导致模型训练起来相对来说比较容易，这也是NLP中使用预训练模型最常用的做法（把学到的特征和输入一起放进去作为一个很好的特征表达）
- 另一个策略是**基于微调**的，这里举的是GPT的例子，就是把预训练好的模型放在下游任务的时候不需要改变太多，只需要改动一点就可以了。这个模型预训练好的参数会在下游的数据上再进行微调（所有的权重再根据新的数据集进行微调）



介绍别人的方法的目的通常来讲是为了铺垫自己的方法，别人哪些地方做的不好，自己的方法有所改进



上述两个途径在预训练的时候都是使用一个相同的目标函数，都是使用一个单向的语言模型（给定一些词去预测下一个词是什么东西，说一句话然后预测这句话下面的词是什么东西，属于一个预测模型，用来预测未来，所以是单向的）



第三段讲述了本文的主要想法：现在这些技术会有局限性，特别是做预训练的表征的时候，主要的问题是标准的语言模型是单向的，这样就导致在选架构的时候会有局限性

- 在GPT中使用的是一个从左到右的架构（在看句子的时候只能从左看到右），这样的坏处在于如果要做句子层面的分析的话，比如说要判断一个句子层面的情绪是不是对的话，从左看到右和从右看到左都是合法的，另外，就算是词元层面上的一些任务，比如QA的时候也是看完整个句子再去选答案，而不是一个一个往下走

因此如果将两个方向的信息都放进去的话，应该是能够提升这些任务的性能的



在指出了相关工作的局限性和提出了自己的想法之后，接下来就开始讲作者是如何解决这个问题的：提出了BERT，BERT是用来减轻之前提到的语言模型是一个单向的限制，使用的是一个带掩码的语言模型（masked language model），这个语言模型是受Cloze任务的启发（引用了一篇1953年的论文）

- 这个带掩码的语言模型每一次随机地选一些资源，然后将它们盖住，目标函数就是预测被盖住的字，等价于将一个句子挖一些空完后进行完形填空
- 跟标准的语言模型从左看到右的不同之处在于：带掩码的语言模型是允许看到左右的信息的（相当于看完形填空的时候不能只看完形填空的左边，也需要看完形填空的右边），这样的话它允许训练深的双向的transformer模型
- 在带掩码的语言模型之外还训练了一个任务，预测下一个句子，核心思想是给定两个句子，然后判断这两个句子在原文里面是相邻的，还是随机采样得来的，这样就让模型学习了句子层面的信息



**这篇文章的贡献**

1. 展示了双向信息的重要性，GPT只用了单向，之前有的工作只是很简单地把一个从左看到右的语言模型和一个从右看到左的语言模型简单地合并到一起，类似于双向的RNN模型（contact到一起），这个模型在双向信息的应用上更好
2. 假设有一个比较好的预训练模型就不需要对特定任务做特定的模型改动。BERT是第一个在一系列的NLP任务上（包括在句子层面上和词元层面上的任务）都取得了最好的成绩的基于微调的模型
3. 代码和模型全部放在：https://github.com/google-research/bert









**4、结论**



最近一些实验表明，使用无监督的预训练是非常好的，这样使得资源不多（训练样本比较少的任务也能够享受深度神经网络）,本文主要的工作就是把前人的工作扩展到深的双向的架构上，使得同样的预训练模型能够处理大量的不同的自然语言任务



简单概括一下：本文之前的两个工作一个叫**ELMo**，它使用了双向的信息但是它网络架构比较老，用的是RNN，另外一个工作是**GPT**，它用的是transformer的架构，但是它只能处理单向的信息，因此本文将ELMo双向的想法和GPT的transformer架构结合起来就成为了BERT

- 具体的改动是在做语言模型的时候不是预测未来，而是变成完形填空



很多时候我们的工作就是把两个东西缝合到一起，或者把一个技术用来解决另外领域的问题，如果所得到的东西确实简单好用，别人也愿意使用，就朴实地将它写出来也没有问题









**5、相关工作**



**非监督的基于特征的一些工作**

- 词监督
- ELMo



**非监督的基于微调的一些工作**

- 代表作是GPT



**在有标号的数据上做迁移学习**

- 在NLP中有标号而且比较大的数据（包括自然语言的推理和机器翻译这两块中都有比较大的数据集）
- 然后在这些有标号的数据集上训练好了模型然后在别的任务上使用

在计算机视觉中这一块使用比较多：经常在ImageNet上训练好模型再去别的地方使用，但是在NLP这一块不是特别理想（可能一方面是因为这两个任务跟别的任务差别还是挺大的，另一方面可能是因为数据量还是远远不够的），BERT和他后面的一系列工作证明了在NLP上面使用没有标号的大量数据集训练成的模型效果比在有标号的相对来说小一点的数据集上训练的模型效果更好，同样的想法现在也在慢慢地被计算机视觉采用，就是说在大量的没有标号的图片上训练出的模型也可能比在ImageNet这个100万数据集上训练的模型可能效果更好









**6、BERT模型**



主要介绍了实现的一些细节



BERT中有两个步骤：

- **预训练**：在预训练中，这个模型是在一个没有标号的数据集上训练的
- **微调**：在微调的时候同样是用一个BERT模型，但是它的权重被初始化成在预训练中得到的权重，所有的权重在微调的时候都会参与训练，用的是有标号的数据



每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早预训练好的BERT模型作为初始化，但是每个下游任务都会根据自己的数据训练好自己的模型



虽然预训练和微调不是BERT独创的，在计算机视觉中用的比较多，但是作者还是做了一个简单的介绍（在写论文的时候遇到一些技术需要使用的时候，而且可能应该所有人都知道，最好不要一笔带过，论文是需要自洽的，后面的人读过来可能不太了解这些技术，但是这些技术又是论文中方法不可缺少的一部分的话，最好还是能够做一个简单的说明）





下图中左图表示预训练，右图表示微调

![img](https://i0.hdslb.com/bfs/note/576a684529ef23d69c4ad806b7c4e81c9d4f19af.png@630w_!web-note.webp)

- 预训练的时候输入是一些没有标号的句子对
- 这里是在一个没有标号的数据上训练出一个BERT模型，把他的权重训练好，对下游的任务来说，对每个任务创建一个同样的BERT模型，但是它的权重的初始化值来自于前面预训练训练好的权重，对于每一个任务都会有自己的有标号的数据，然后对BERT继续进行训练，这样就得到了对于某一任务的BERT版本





**模型架构**

![img](https://i0.hdslb.com/bfs/note/2850d9e7574d552db3406e49b4e946a6fef775d8.png@630w_!web-note.webp)

BERT模型就是一个多层的双向transformer编码器，而且它是直接基于原始的论文和它原始的代码，没有做改动

三个参数

- L：transformer块的个数
- H：隐藏层的大小
- A：自注意力机制中多头的头的个数

两个模型

- BERT base：它的选取是使得跟GPT模型的参数差不多，来做一个比较公平的比较
- BERT large：用来刷榜
- BERT中的模型复杂度和层数是一个线性关系，和宽度是一个平方的关系



**怎样把超参数换算成可学习参数的大小**

模型中可学习参数主要来自两块

- 嵌入层：就是一个矩阵，输入是字典的大小（假设是30k），输出等于隐藏单元的个数（假设是H）
- transformer块：transformer中有两部分：一个是自注意力机制（它本身是没有可学习参数的，但是对多头注意力的话，他会把所有进入的K、V、Q分别做一次投影，每一次投影的维度是等于64的，因为有多个头，头的个数A乘以64得到H，所以进入的话有key、value、q，他们都有自己的投影矩阵，这些投影矩阵在每个头之间合并起来其实就是H*H的矩阵了，同样道理拿到输出之后也会做一次投影，他也是一个H*H的矩阵，所以对于一个transformer块，他的自注意力可学习的参数是H的平方乘以4），一个是后面的MLP（MLP里面需要两个全连接层，第一个层的输入是H，但是它的输出是4*H，另外一个全连接层的输入是4*H，输出是H，所以每一个矩阵的大小是H*4H，两个矩阵就是H的平方乘以8），这两部分加起来就是一个transformer块中的参数，还要乘以L（transformer块的个数）

所以总参数的个数就是30k乘以H（这部分就是嵌入层总共可以学习的参数个数）再加上L层乘以H的平方再乘以12

![img](https://i0.hdslb.com/bfs/note/ff210a3cba166eaecd3db56e6242a725a3e45994.png@630w_!web-note.webp)





**输入和输出**

对于下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使BERT模型能够处理所有的任务，它的输入既可以是一个句子，也可以是一个句子对

- 这里的一个句子是指一段连续的文字，不一定是真正的语义上的一段句子
- 输入叫做一个序列，可以是一个句子，也可以是两个句子
- 这和之前文章里的transformer是不一样的：transformer在训练的时候，他的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是BERT只有一个编码器，所以为了使它能够处理两个句子，就需要把两个句子变成一个序列





**序列的构成**

这里使用的切词的方法是WordPiece，核心思想是：

- 假设按照空格切词的话，一个词作为一个token，因为数据量相对比较大，所以会导致词典大小特别大，可能是百万级别的，那么根据之前算模型参数的方法，如果是百万级别的话，就导致整个可学习参数都在嵌入层上面
- WordPiece是说假设一个词在整个里面出现的概率不大的话，那么应该把它切开看它的一个子序列，它的某一个子序列很有可能是一个词根，这个词很有可能出现的概率比较大话，那么就只保留这个子序列就行了。这样的话，可以把一个相对来说比较长的词切成很多一段一段的片段，而且这些片段是经常出现的，这样的话就可以用一个相对来说比较小的词典就能够表示一个比较大的文本了

切好词之后如何将两个句子放在一起

- 序列的第一个词永远是一个特殊的记号[CLS]，CLS表示classification，这个词的作用是BERT希望最后的输出代表的是整个序列的信息（比如说整个句子层面的信息），因为BERT使用的是transformer的编码器，所以它的自注意力层中每一个词都会去看输出入中所有词的关系，就算是词放在第一的位置，它也是有办法能够看到之后的所有词

把两个句子合在一起，但是因为要做句子层面的分类，所以需要区分开来这两个句子，这里有两个办法：

1. 在每一个句子后面放一个特殊的词：[SEP]表示separate
2. 学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子

下图中红线画出来的粉色方框表示输入的序列，[CLS]是第一个特殊的记号表示分类，中间用一个特殊的记号[SEP]分隔开，每一个token进入BERT得到这个token的embedding表示（对BERT来讲，就是输入一个序列，然后得到另外一个序列），最后transformer块的输出就表示这个词元的BERT表示，最后再添加额外的输出层来得到想要的结果

![img](https://i0.hdslb.com/bfs/note/4135badc3a7e7d87d7a74b989c37d29da545e14b.png@630w_!web-note.webp)





对于每一个词元进入BERT的向量表示，它是这个词元本身的embedding加上它在哪一个句子的embedding再加上位置的embedding，如下图所示

![img](https://i0.hdslb.com/bfs/note/42f0d13adc6530b7cae7e2e5018affdef470729b.png@630w_!web-note.webp)

- 上图演示的是BERT的嵌入层的做法，即由一个词元的序列得到一个向量的序列，这个向量的序列会进入transformer块
- 上图中每一个方块是一个词元
- token embedding：这是一个正常的embedding层，对每一个词元输出它对应的向量
- segment embedding：表示是第一句话还是第二句话
- position embedding：输入的大小是这个序列的最大长度，它的输入就是每个词元这个序列中的位置信息（从零开始），由此得到对应的位置的向量
- 最终就是每个词元本身的嵌入加上在第几个句子的嵌入再加上在句子中间的位置嵌入
- 在transformer中，位置信息是手动构造出来的一个矩阵，但是在BERT中不管是属于哪个句子，还是具体的位置，它对应的向量表示都是通过学习得来的





**预训练和微调的不同之处**

在预训练的时候，主要有两个东西比较关键

- 目标函数
- 用来做预训练的数据



**带掩码的语言模型**

对于输入的词元序列，如果词元序列是由WordPiece生成的话，那么它有15%的概率会随机替换成掩码，但是对于特殊的词元（第一个词元和中间的分割词元不做替换），如果输入序列长度是1000的话，那么就要预测150个词

这里也会存在问题：因为在做掩码的时候会把词元替换成一个特殊的token（[MASK]），在训练的时候大概会看到15%的词元，但是在微调的时候是没有的，因为在微调的时候不用这个目标函数，所以没有mask这个东西，导致在预训练和微调的时候所看到的数据会有多不同

- 解决方法：对这15%的被选中作为掩码的词有80%的概率是真的将它替换成这个特殊的掩码符号（[MASK]），还有10%的概率将它替换成一个随机的词元（其实是加入了一些噪音），最后有10%的概率什么都不干，就把它存在那里用来做预测（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/397b25bd813702907a728a46ba47747f263c8c97.png@630w_!web-note.webp)





**预训练中的第二个任务就是预测下一个句子**

在QA和自然语言推理中都是句子对，如果让它学习一些句子层面的信息也不错，具体来说，一个输入序列里面有两个句子：a和b，有50的概率b在原文中间真的是在a的后面，还有50%的概率b就是随机从别的地方选取出来的句子，这就意味着有50%的样本是正例（两个句子是相邻的关系），50%的样本是负例（两个句子没有太大的关系），加入这个目标函数能够极大地提升在QA和自然语言推理的效果（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/8e4c09357f2fb5173215cc5b91710b19e6dbfdbc.png@630w_!web-note.webp)

- 上图中高亮部分的##：在原文中 flightless 是一个词，但是由于这个词出现的概率不高，所以在WordPiece中把它砍成了两个词 flight 和 less ，他们都是比较常见的词，##表示后面的词在原文中其实是跟在前面那个词后面的意思





**预训练数据**

![img](https://i0.hdslb.com/bfs/note/df1a2fac07b73131eccdc47b80795d118ead5ce0.png@630w_!web-note.webp)

使用了两个数据集

- BooksCorpus
- English Wikipedia

应该使用文本层面的数据集，即数据集里面是一篇一篇的文章而不是一些随机打乱的句子，因为transformer确实能够处理比较长的序列，所以对于整个文本序列作为数据集效果会更好一些





**用BERT做微调**

BERT和一些基于编码器解码器的架构有什么不同

- transformer是编码器解码器架构
- 因为把整个句子对都放在一起放进去了，所以自注意力能够在两端之间相互能够看到，但是在编码器解码器这个架构中，编码器一般是看不到解码器的东西的，所以BERT在这一块会更好一点，但是实际上也付出了一定的代价（不能像transformer一样能够做机器翻译）



在做下游任务的时候会根据任务设计任务相关的输入和输出，好处是模型其实不需要做大的改动，主要是怎么样把输入改成所要的那个句子对

- 如果真的有两个句子的话就是句子a和b
- 如果只有一个句子的话，比如说要做一个句子的分类，b就没有了

然后根据下游的任务要求，要么是拿到第一个词元对应的输出做分类或者是拿到对应的词元的输出做所想要的输出，不管怎么样都是在最后加一个输出层，然后用一个softnax得到想要的标号



跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU跑一个小时就可以了，使用GPU的话多跑几个小时也行









**7、实验**



介绍了BERT怎么样用在各个下游任务上



**GLUE**

它里面包含了多个数据集，是一个句子层面的任务

BERT就是把第一个特殊词元[CLS]的最后的向量拿出来，然后学习一个输出层w，放进去之后用softmax就能得到标号，这就变成了一个很正常的多分类问题了

下图表示了在这个分类任务上的结果

![img](https://i0.hdslb.com/bfs/note/30a0222074f444f72f042dfdeb0f05e43c3e3f9d.png@630w_!web-note.webp)

- average表示在所有数据集上的平均值，它表示精度，越高越好
- 可以发现就算是BERT就算是在base跟GPT可学习参数差不多的情况下，也还是能够有比较大的提升



**SQuAD v1.1**

斯坦福的一个QA数据集

QA任务是说给定一段话，然后问一个问题，需要在这段话中找出问题的答案（类似于阅读理解），答案在给定的那段话中，只需要把答案对应的小的片段找出来就可以了（找到这个片段的开始和结尾）

- 就是对每个词元进行判断，看是不是答案的开头或者答案的结尾

具体来说就是学两个向量S和E，分别对应这个词元是答案开始的概率和答案最后的概率，它对每个词元（也就是第二句话中每个词元）的S和Ti相乘，然后再做softmax，就会得到这个段中每一个词元是答案开始的概率，公式如下图所示，同理也可以得出是答案末尾的概率

![img](https://i0.hdslb.com/bfs/note/7cca66ff2bb525232c965370ccf17888bd7ace21.png@630w_!web-note.webp)

- Ti表示第 i 个输入词元对应的最后一个隐藏向量



**在做微调的时候的参数设置**

- 使用了3个epoch，扫描了3遍数据
- 学习率是5e-5
- batchsize是32

用BERT做微调的时候结果非常不稳定，同样的参数、同样的数据集，训练十遍，可能会得到不同的结果。最后发现3其实是不够的，可能多学习几遍会好一点



BERT用的优化器是adam的不完全版，当BERT要训练很长时间的时候是没有影响的，但是如果BERT只训练一小段时间的话，它可能会带来影响（将这个优化器换成adam的正常版就可以解决这个问题了）



**SQuAD v2.0**



**SWAG数据集**

它用来判断两个句子之间的关系

跟之前的训练没有太多区别，BERT的结果比别的模型要好很多





对这些不同的数据集，BERT基本上只要把这些数据集表示成所要的句子对的形式，最后拿到一个对应的输出然后再加一个输出层就可以了，所以BERT对整个NLP领域的贡献还是非常大的，大量的任务可以用一个相对来说比较简单的架构，不需要改太多的东西就能够完成了





**ablation study**



介绍了BERT中每一块最后对结果的贡献

![img](https://i0.hdslb.com/bfs/note/ec4e09e283db95be14d7c2037af29101264d5d16.png@630w_!web-note.webp)

- No NSP：假设去掉对下一个句子的预测
- LTR & No NSP：使用一个从左看到右的单向的语言模型（而不是用带掩码的语言模型），然后去掉对下一个句子的预测
-  \+ BiLSTM：在上面加一个双向的LSTM

从结果来看，去掉任何一部分，结果都会打折扣





**模型大小的影响**



BERT base中有1亿的可学习参数

BERT large中有3亿可学习的参数

相对于之前的transformer，可学习参数数量的提升还是比较大的

当模型变得越来越大的时候，效果会越来越好，这是第一个展示将模型变得特别大的时候对语言模型有较大提升的工作

虽然现在GPT3已经做到1000亿甚至在向万亿级别发展，但是在三年前，BERT确实是开创性地将一个模型推到如此之大，引发了之后的模型大战





假设不用BERT做微调而是把BERT的特征作为一个静态特征输进去会怎样

结论是效果确实没有微调好，所有用BERT的话应该用微调









**8、评论**



**写作**

- 先写了BERT和GPT的区别
- 然后介绍了BERT模型
- 接下来是在各个实验上的设置
- 最后对比结果，结果非常好

这篇文章认为本文的最大贡献就是双向性（写文章最好能有一个明确的卖点，有得有失，都应该写出来）

- 但是今天来看，这篇文章的贡献不仅仅只有双向性，还有其它东西
- 从写作上来说，至少要说选择双向性所带来的不好的地方是什么，做一个选择，会得到一些东西，也会失去一些东西：和GPT比，BERT用的是编码器，GPT用的是解码器，得到了一些好处，但是也有坏处（比如做机器翻译和文本摘要比较困难，做生成类的东西就没那么方便了）
- 分类问题在NLP中更加常见，所以NLP的研究者更喜欢用BERT，会更容易一些



BERT所提供的是一个完整的解决问题的思路，符合了大家对于深度学习模型的期望：在一个很大的数据集上训练好一个很深很宽的模型，这个模型拿出来之后可以用在很多小问题上，通过微调可以全面提升这些小数据上的性能



