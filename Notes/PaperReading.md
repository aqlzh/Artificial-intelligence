[toc]



> [深度学习复盘与论文复现B_深度学习论文复现博客-CSDN博客](https://blog.csdn.net/QuantumYou/article/details/139354339?ops_request_misc=%7B%22request%5Fid%22%3A%228B832DBE-D254-4E91-8091-D1BAFA2EDA70%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fblog.%22%7D&request_id=8B832DBE-D254-4E91-8091-D1BAFA2EDA70&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-139354339-null-null.nonecase&utm_term=Convolutional Neural Networks&spm=1018.2226.3001.4450) CNN

## 论文基本知识



### 1、卷积核

在卷积神经网络（CNN）中，**滤波器（Filter）**是核心组件，它负责从输入数据（如图像）中提取不同层次的特征。滤波器也称为**卷积核（Kernel）**，通过卷积操作来捕捉图像中的局部模式，如边缘、纹理、形状等。



1. **大小（Size）**：滤波器通常是一个较小的矩阵，大小通常为 \(3 \times 3\)、\(5 \times 5\)、\(7 \times 7\) 等，大小依赖于具体的任务。
2. **数量（Number of Filters）**：每一层中可以使用多个滤波器，每个滤波器在输入数据上滑动（即卷积），从中提取不同的特征。更多的滤波器可以提取更多的特征。
3. **权重（Weights）**：滤波器的元素（权重）在训练过程中通过反向传播算法进行调整，使得CNN能够自适应学习到最佳的特征提取方式。



滤波器的工作原理

**滤波器通过与输入数据进行卷积运算，输出一个新的特征图**。卷积操作是通过将滤波器在输入数据上滑动，并对局部区域进行加权求和来实现的。具体过程如下：

1. **卷积操作**：滤波器与输入的局部区域逐个元素相乘，然后将这些乘积求和，得到输出特征图中的一个值。
2. **滑动窗口**：滤波器以一定的步长（stride）在输入图像上滑动，卷积操作会不断在不同的局部区域进行。
3. **生成特征图**：滤波器滑过输入数据的所有区域后，将输出一个特征图，该特征图表示滤波器在整个输入数据上提取到的模式。

![](https://i-blog.csdnimg.cn/blog_migrate/fc7588939aa91c81b3ad51abfc5f4f3b.gif)

卷积操作的数学公式如下：

$$
f(x, y) = \sum_{i=1}^{n}\sum_{j=1}^{m} X(x+i, y+j) \cdot W(i, j)
$$


其中：
- \(X\) 是输入数据，\(W\) 是滤波器。
- \(f(x, y)\) 是输出特征图在位置 \(x, y\) 的值。
- \(n \times m\) 是滤波器的大小。



滤波器的作用

1. **低层特征提取**：在卷积网络的前几层，滤波器通常负责提取简单的特征，如边缘、角点等。这些滤波器通常能识别出特定方向的边缘或纹理。
   
2. **高层特征提取**：随着网络层数的加深，滤波器可以提取更高级别的特征，如形状、对象的一部分等。

3. **感受野（Receptive Field）**：滤波器的大小决定了它的“感受野”，即每次卷积操作所覆盖的输入区域。随着层数增加，感受野会变大，因此滤波器能在深层网络中捕获更全局的特征。



总结

1. **滤波器**是CNN中提取特征的核心组件，卷积操作通过滤波器在输入数据上滑动，生成特征图。
2. <font color=red>**滤波器的大小**和**数量**是超参数，直接影响到模型的特征提取能力</font>
3. **多层卷积**网络能够逐步提取从低级到高级的特征，使得CNN在图像识别、分类等任务上表现优异。

滤波器的调整和选择直接影响到CNN的性能和效果，通常需要在实验中调优。



### 2、Inception

Inception模块的主要目的是通过在同一层中应用不同大小的卷积核和池化操作，提取图像的多尺度特征，从而提高网络的表达能力。



- 在Google Net  中的运用

### 3、BN

Batch Normalization（批量归一化）是一种在训练深度神经网络时常用的技术，旨在提高训练速度、稳定性和性能。它通过规范化（归一化）神经网络中间层的输入来工作，从而减少了所谓的“内部协变量偏移”（internal covariate shift），即网络中间层的输入分布随时间变化的情况。

内部协变量偏移是指在训练过程中，由于每层的参数更新，神经网络中间层的激活值的分布可能会发生变化。这可能导致训练过程中的梯度问题，比如梯度消失或梯度爆炸。

Batch Normalization 的核心思想是在网络的中间某些层（通常是卷积层或全连接层）中插入一个归一化层，对每个小批量数据的激活值进行归一化处理。归一化层会学习到两个参数，γ（gamma）和β（beta），它们允许对归一化后的数据进行缩放和偏移，以保持模型的表达能力。

Batch Normalization 的具体操作步骤如下：

1. **归一化**：对于每个小批量数据，计算其均值和方差，并使用这些统计量来归一化数据。
   $\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
   其中，\(x^{(k)}\) 是第 \(k\) 个数据点，\(\mu_B\) 是小批量数据的均值，\(\sigma_B^2\) 是小批量数据的方差，\(\epsilon\) 是一个很小的常数，用来防止除以零。

2. **缩放和偏移**：使用可学习的参数 \(\gamma\) 和 \(\beta\) 对归一化后的数据进行缩放和偏移。
   $y^{(k)} = \gamma \hat{x}^{(k)} + \beta $
   其中，\(y^{(k)}\) 是归一化层的输出。

3. **反向传播**：在训练过程中，通过反向传播算法更新 \(\gamma\) 和 \(\beta\) 的值。

Batch Normalization 的优点包括：

- **加速训练**：使得网络可以更快地收敛。
- **允许更高的学习率**：由于减少了梯度消失的问题，可以使用更大的学习率。
- **减少初始化依赖**：对权重的初始化不那么敏感。
- **作为正则化**：可以减少模型对 Dropout 的依赖，因为它本身就有一定的正则化效果。

Batch Normalization 在各种深度学习框架中都有实现，如 TensorFlow、PyTorch 等，并且已经成为许多现代神经网络架构的标准组件。



### 4、VAE



**变分自编码器（Variational Autoencoder, VAE）** 是一种生成模型，由 Kingma 和 Welling 于 2013 年提出。与传统的自编码器不同，==VAE 不仅可以进行数据的压缩与重建，还能够生成新的数据样本==。其核心思想是将数据编码为一个概率分布，然后通过该分布进行采样，从而生成新数据。



VAE 的基本概念



VAE 是一种基于概率的生成模型，结合了自编码器的特性和贝叶斯推断。其架构包括两个主要部分：

1. **编码器（Encoder）**：将输入数据映射到一个隐变量空间（通常为高斯分布）。
2. **解码器（Decoder）**：从隐变量空间中采样，通过解码器生成重建的输出。

VAE 的生成过程可以分为以下几个步骤：

1. **编码（Encoding）**：给定输入数据 \( x \)，编码器将其映射为潜在空间中的均值 \( \mu(x) \) 和标准差 \( \sigma(x) \)，并假设隐变量 \( z \) 服从高斯分布：
   
   $z \sim \mathcal{N}(\mu(x), \sigma(x))$
   
2. **采样（Sampling）**：从编码器输出的概率分布中采样隐变量 \( z \)。

3. **解码（Decoding）**：通过解码器将隐变量 \( z \) 映射回原始数据空间，生成重建的数据 \( \hat{x} \)。

VAE 的关键点：重参数技巧（Reparameterization Trick）

为了能够通过梯度下降进行训练，VAE 引入了**重参数化技巧**（Reparameterization Trick）。因为直接从高斯分布中采样 \( z \) 不能进行反向传播，重参数化技巧将采样过程拆解为一个确定性函数和一个随机变量的组合：

$z = \mu(x) + \sigma(x) \odot \epsilon$

其中 $ \epsilon \sim \mathcal{N}(0, 1) $ 是一个标准正态分布的噪声。这种方法使得 VAE 可以通过标准反向传播算法训练。



VAE 的损失函数



VAE 的损失函数包含两部分：

1. **重建损失（Reconstruction Loss）**：衡量重建数据 \( \hat{x} \) 与原始数据 \( x \) 之间的差异，通常使用均方误差或交叉熵损失。

   $\text{Reconstruction Loss} = \mathbb{E}_{q(z|x)} [\log p(x|z)]$

2. **KL 散度（KL Divergence）**：<font color=red>衡量隐变量分布 \( q(z|x) \) 与先验分布 \( p(z) \) 之间的差异</font>。通常，先验分布 \( p(z) \) 设为标准正态分布 $ \mathcal{N}(0, 1) $

   

   $D_{\text{KL}}(q(z|x) \| p(z)) = \frac{1}{2} \sum_{i=1}^{d} \left( 1 + \log(\sigma_i^2) - \mu_i^2 - \sigma_i^2 \right)
   $

因此，VAE 的总损失函数为：
$
\mathcal{L} = \text{Reconstruction Loss} + D_{\text{KL}}(q(z|x) \| p(z))
$

通过最小化这一损失函数，VAE 可以同时优化编码器和解码器，使得模型不仅能够重建输入数据，还能够从潜在空间中生成新样本。



应用

1. **图像生成**：VAE 可以从潜在空间中采样隐变量，**并通过解码器生成类似于训练数据的新图像**。
2. **数据降维**：VAE 的编码器部分可以用于将高维数据降维，同时保证潜在变量有良好的分布结构。
3. **异常检测**：由于 VAE 学习了数据的生成过程，它能够识别与训练数据分布不同的异常数据。

VAE 的优缺点

- **优点**：
  - 能够生成新数据，并且生成的数据具有很强的连贯性。
  - ==通过 KL 散度，VAE 保证了潜在空间中的点与先验分布一致，使得生成的样本在隐变量空间中是连续的==。

- **缺点**：
  - 生成的图像质量通常不如 GAN（生成对抗网络）高。
  - KL 散度的权重较难平衡，有时会出现“KL 瓶颈”问题，即模型过度依赖于重建损失，而忽略潜在变量的正则化。

总结

变分自编码器（VAE）是自编码器和生成模型的结合，它能够学习数据的分布并生成新的样本。VAE 的关键在于其通过重参数化技巧，<font color=red>使用编码器和解码器的组合来学习潜在空间中的概率分布</font>。尽管生成质量不如 GAN，但 VAE 在连贯性、理论优雅性和训练稳定性方面具有明显优势。





### 5、SGD

**随机梯度下降（Stochastic Gradient Descent, SGD）**是一种用于优化机器学习模型的常用方法，尤其在深度学习和大规模数据集训练中表现突出。它是一种基于梯度下降的优化算法，<font color=blue>区别在于每次更新参数时只使用一个或少量的样本，而不是整个数据集</font>。



工作原理：

1. **初始化参数**：首先，模型的参数随机初始化。
2. **随机选择样本**：==从训练数据集中随机选择一个样本或一小批样本（称为mini-batch）==。
3. **计算损失和梯度**：基于选定的样本，计算损失函数的值，并对模型参数求梯度。
4. **更新参数**：根据损失函数的梯度，用以下公式更新参数：
   $
   \theta = \theta - \eta \cdot \nabla L(\theta)
   $
   其中，\(\theta\) 是模型的参数，\(\eta\) 是学习率，\(\nabla L(\theta)\) 是当前样本的梯度。
5. **重复迭代**：重复步骤2-4，直到模型的参数收敛或达到设定的迭代次数。



特点与优点：

- **速度快**：相比批量梯度下降（Batch Gradient Descent），SGD每次仅使用一部分数据进行参数更新，能够快速收敛。
- **在线学习**：适用于流式数据或者数据规模非常大的场景，可以在数据逐渐到达的过程中实时更新模型。
- **避免局部最优**：由于引入了噪声，SGD可以帮助模型跳出局部最优点，找到更好的全局最优。



缺点：

- **更新不稳定**：由于每次仅使用少量样本更新参数，更新路径会有较大的波动，可能导致收敛较慢或难以找到最优解。
- **调参困难**：学习率的选择非常关键，过大可能导致不收敛，过小则收敛过慢。

<font color=red>常见改进</font>：

- **Mini-batch SGD**：通过每次使用多个样本（小批量）来更新参数，可以减少梯度的波动，同时保持SGD的高效性。
- **动量（Momentum）**：引入动量机制，以减少更新过程中的震荡并加速收敛。
- **自适应学习率**：像Adam、RMSProp这样的算法，动态调整学习率，以加速收敛。

SGD广泛用于神经网络训练等大规模机器学习任务中，是深度学习优化中的核心工具之一。



### 6、Identity mapping

**Identity Mapping**（恒等映射）是指在函数或网络层中输出保持与输入完全相同的一种映射方式。数学上，它可以表示为：
$
f(x) = x
$
其中，输入 \( x \) 与输出 \( x \) 完全一致，不进行任何修改。



在神经网络中的作用：

在深度学习中，**identity mapping** 通常出现在深度残差网络（ResNet）等架构中。==ResNet通过引入“残差连接”解决了深度神经网络中训练困难、梯度消失等问题==。残差连接实现的就是一种 **identity mapping**，即让网络层的某些部分直接将输入复制到输出，以便保留原始信息。

为什么有用？

1. **防止梯度消失**：在深层网络中，梯度会随着层数增多而逐渐衰减，导致模型训练困难。通过 identity mapping，网络可以轻松地跨层传递信息，避免梯度消失问题。
2. **简化优化问题**：在ResNet中，网络不需要学习所有层的复杂映射，只需要学习与输入的残差部分。换句话说，即使某些层不对输出做任何处理，模型也能保持有效。



ResNet中的残差块结构：

残差块可以被表示为：
$
y = f(x) + x
$
其中 \( f(x) \) 是一个非线性变换（卷积、ReLU等），而 \( x \) 是输入。通过这种设计，若学习的 \( f(x) \) 是0，网络也可以通过 identity mapping 直接将输入传递到输出，从而使得深层网络的训练更加稳定。



总结：

**Identity Mapping** 是一种保持输入与输出相同的映射方式，在深度学习中，它通过“跳跃连接”帮助缓解深层网络的训练难题，尤其是在残差网络中起到了至关重要的作用。



### 7、Bottleneck Design

**Bottleneck Design** 是在深度神经网络（特别是残差网络，ResNet）中常用的一种架构设计，其目的是减少网络的计算量和参数，同时保持模型的表示能力。



“Bottleneck” 是一种通过缩减网络内部维度来减少计算复杂度的设计。具体来说，**bottleneck block** 通常采用一个三层的结构，==通过先缩减维度，再恢复维度的方式进行卷积运算==：

1. **1x1卷积（降维）**：先用一个 1x1 的卷积核来减少输入通道的数量，降低计算量。
2. **3x3卷积（核心处理）**：再进行标准的 3x3 卷积操作，保持特征的空间结构。
3. **1x1卷积（升维）**：最后再通过 1x1 的卷积核把通道数还原到原始维度。

整个过程类似于“压缩-处理-解压”，中间的 3x3 卷积操作可以在低维空间中完成，从而显著减少计算复杂度。



公式：

假设输入通道数为 \( C \)，bottleneck block 的设计可以表示为：
- **降维（1x1卷积）：** 将通道数从 \( C \) 减少到一个较小的数（如 \( C/4 \)）。
- **3x3卷积：** 在低维空间进行卷积操作。
- **升维（1x1卷积）：** 将通道数恢复到原始的 \( C \)。

为什么使用Bottleneck设计？

1. **减少计算成本**：普通卷积操作随着通道数的增加，计算成本会成倍增长。bottleneck通过降低中间维度，使得计算量大大减少，尤其是在处理高维输入时。
   
2. **参数减少**：这种设计不仅减少了计算，还显著降低了模型的参数量，从而加快训练速度，同时减少过拟合风险。

3. **保持表示能力**：尽管中间维度被压缩，但由于有降维和升维的操作，网络依然能够有效地保持特征表达能力。

应用：

Bottleneck Design 在深度残差网络（ResNet）中广泛使用，特别是在 ResNet-50、ResNet-101、ResNet-152 这些较深的模型中，bottleneck block 用于构建残差模块。这种设计使得网络可以在增加深度的同时，避免模型过于庞大，提升训练和推理的效率。



### 8、NCE



在机器学习中，**NCE（Noise Contrastive Estimation）** 是一种用于**概率模型**的高效学习方法，尤其在**自然语言处理**和**深度学习**中得到广泛应用。<font color=red>NCE 的主要思想是将**密度估计问题**转化为一个**分类问题**，通过对比**真实样本**和**噪声样本**来学习模型的参数</font>。



NCE 的主要思想

NCE 的核心理念是：与其直接计算数据的精确概率分布，不如通过引入噪声分布，将密度估计简化为一个**二分类问题**，即区分“**真实样本**”和“**噪声样本**”。

- **真实样本**：来自训练数据的样本，符合模型的真实分布。
- **噪声样本**：由某个已知的简单分布（例如均匀分布）生成的样本。

通过这个对比，NCE 避免了计算复杂的**归一化常数**，从而提高了计算效率。



NCE 的应用场景

NCE 经常用于以下场景：

1. **词向量学习**：如在 Word2Vec 中，NCE 被用来高效地训练语言模型，以降低计算复杂度。
2. **语言模型**：NCE 通过最大化对真实数据与噪声样本的区分，帮助语言模型学习词的概率分布。
3. **生成模型**：在生成模型的训练中，NCE 可以作为一种近似方法来优化难以计算的似然估计。



NCE 的好处

- **减少计算复杂度**：==相比于传统的最大似然估计（MLE），NCE 不需要计算复杂的归一化常数==。
- **高效训练**：尤其在处理大规模数据或词汇量时，NCE 能显著减少训练时间。
- **灵活性**：NCE 允许引入不同的噪声分布，使其可以适应不同类型的模型和数据。

NCE 本质上是通过噪声对比来简化概率模型的训练过程，并且已经成为深度学习中高效概率估计的重要工具。





### 9、KL 散度和JS 散度



==KL 散度 (Kullback-Leibler Divergence)==



KL 散度（Kullback-Leibler Divergence）是度量两个概率分布之间差异的非对称度量。它描述了一个**真实分布 P** 和一个**近似分布 Q** 之间的差异。KL 散度不是真正的“距离”，因为它不满足对称性，也不满足三角不等式。

KL 散度的定义为：
$
D_{KL}(P \| Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
$
或者在连续情况下：
$
D_{KL}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx
$
其中：

- \( P(x) \) 是真实的概率分布。
- \( Q(x) \) 是近似的概率分布。
- \( \log \frac{P(x)}{Q(x)} \) 是两个分布之间的对数比例（它衡量每一个点上这两个分布的差异）。

**直观理解**：

- KL 散度衡量的是：使用分布 \( Q \) 来“替代”分布 \( P \) 所导致的信息损失。
- 如果 \( P(x) = Q(x) \) 对所有 \( x \) 都成立，则 \( D_{KL}(P \| Q) = 0 \)，表示两个分布完全一致。
- \( D_{KL}(P \| Q) \) 一般为正数，且 \( D_{KL}(P \| Q) \neq D_{KL}(Q \| P) \)，因此 KL 散度是不对称的。

**应用**：

1. **信息论**：KL 散度最早出现在信息论中，作为两种概率分布（或事件）的“相对熵”。
2. **机器学习**：在生成模型中，特别是变分自动编码器（VAE）中，KL 散度用于衡量潜在变量分布与预设分布（如标准正态分布）的差异。
3. **概率模型**：在贝叶斯推理中，KL 散度常用于衡量后验分布与先验分布之间的差异。



==JS 散度 (Jensen-Shannon Divergence)==



JS 散度（Jensen-Shannon Divergence）是 KL 散度的对称化版本，解决了 KL 散度的不对称问题。它通过引入两个分布的“中间分布”来衡量两者的差异，是一种对称的、有限的散度度量。

JS 散度的定义为：
$
D_{JS}(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M)
$
其中 $M = \frac{1}{2}(P + Q) $ 是 \( P \) 和 \( Q \) 的中间分布。

JS 散度的特点：
- **对称性**：$D_{JS}(P \| Q) = D_{JS}(Q \| P) $，这是相对于 KL 散度的一大优点。
- **有限值**：JS 散度是有限的，且其值在 \( [0, 1] \) 之间。KL 散度在某些情况下会趋向无穷大，而 JS 散度则受限于 1。
- **平滑**：通过在两个分布之间引入一个中间分布 \( M \)，JS 散度对异常值的敏感性较低。

**直观理解**：

- JS 散度的本质是衡量 \( P \) 和 \( Q \) 之间的平均散度，即两者相对于它们“均值”分布 \( M \) 的距离之和。
- 如果两个分布 \( P \) 和 \( Q \) 完全一致，那么 JS 散度为 0；如果它们完全不同，JS 散度则接近 1。

**应用**：

1. **文本分析与分类**：JS 散度经常用于自然语言处理和文本分析中，来衡量两个文本的词频分布之间的差异。
2. **聚类分析**：在聚类任务中，JS 散度可以用来评估不同簇的分布是否相似。
3. **生成模型评估**：在 GAN（生成对抗网络）中，JS 散度用于衡量生成的分布与真实数据分布之间的差异。



KL 散度与 JS 散度的区别

1. **对称性**：KL 散度不对称，JS 散度对称。

2. **范围**：KL 散度的值可以是正无穷，而 JS 散度的值始终是有限的，且通常在 \( [0, 1] \) 之间。

3. **应用场景**：KL 散度通常用于机器学习中的概率推断和生成模型；JS 散度则更常用于衡量两种分布的相似度，特别是在文本、图像等任务中。

   

   总结

- **KL 散度**：衡量两个概率分布之间的信息差异，主要关注的是信息损失。它对分布不对称，因此 \( D_{KL}(P \| Q) \neq D_{KL}(Q \| P) \)。
- **JS 散度**：是一种对称、平滑、有限的散度度量，适用于衡量两者的对称差异，尤其在生成模型和分类任务中应用广泛。

两者都是衡量概率分布差异的重要工具，根据具体任务的需求，可以选择合适的散度度量。





### 10、脱敏数据



在机器学习中，**脱敏数据**（Data Anonymization or Data Masking）指的是在==数据处理中，通过对敏感信息进行转换或隐藏，以保护个人隐私和敏感数据的过程==。这在处理个人信息、医疗数据、金融数据等涉及隐私保护的领域尤为重要。机器学习中的数据脱敏既要确保数据的隐私性，也要保留数据的有效性，便于模型学习。

1. **脱敏数据的常见方法**

以下是一些常见的脱敏技术，通常在机器学习训练数据集准备阶段应用：

1.1 **数据屏蔽（Data Masking）**

   - 用随机字符或数据替换原有数据中的敏感信息。例如，将身份证号码替换成随机生成的数字，或将用户姓名用字符“X”替代。

   **示例**:
   - 原始数据：`1234-5678-9012`
   - 脱敏后数据：`XXXX-XXXX-XXXX`

1.2 **泛化（Generalization）**

   - 将数据细节缩小到更广泛的类别。例如，将精确的年龄用年龄段代替，或将具体的地址用城市名代替。

   **示例**:
   - 原始数据：`年龄：33岁`
   - 脱敏后数据：`年龄：30-40岁`

1.3 **加噪（Noise Addition）**

   - 向原始数据中添加随机噪声，使得无法轻易还原出敏感信息。这种方法在数值型数据中使用较多，尤其是保护个人收入、消费数据等信息。

   **示例**:
   - 原始数据：`收入：50000美元`
   - 脱敏后数据：`收入：49000美元`

1.4 **哈希处理（Hashing）**

   - 使用哈希函数将原始数据转换为固定长度的散列值，常用于脱敏文本数据，如邮箱地址、用户名等。由于哈希值不可逆，原始数据无法通过哈希值恢复。

   **示例**:
   - 原始数据：`用户ID：john_doe`
   - 脱敏后数据：`用户ID：2a7d0b1...`



1.5 **数据删除（Data Suppression）**

   - 完全删除数据集中的敏感字段或记录。这种方法可能导致模型性能下降，因为删除的数据可能对模型的学习有帮助。

   **示例**:
   - 删除患者医疗数据中的姓名和地址字段，仅保留病历和诊断信息。



### 11、word2vec   FastText



**Word2Vec** 和 **FastText** 是两种常用的==词向量==（word embeddings）生成技术，广泛应用于自然语言处理（NLP）任务中。虽然它们都是基于神经网络的模型，并且旨在将词语映射为稠密的向量空间表示，但它们在生成词向量的具体方法上存在一些重要差异。

**Word2Vec**

**Word2Vec** 是由 Google 的 Tomas Mikolov 团队在 2013 年提出的模型。它通过使用一个浅层神经网络来学习词向量。主要有两种训练方法：**CBOW**（Continuous Bag of Words）和 **Skip-gram**



核心思想：

- **CBOW（连续词袋模型）**：通过上下文词预测中心词。例如，给定句子中的上下文（周围的词），模型将预测中心的词。
- **Skip-gram**：通过中心词来预测周围的上下文词。此方法在训练中表现得更好，尤其是在小数据集和稀疏数据场景中。



特点：

- **静态词向量**：**Word2Vec 为每个单词生成一个固定的向量，无论它出现在什么上下文中，词向量保持不变。这意味着每个单词在不同的上下文中具有相同的向量表示**。
- **捕捉语义关系**：通过 Word2Vec，生成的词向量能够捕捉到一些语义关系。例如，`king - man + woman ≈ queen`。



优点：

- 计算高效，训练快。
- 能够捕捉词与词之间的语义关系和距离，且效果良好。
  



局限：

- **OOV（Out of Vocabulary）问题**：==对训练集中没有出现过的单词（未知词）无法生成词向量==。
- **不考虑词内部结构**：Word2Vec 将每个词看作一个独立的整体，而不考虑词的内部结构（如前缀、后缀等）。

2. **FastText**

**FastText** 是由 Facebook AI 研究团队提出的一种改进方法，它在 Word2Vec 的基础上进行了扩展，特别是在处理形态丰富的语言时表现更好。



核心思想：

- **子词级别建模**：FastText 并不是直接为每个单词生成一个向量，而是将单词分解为若干个子词（n-gram），然后通过子词的组合来表示词的向量。例如，单词 "apple" 可能会分成 `app`, `ple`, `apple` 等 n-gram，通过这些 n-gram 的组合生成单词的词向量。



特点：

- **动态词向量**：FastText 的词向量是通过子词组合得到的，因此可以更好地处理新词（OOV），尤其是在拼写错误或形态变化时，FastText 可以生成这些词的词向量。
- **更好地处理形态复杂语言**：对于形态变化较多的语言（如俄语、阿拉伯语等），FastText 的效果通常优于 Word2Vec。

优点：

- **处理 OOV 单词**：因为模型是基于子词的，所以即使是训练集中没有见过的词（新词），FastText 也能通过它的子词来生成向量。
- **处理拼写错误**：模型能有效处理拼写错误，因其基于子词，因此拼写错误往往只会影响部分 n-gram。

局限：

- **计算开销稍大**：因为它需要将词分解为子词来进行计算，FastText 的计算复杂度比 Word2Vec 稍高。
- **对于一些简化任务可能是过度设计**：在一些只需要处理简单词汇的场景下，FastText 的子词分解可能没有太大必要。

**比较：Word2Vec vs FastText**

| **特性**             | **Word2Vec**               | **FastText**                     |
| -------------------- | -------------------------- | -------------------------------- |
| **训练方式**         | 基于完整的单词             | 基于子词（n-gram）               |
| **OOV 问题**         | 无法处理                   | 能处理新词及拼写错误             |
| **词向量生成**       | 为每个词生成单一固定向量   | 通过子词组合生成词向量           |
| **处理形态丰富语言** | 表现一般                   | 表现优异，特别是形态变化多的语言 |
| **计算复杂度**       | 较低，训练速度较快         | 略高，计算更多                   |
| **模型语义性**       | 能捕捉词与词之间的语义关系 | 除了捕捉语义关系，还能处理词形态 |

**应用场景**

- **Word2Vec 应用场景**：适用于大多数标准的 NLP 任务，如文本分类、情感分析、信息检索等。由于其效率较高，适合处理规模较大的语料库。
  
- **FastText 应用场景**：FastText 更适合处理语言形态复杂的任务或新词较多的情况，如处理社交媒体评论（存在大量新词或拼写错误）以及形态复杂语言的 NLP 任务。

 **总结**

- **Word2Vec** 强调词的语义关系，在大多数 NLP 任务中表现良好，但在处理未见过的单词时能力有限。
- **FastText** 通过引入子词级别的信息，能够有效处理词形变化、新词以及拼写错误等情况，因此在处理形态复杂语言或噪声数据时表现更好。

如果你正在开发某个项目，需要选择使用哪种方法，可以根据你的具体应用场景决定：如果你的数据集中存在大量的新词或拼写错误，FastText 可能是更好的选择；否则，Word2Vec 的效率和表现通常已经足够好。



### 12、微调和下游任务

 **微调（Fine-tuning）** 是机器学习中的一种常见策略，特别是在深度学习和预训练模型中。==它的核心思想是在一个已经训练好的模型上，针对一个特定的下游任务进行再训练，以提高模型在该任务上的性能==。



背景

预训练-微调的方式在现代深度学习中非常流行，尤其是在大规模模型和数据集（如 BERT、GPT、ResNet 等）中。通过在大规模数据集上进行预训练，模型能够学习到通用的特征，然后再通过微调，将这些特征应用到特定任务上。

- **预训练（Pre-training）**：模型在一个大型数据集（如 ImageNet、Wikipedia 文本数据）上训练，学习到通用的表示。这可以是无监督或自监督的方式。
- **微调（Fine-tuning）**：在预训练模型的基础上，使用一个较小的、特定领域的数据集对模型进行进一步训练，使其在该任务上表现得更好。

2. **微调的步骤**

微调通常涉及以下几个步骤：



2.1 **选择预训练模型**

首先，需要选择一个预训练的模型，该模型通常已经在大量数据上训练过，并且具有通用的特征。比如：
- 在自然语言处理（NLP）任务中，常用的预训练模型有 BERT、GPT、T5 等。
- 在计算机视觉任务中，常用的预训练模型包括 ResNet、VGG、EfficientNet 等。



2.2 **冻结部分层或全部层**

为了避免对整个模型进行重新训练，通常会**冻结模型的部分层**，特别是低层的特征提取层。这些层通常包含通用的特征，而高层则更加与特定任务相关。
- **冻结层**：在微调过程中，冻结的层不会更新权重，只有高层或最后几层会更新，以适应特定任务。
- **解冻层**：有时在微调后期，可以逐渐解冻更多层，使整个模型更加适应特定任务。



2.3 **添加任务特定的层**

在预训练模型的基础上，通常会添加一些特定任务的层。例如：
- 在 NLP 任务中，可能会添加分类层用于情感分析或文本分类。
- 在计算机视觉中，可能会添加额外的卷积层或全连接层用于特定的图像分类任务。



2.4 **调整超参数**

微调时的学习率通常比预训练阶段要低，因为模型已经接近收敛。需要根据数据集的规模和任务的复杂性，调整一些超参数，如学习率、批量大小、迭代次数等。



2.5 **训练和验证**

在进行微调时，模型会在目标任务的数据集上进行训练。通常，训练会包括验证步骤，以确保模型不会过拟合并能在实际任务上表现良好。



3. **微调的优势**

- **高效利用数据**：通过使用预训练模型，可以在相对较少的数据上训练出高性能的模型，尤其是在小数据集或标注数据不足的情况下，效果尤为显著。
- **减少计算资源**：预训练模型已经学习到了许多通用特征，因此微调只需要训练少量参数，这比从头训练一个全新的模型要快得多，计算资源消耗也更少。
- **提升模型性能**：预训练的模型已经在大规模数据上学习了通用的表示，微调可以快速提高模型在特定任务上的表现



==**下游任务（Downstream Tasks）** 是机器学习和自然语言处理（NLP）领域中的一个常见术语，指的是在**预训练阶段之后，模型被应用于具体任务或应用场景的任务**。==





### 13、自编码器



**自编码器（Autoencoder）** 是一种用于==无监督学习==的神经网络模型，通常用于数据的特征学习、降维、去噪、生成数据等任务。自编码器的基本思想是将输入数据压缩成低维表示（编码过程），然后再将其重建为原始输入（解码过程），并通过最小化重建误差来训练网络。与传统的降维方法（如PCA）不同，自编码器能够通过非线性函数捕捉数据中的复杂结构。





**自编码器的基本结构**

自编码器由两部分组成：
- **编码器（Encoder）**：将高维的输入数据映射到低维的潜在空间表示（latent representation），也称为**隐变量**或**瓶颈层**。编码器通常是一个多层神经网络，压缩输入数据。
- **解码器（Decoder）**：将低维的潜在表示映射回原始的输入空间，==尝试重建输入数据==。解码器也是一个多层神经网络，功能是从压缩的表示中恢复信息。

整个自编码器的目标是尽可能准确地重建输入数据，最小化输入数据和输出数据之间的差异（通常使用均方误差作为损失函数）。

自编码器的结构图：

```
Input (x)  → [ Encoder ] → Latent Space (z) → [ Decoder ] → Reconstructed Output (x')
```



**自编码器的工作原理**

训练自编码器时，网络通过前向传播将输入数据转换为潜在空间的低维表示，并通过反向传播更新网络参数以最小化重建误差。通过学习过程中，编码器能够提取出输入数据的有效特征，而解码器则试图利用这些特征重构原始数据。



**总结**

自编码器是一类强大的无监督学习模型，能够通过压缩和重建数据来提取有意义的特征，并在降维、特征学习、去噪、生成数据等任务中得到了广泛应用。自编码器的不同变体可以应对不同的任务需求，如卷积自编码器用于图像处理，去噪自编码器用于去噪，变分自编码器则具备生成新样本的能力。尽管自编码器有其局限性，但它为无监督学习和生成模型提供了强有力的工具。



在机器学习中，**logits** 是指在分类模型（特别是神经网络和逻辑回归模型）的输出层中，未经过激活函数（如 sigmoid 或 softmax 函数）处理的**未归一化分数**或**原始预测值**。logits 是模型对各个类别的直接预测，通常是线性变换的结果，在经过激活函数之前，它们还不是有效的概率值。

**Logits 的定义**

logits 是指模型最后一层的线性输出。例如，在神经网络的分类模型中，最后一层通常是一个全连接层，其输出为 logits。假设模型的最后一层线性输出为 \( z \)，则：


$z = w^T x + b $

其中：
- \( w \) 是权重向量，
- \( x \) 是输入特征向量，
- \( b \) 是偏置项。

这些 \( z \) 值就是 logits，通常表示每个类别的**得分**或**置信度**，但它们并不直接代表概率。



**Logits 与激活函数**

logits 是模型最后一层的输出值，之后需要经过激活函数转换为概率。常用的激活函数有：
- **Sigmoid 函数**：用于二分类问题，将 logit 转化为一个介于 0 和 1 之间的概率。
  
  $
  \sigma(z) = \frac{1}{1 + e^{-z}}
  $
  
  这里 \( z \) 是 logit 值，\(\sigma(z)\) 就是模型对正类的预测概率。

- **Softmax 函数**：用于多分类问题，将 logits 转换为各类别的概率分布。

  $
  P(y=i | x) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
  $

  其中，\( z_i \) 是类别 \( i \) 的 logit 值，softmax 函数会将所有类别的 logits 转换为概率，且所有类别的概率之和为 1。



**Logits 在分类模型中的作用**

在分类问题中，模型通过 logits 来进行预测。以下是 logits 在不同类型的分类问题中的作用：

3.1 **二分类问题**

在二分类任务中，模型通常输出一个 logit 值 \( z \)，表示正类的预测分数。这个 logit 经过 sigmoid 函数后，转换为介于 0 和 1 之间的概率，代表正类的发生概率。通常会设置一个阈值（如 0.5），来将该概率转化为最终的分类标签：

$
P(y = 1 | x) = \frac{1}{1 + e^{-z}}
$

3.2 **多分类问题**

对于多分类问题，模型输出一组 logits，每个类别一个 logit 值。这些 logits 经过 softmax 函数处理后，转化为类别的概率分布，表示模型对每个类别的预测概率：

$
P(y = i | x) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$

通过选择概率最大的类别，模型可以给出最终的预测结果。

4. **Logits 与损失函数**

logits 在计算损失函数时也扮演着重要角色。常见的损失函数与 logits 的关系如下：



4.1 **交叉熵损失（Cross-Entropy Loss）**

交叉熵损失是分类问题中的常用损失函数，尤其适用于与 sigmoid 或 softmax 激活函数配合使用。它衡量模型预测的概率分布与真实分布之间的差异。

- **二分类交叉熵损失**：用于二分类问题，结合 sigmoid 函数计算。

  $
  \text{Loss} = - \left( y \log(\sigma(z)) + (1 - y) \log(1 - \sigma(z)) \right)
  $

  其中，\( z \) 是 logit 值，\( \sigma(z) \) 是通过 sigmoid 函数转换后的概率。

- **多分类交叉熵损失**：用于多分类问题，结合 softmax 函数计算。

  $
  \text{Loss} = - \sum_{i=1}^{C} y_i \log(P(y=i | x))
  $

  其中，\( y_i \) 是真实标签的 one-hot 编码，\( P(y=i | x) \) 是通过 softmax 函数转换后的类别概率。



4.2 **损失函数与 logits 的直接计算**

在计算交叉熵损失时，有时可以直接使用 logits，而不需要先通过 softmax 或 sigmoid 转换概率。这种方法称为 **logits 版本的交叉熵损失**，它在数值稳定性和计算效率上更有优势。比如，在 PyTorch 和 TensorFlow 等深度学习框架中，常用的损失函数有 `BCEWithLogitsLoss`（用于二分类）和 `CrossEntropyLoss`（用于多分类），这些函数直接接受 logits 作为输入，并在内部自动应用 sigmoid 或 softmax 处理。



5. **Logits 的数值稳定性问题**

在计算 softmax 和交叉熵损失时，直接使用 logits 可能会遇到数值不稳定性问题。具体来说，logits 可能会出现非常大的正值或负值，导致在计算 softmax 或 sigmoid 时产生溢出。因此，框架通常会在内部进行数值稳定化处理。

- **Softmax 的数值稳定性**：在计算 softmax 时，通常会从每个 logit 中减去最大 logit 值，这不会改变 softmax 输出的相对概率，但可以防止数值溢出。

  $
  \text{Softmax}(\mathbf{z}) = \frac{e^{z_i - \max(\mathbf{z})}}{\sum_{j} e^{z_j - \max(\mathbf{z})}}
  $

6. **Logits 的解释性**

logits 值可以视为模型对每个类别的**未归一化分数**。它们本质上是通过输入特征的线性组合（加权求和）得到的结果，表示模型对某一类别的置信度。虽然 logits 本身不代表概率，但它们在相对大小上反映了模型的预测倾向。

例如：
- **logit 值较大**，表示模型认为该类别更有可能是正确的分类结果。
- **logit 值较小甚至负值**，表示模型认为该类别的可能性较低。

7. **Logits 的总结**

- **Logits** 是机器学习分类模型最后一层的线性输出，表示未经过激活函数处理的分类得分。
- 在二分类问题中，logit 通常通过 sigmoid 函数转换为概率；在多分类问题中，logits 通常通过 softmax 函数转换为类别概率分布。
- Logits 在计算损失函数时至关重要，交叉熵损失函数可以直接接受 logits 输入，并在内部进行数值处理。
- 对 logits 的合理处理有助于提高模型的数值稳定性，并为最终的分类结果提供可靠的基础。



### 14、Ground truth

- 在机器学习和计算机视觉等领域，**ground truth**（中文通常称为“**真实标签**”或“**真实数据**”）是指对模型训练和评估时所使用的**真实、精确的参考答案**。它通常代表对数据的客观、可靠的标注或测量，作为模型预测结果的基准。

## 一、AlexNet  阅读

> 文章链接 [ImageNet classification with deep convolutional neural networks (acm.org)](https://dl.acm.org/doi/pdf/10.1145/3065386)

### 1、introduction

**第一段**

一篇论文的第一段通常是讲个故事

- ==做什么研究==
- 哪个方向
- 这个方向有什么东西
- ==为什么很重要==



**第二段**

- 描述了怎么做神经网络，这里只介绍了CNN

- 写论文的时候，<font color=red>千万不要只说自己这个领域这个小方向大概怎么样，还要提到别的方向怎么样</font>



**第三段**

CNN虽然很好，但是很难训练，但是现在有GPU了，GPU算力能够跟上，所以能够训练很大的东西，**而且数据集够大，确实能够训练比较大的CNN**



前三段基本描述了

- 我做了什么东西
- 为什么能做



**第四段**

paper的贡献

- 训练了一个最大的的神经网络，然后取得了特别好的结果
- 实现了GPU上性能很高的一个2D的卷积
- 网络有一些新的和不常见的一些特性，能够提升性能，降低模型的训练时间
- 使用了什么过拟合的方法使得模型变得更好
- 模型具有5个卷积层，3个全连接层，发现深度很重要，移掉任何一层都不行



结果很好，但是还是有新东西在里面的，如果就结果很好，没有新东西，大概是不会称为奠基作



### 2、the dataset



大概描述了一下所用的数据集



重点是最后一段：ImageNet中图片的分辨率是不一样的，因此将每张图片变成了一个256*256的图片：

- **将图片的短边减少到256，长边是保证高宽比不变的情况下也往下降**，长边如果依然多出来的话，如果多于256的话，<font color=red>就以中心为界将两边裁掉</font>，裁成一个256*256的图片
- 没有做任何的预处理，只是对图片进行了裁剪
- 网络是在raw RGB Value上训练的
- 当时做计算机视觉都是将特征抽出来，抽SIFT也好，抽别的特征也好（imagenet数据集也提供了一个==SIFT==版本的特征），这篇文章说不要抽特征，直接是在原始的Pixels上做了
- <font color=blue>在之后的工作里面基本上主要是**end to end（端到端）**：及那个原始的图片或者文本直接进去，不做任何的特征提取，神经网络能够帮助你完成这部分工作</font>





### 3、the architecture



讲整个网络的架构

- relu非线性激活函数
- 使用了多GPU进行训练
- 正则化、归一化
- overlapping pooling
- 总体架构





![img](https://i0.hdslb.com/bfs/note/fec00a250446e40b26248c49b4e86e1d215d562b.png@630w_!web-note.webp)

- 方框表示每一层的输入和输出的数据的大小
- 输入的图片是一个高宽分别为224*224的3通道RGB图片
- **第一层卷积：卷积的窗口是11*11，有48个输出通道，stride等于4**
- 有两个GPU，GPU1和GPU0都有自己的卷积核参数

![img](https://i0.hdslb.com/bfs/note/787fdba5c593948017ec66d0bc130dc6187dcac3.png@630w_!web-note.webp)

- 第一个卷积层在两个GPU上各有一个
- 第二个卷积层是在每个GPU把当前的卷积结果拿过来（GPU0的第二个卷积层读的是GPU0的第一个卷积层的卷积结果，GPU0和GPU1之间没有任何通讯）
- 到第三个卷积层的时候，GPU还是每个GPU上有自己的卷积核，但是每个卷积核会同时将第二个卷积层中GPU0和GPU1的卷积结果作为输入，两个GPU之间会通讯一次
- 第4、5个卷积层之间没有任何通讯
- 每个卷积层的通道数是不一样的，通道数有所增加，高和宽也有所变化
- 高宽慢慢变小、深度慢慢增加，随着深度的增加，慢慢地将空间信息压缩，直到最后每一个像素能够代表前面一大块的像素，然后再将通道数慢慢增加，可以认为每个通道数是去看一种特定的模式（例如192个通道可以简单地认为，能够识别图片中的192种不同的模式）
- 慢慢将空间信息压缩，语义空间慢慢增加，到最后卷积完之后，进入全连接层
- 全连接层中又出现了GPU之间的通讯，全连接层的输入是每个GPU第五个卷积的输出合并起来做全连接

![img](https://i0.hdslb.com/bfs/note/ccc807a47db1ad370b23c9552cc75f226c50f89e.png@630w_!web-note.webp)

- 最后进入分类层的时候，变成了一个4096长的向量，每一块来自两个GPU，每片是2048，最后拼起来，所以一张图片会表示成一个4096维的向量，最后用一个线性分类做链接
- 深度学习的主要作用是将一张输入的图片，通过**卷积、池化、全连接**等一系列操作，将他压缩成一个长为4096的向量，这个向量能够将中间的语义信息都表示出来（将一个人能够看懂的像素通过一系列的特征提取变成了一个长为4096的机器能够看懂的东西，这个东西可以用来做搜索、分类等）
- 整个机器学习都可以认为是一个知识的压缩过程，不管是图片、语音还是文字或者视频，通过一个模型最后压缩成一个向量，然后机器去识别这个向量，然后在上面做各种事情
- 模型并行（model parallel）：现在在计算机视觉里面用的不多，但是在自然语言处理方面又成为主流了（将模型切开进行训练）



### 4、reducing overfitting



第四章讲述了如何降低过拟合



数据增强（data augmentation）

- 把一些图片人工地变大
- 在图片中随机地抠出一部分区域，做一张新的图片
- 把整个RGB的颜色通道channel上做一些改变，这里使用的是一个**PCA（主成分分析）**的方法，颜色会有不同，因此每次图片跟原始图片是有一定的不同的





dropout

- 随机的把一些隐藏层的输出变成用50%的概率设为0，每一次都是把一些东西设置为0，所以模型也就发生了变化，每次得到一个新的模型，但是这些模型之间权重是共享的除了设置成0的，非0的东西都是一样的，这样就等价于做了模型融合
- 后来大家发现dropout其实也不是在做模型融合，==更多的dropout就是一个正则项（dropout在现行模型上等价于一个L2正则项）==
- 这里将dropout用在了前面的两个全连接层上面
- 文章说没有dropout的话，overfitting会非常严重，有dropout的话，训练会比别人慢两倍
- 现在CNN的设计通常不会使用那么大的全连接层，所以dropout也不那么重要，而且GPU、内存也没那么吃紧了
- dropout在全连接上还是很有用的，在RNN和Attension中使用的非常多



### 5、details of learning



讲述了模型是如何训练的

- 使用SGD（随机梯度下降）来进行训练，SGD调参相对来说可能会比较难调，后来发现SGD里面的噪音对模型的泛化性其实是有好处的，所以现在深度学习中普遍使用SGD对模型进行训练。在这个文章之后SGD基本上在机器学习界成为了最主流的一个优化算法
- 批量大小是128
- momentum是0.9
- weight decay是0.0005，<font color=red>也就是L2正则项，但是这个东西不是加在模型上，而是加在优化算法上</font>，虽然他们两个是等价关系，但是因为深度学习的学习，所以大家现在基本上把这个东西叫做weight decay了
- momentum也是因为这篇文章之后用的特别多，虽然在2010年的时候有大量的加速算法，里面有很fancy的各种加速SGD算法，但是现在看起来似乎用一个简单的momentum也是不错的
- momentum实际上是，当优化的表面非常不平滑的时候，冲量使得不要被当下的梯度过多的误导，可以保持一个冲量从过去那个方向，沿着一个比较平缓的方向往前走，这样子不容易陷入到局部最优解
- 权重用的是一个均值为0，方差为0.01的高斯随机变量来初始化（0.01对很多网络都是可以的，但是如果特别深的时候需要更多优化，但是对于一些相对简单的神经网络，0.01是一个不错的选项）
- 现在就算是比较大的那些BERT，也就是用了0.02作为随机的初始值的方差
- 在第二层、第四层和第五层的卷积层把初始的偏移量初始化成1，剩下的全部初始化成0
- 每个层使用同样的学习率，从0.01开始，然后呢如果验证误差不往下降了，就手动的将他乘以0.1，就是降低十倍
- ResNet中，每训练120轮，学习率每30轮就下降0.1另外一种主流的做法就是，前面可以做得更长一点，必须能够60轮或者是100轮，然后再在后面下降
- 在Alex之后的很多训练里面，都是做规则性地将学习率往下下降十倍，这是一个非常主流的做法，但是现在很少用了，现在使用更加平滑的曲线来降低学习率，比如果用一个cos的函数比较平缓地往下降。一开始的选择也很重要，如果选的太大可能会发生爆炸，如果太小又有可能训练不动，所以现在主流的做法是学习率从0开始再慢慢上升，慢慢下降

![img](https://i0.hdslb.com/bfs/note/c490536f8d63015d57bf2564fd9e249d6e3f3aa8.png@630w_!web-note.webp)

- 模型训练了90个epoch，然后每一遍用的是ImageNet完整的120万张图片，需要5-6天在两个GTX GPU上训练





### 6、result



- 有时候结果可能不重要

- 有些东西可能还不是很理解，可以去看文章所引用的文章





## 二、ResNet 阅读

- xavier初始化

### 1、introduction

- 深度神经网络好在可以加很多层把网络变得特别深，==然后不同程度的层会得到不同等级的feature，比如低级的视觉特征或者是高级的语义特征==



提出问题：随着网络越来越深，<font color=red>梯度就会出现爆炸或者消失</font>

- 解决他的办法就是：1、*在初始化的时候要做好一点，就是权重在随机初始化的时候，权重不要特别大也不要特别小*。2、在中间加入一些normalization，包括BN（batch normalization）可以使得校验每个层之间的那些输出和他的梯度的均值和方差相对来说比较深的网络是可以训练的，避免有一些层特别大，有一些层特别小。使用了这些技术之后是能够训练（能够收敛），虽然现在能够收敛了，但是当网络变深的时候，性能其实是变差的（精度会变差）
- 文章提出出现精度变差的问题不是因为层数变多了，模型变复杂了导致的过拟合，而是因为训练误差也变高了（overfitting是说训练误差变得很低，但是测试误差变得很高），训练误差和测试误差都变高了，所以他不是overfitting。虽然网络是收敛的，但是好像没有训练出一个好的结果





深入讲述了**深度增加了之后精度也会变差** 

- 考虑一个比较浅一点的网络和他对应的比较深的版本（在浅的网络中再多加一些层进去），如果浅的网络效果还不错的话，深的网络是不应该变差的：深的网络新加的那些层，总是可以把这些层学习的变成一个**identity mapping**（输入是x，输出也是x，等价于可以把一些权重学成比如说简单的n分之一，是的输入和输出是一一对应的），但是实际情况是，虽然理论上权重是可以学习成这样，但是实际上做不到：假设让SGD去优化，深层学到一个跟那些浅层网络精度比较好的一样的结果，上面的层变成identity（相对于浅层神经网络，深层神经网络中多加的那些层全部变成identity），这样的话精度不应该会变差，应该是跟浅层神经网络是一样的，但是实际上SGD找不到这种最优解
- 这篇文章提出显式地构造出一个identity mapping，使得深层的神经网络不会变的比相对较浅的神经网络更差，它将其称为deep residual learning framework
- 要学的东西叫做H（x），假设现在已经有了一个浅的神经网络，他的输出是x，然后要在这个浅的神经网络上面再新加一些层，让它变得更深。新加的那些层不要直接去学H（x），而是应该去学H（x）-x，x是原始的浅层神经网络已经学到的一些东西，新加的层不要重新去学习，而是去学习学到的东西和真实的东西之间的残差，最后整个神经网络的输出等价于浅层神经网络的输出x和新加的神经网络学习残差的输出之和，将优化目标从H（x）转变成为了H（x）-x

![img](https://i0.hdslb.com/bfs/note/0a418ebf24535ae9494157b84c95460d67c4f11a.png@706w_!web-note.webp)

- 上图中最下面的红色方框表示所要学习的H（x）
- 蓝色方框表示原始的浅层神经网络
- 红色阴影方框表示新加的层
- o表示最终整个神经网络的输出
- 这样的好处是：只是加了一个东西进来，没有任何可以学的参数，不会增加任何的模型复杂度，也不会使计算变得更加复杂，而且这个网络跟之前一样，也是可以训练的，没有任何改变





非常深的residual nets非常容易优化，但是如果不添加残差连接的话，效果就会很差。越深的网络，精度就越高



<font color=red>introduction是摘要的扩充版本，也是对整个工作比较完整的描述</font>



### 2、related work



一篇文章要成为经典，不见得一定要提出原创性的东西，很可能就是把之前的一些东西很巧妙的放在一起，能解决一个现在大家比较关心难的问题





残差连接如何处理输入和输出的形状是不同的情况：

- 第一个方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加
- 第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做1*1的卷积层，这个卷积层的特点是在空间维度上不做任何东西，主要是在通道维度上做改变。所以只要选取一个1*1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上





### 3、Deep Residual Learning

implementation中讲了实验的一些细节

- 把短边随机的采样到256和480（AlexNet是直接将短边变成256，而这里是随机的）。随机放的比较大的好处是做随机切割，切割成224*224的时候，随机性会更多一点
- 将每一个pixel的均值都减掉了
- 使用了颜色的增强（AlexNet上用的是PCA，现在我们所使用的是比较简单的RGB上面的，调节各个地方的亮度、饱和度等）
- 使用了BN（batch normalization）
- 所有的权重全部是跟另外一个paper中的一样（作者自己的另外一篇文章）。注意写论文的时候，尽量能够让别人不要去查找别的文献就能够知道你所做的事情
- 批量大小是56，学习率是0.1，然后每一次当错误率比较平的时候除以10
- 模型训练了60*10^4个批量。建议最好不要写这种iteration，因为他跟批量大小是相关的，如果变了一个批量大小，他就会发生改变，所以现在一般会说迭代了多少遍数据，相对来说稳定一点
- 这里没有使用dropout，因为没有全连接层，所以dropout没有太大作用
- 在测试的时候使用了标准的10个crop testing（给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后再每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。
- 采样的时候是在不同的分辨率上去做采样，这样在测试的时候做的工作量比较多，但是在实际过程中使用比较少







### 4、experiments



- 如何评估ImagNet
- 各个不同版本的ResNet是如何设计的

首先阐述了ImageNet

描述了plain networks

没有带残差的时候，使用了一个18层和34层

![img](https://i0.hdslb.com/bfs/note/bcecbccc1aeaeb5620412f501396378706cbd175.png@686w_!web-note.webp)

- 上表是整个ResNet不同架构之间的构成信息（5个版本）
- 第一个7*7的卷积是一样的
- 接下来的pooling层也是一样的
- 最后的全连接层也是一样的（最后是一个全局的pooling然后再加一个1000的全连接层做输出）
- 不同的架构之间，主要是中间部分不一样，也就是那些复制的卷积层是不同的
- conv2.x：x表示里面有很多不同的层（块）
- 【3*3,64】:46是通道数
- 模型的结构为什么取成表中的结构，论文中并没有细讲，这些超参数是作者自己调出来的，实际上这些参数可以通过一些网络架构的自动选取
- **flops**：整个网络要计算多少个浮点数运算。卷积层的浮点运算等价于输入的高乘以宽乘以通道数乘以输出通道数再乘以核的窗口的高和宽



表1：ImageNet的架构

表1详细列出了用于ImageNet分类任务的不同残差网络（ResNet）的架构。这些架构通过使用不同数量的残差模块来构建不同深度的网络。以下是表1中的主要内容：

- **layer name**: 层的名称，表示网络中不同层次的名称。
- **output size**: 该层输出的特征图尺寸。
- **18-layer, 34-layer, 50-layer, 101-layer, 152-layer**: 这些列分别展示了不同深度的残差网络架构，包括18层、34层、50层、101层和152层。

每个层级包含的详细信息如下：

- **卷积层**: 表示为 `3x3 conv, 64` 意味着这是一个3x3的卷积核，有64个滤波器的卷积层。
- **步幅**: 如 `/2` 表示该层操作会将宽度和高度缩小一半。
- **池化层**: 表示为 `3x3 max pool, stride 2` 表示使用3x3的最大池化，步幅为2。
- **线性层**: 如 `1x1, 256` 表示1x1的卷积核用于改变特征图的深度，这里是将特征图深度从64变为256。
- **平均池化层**: `1x1 average pool` 表示1x1的平均池化层。
- **全连接层**: `fc 1000` 表示1000个神经元的全连接层。

**FLOPs**: 表示每层操作的浮点运算次数，用于衡量计算复杂度。



图4：在ImageNet上的训练情况

图4展示了18层和34层的普通网络（plain networks）与残差网络（ResNet）在ImageNet数据集上的训练和验证误差对比。

- **左侧**: 展示了18层和34层普通网络的训练过程。图中细线表示训练误差，粗线表示验证误差。可以观察到，随着网络深度的增加，34层网络的训练误差和验证误差都比18层网络要高，这表明普通网络在增加深度时会遇到优化难题。

- **右侧**: 展示了18层和34层残差网络的训练过程。同样，细线表示训练误差，粗线表示验证误差。与普通网络相反，34层残差网络不仅训练误差比18层残差网络要低，而且验证误差也更低。这说明残差网络能够通过增加深度来提高准确率，并且解决了普通网络遇到的优化难题。

总结

表1和图4共同展示了残差网络架构的细节以及它们在ImageNet数据集上的性能表现。通过引入残差学习，可以有效地训练更深的网络，并在图像识别任务中取得更好的性能。



![img](https://i0.hdslb.com/bfs/note/316285db5ae7f1c4dcab40937540681a5b0a7f04.png@686w_!web-note.webp)

- 上图中比较了18层和34层在==有残差连接和没有残差连接的结果==
- 左图中，红色曲线表示34的验证精度（或者说是测试精度）
- 左图中，粉色曲线表示的是34的训练精度
- 一开始训练精度是要比测试精度高的，因为在一开始的时候使用了大量的数据增强，使得寻来你误差相对来说是比较大的，而在测试的时候没有做数据增强，噪音比较低，所以一开始的测试误差是比较低的
- 图中曲线的数值部分是由于学习率的下降，每一次乘以0.1，对整个曲线来说下降就比较明显。为什么现在不使用乘0.1这种方法：在什么时候乘时机不好掌控，如果乘的太早，会后期收敛无力，晚一点乘的话，一开始找的方向更准一点，对后期来说是比较好的
- 上图主要是想说明在有残差连接的时候，34比28要好；另外对于34来说，有残差连接会好很多；其次，有了残差连接以后，收敛速度会快很多，核心思想是说，在所有的超参数都一定的情况下，有残差的连接收敛会快，而且后期会好





<font color=red>输入输出形状不一样的时候怎样做残差连接</font>

- 填零
- 投影
- 所有的连接都做投影：就算输入输出的形状是一样的，一样可以在连接的时候做个1*1的卷积，但是输入和输出通道数是一样的，做一次投影



对比以上三种方案

![img](https://i0.hdslb.com/bfs/note/8c75c28d1b2ec0943678d40962aa0c5101d4fd4f.png@686w_!web-note.webp)

- A表示填0
- B表示在不同的时候做投影
- C表示全部做投影
- B和C的表现差不多，但是还是要比A好一点
- B和C虽然差不多，但是计算复杂度更高，B对计算量的增加比较少，作者采用了B





怎样构建更深的ResNet

如果要做50或者50层以上的，会引入bottleneck design

![img](https://i0.hdslb.com/bfs/note/9a6493ed10b6a1cb12739837870d10c86c030419.png@686w_!web-note.webp)

- 左图是之前的设计，当通道数是64位的时候，通道数不会发生改变

- 如果要做到比较深的话，可以学到更多的模式，可以把通道数变得更大，右图从64变到了256

- 当通道数变得更大的时候，计算复杂度成平方关系增加，这里通过1个1*1的卷积，将256维投影回到64维，然后再做通道数不变的卷积，然后再投影回256（将输入和输出的通道数进行匹配，便于进行对比）。等价于先对特征维度降一次维，在降一次维的上面再做一个空间上的东西，然后再投影回去

- 虽然通道数是之前的4倍，但是在这种设计之下，二者的算法复杂度是差不多的

  32:25

  

>每一个卷积核的通道数增加到四倍，为了保持**输出通道数**不变，卷积核数量也变为四倍，一共十六倍
>
>ImageNet标号的错误率本来挺高的，估计有1%



CIFAR-10是一个很小的数据集，跑起来相对来说比较容易，32*32，五万个样本，10类的数据集





在整个残差连接，如果后面新加上的层不能让模型变得更好的时候，因为有残差连接的存在，新加的那些层应该是不会学到任何东西，应该都是靠近0的，这样就等价于就算是训练了1000层的ResNet，但是可能就前100层有用，后面的900层基本上因为没有什么东西可以学的，基本上就不会动了





这篇文章没有结论





==**mAP**：目标检测上最常见的一个精度，锚框的平均精度，越高越好==





**为什么ResNet训练起来比较快？**

- 一方面是因为梯度上保持的比较好，新加一些层的话，加的层越多，梯度的乘法就越多，因为梯度比较小，一般是在0附近的高斯分布，所以就会导致在很深的时候就会比较小（梯度消失）。虽然batch normalization或者其他东西能够对这种状况进行改善，但是实际上相对来说还是比较小，但是如果加了一个ResNet的话，它的好处就是在原有的基础上加上了浅层网络的梯度，深层的网络梯度很小没有关系，浅层网络可以进行训练，变成了加法，一个小的数加上一个大的数，相对来说梯度还是会比较大的。也就是说，不管后面新加的层数有多少，前面浅层网络的梯度始终是有用的，这就是从误差反向传播的角度来解释为什么训练的比较快

![img](https://i0.hdslb.com/bfs/note/9793eefee9774a679bd59f35543ba79ce40cd3b5.png@686w_!web-note.webp)

- 在CIFAR上面加到了1000层以上，没有做任何特别的regularization，然后效果很好，overfitting有一点点但是不大。SGD收敛是没有意义的，SGD的收敛就是训练不动了，收敛是最好收敛在比较好的地方。做深的时候，用简单的机器训练根本就跑不动，根本就不会得到比较好的结果，所以只看收敛的话意义不大，但是在加了残差连接的情况下，因为梯度比较大，所以就没那么容易收敛，所以导致一直能够往前（SGD的精髓就是能够一直能跑的动，如果哪一天跑不动了，梯度没了就完了，就会卡在一个地方出不去了，所以它的精髓就在于需要梯度够大，要一直能够跑，因为有噪音的存在，所以慢慢的他总是会收敛的，所以只要保证梯度一直够大，其实到最后的结果就会比较好）

![img](https://i0.hdslb.com/bfs/note/4f84d817570adb88364b9dad35af6db13710a6a4.png@686w_!web-note.webp)





为什么ResNet在CIFAR-10那么小的数据集上他的过拟合不那么明显？

open question

虽然模型很深，参数很多，但是因为模型是这么构造的，所以使得他内在的模型复杂度其实不是很高，也就是说，很有可能加了残差链接之后，使得模型的复杂度降低了，一旦模型的复杂度降低了，其实过拟合就没那么严重了

- 所谓的模型复杂度降低了不是说不能够表示别的东西了，而是能够找到一个不那么复杂的模型去拟合数据，就如作者所说，不加残差连接的时候，理论上也能够学出一个有一个identity的东西（不要后面的东西），但是实际上做不到，因为没有引导整个网络这么走的话，其实理论上的结果它根本过不去，所以一定是得手动的把这个结果加进去，使得它更容易训练出一个简单的模型来拟合数据的情况下，等价于把模型的复杂度降低了





这篇文章的residual和gradient boosting是不一样的

- gradient boosting是在标号上做residual
- 这篇文章是在feature维度上





## 三、Transformer 阅读

**transformer**



- 最近三年以内深度学习里面最重要的文章之一
- 可以认为是开创了继MLP、CNN、RNN之后的第四大类模型

### 1、标题：Attention is all you need



作者后面打上星号表示同等贡献

![img](https://i0.hdslb.com/bfs/note/308006683fd345e35b1f028664a22a545a1d01c4.png@630w_!web-note.webp)



### 2、摘要



- 在主流的序列转录模型里面，主要是依赖于比较复杂的循环或者是卷积神经网络，一般是使用encoder和decoder的架构

- 序列转录模型：给定一个序列，然后生成另外一个序列，比如机器翻译



在性能最好的模型之中，通常也会在编码器和解码器之间使用注意力机制



这篇文章提出了一个新的简单的架构（simple，之前都倾向于写成novel），这个模型仅仅依赖于注意力机制，而没有用之前的循环或者卷积



做了两个机器翻译的实验，显示这个模型在性能上特别好，可以并行度更好然后使用更少的时间来训练



模型达到了28.4的BLEU

- BLEU score：机器翻译里面常用的一个衡量标准



这篇文章一开始写的时候是针对机器翻译这个小任务写的，但是随着之后BERT、GPT把这个架构用在更多的语言处理的任务上的时候，整个工作就出圈了，最近用在了图片和video上面，几乎什么东西都能用



### 3、导言



==transformer这个模型是第一个仅仅使用注意力机制做序列转录的模型，将之前的循环层全部换成了multi-headed self-attetion==



在机器翻译的这个任务上面，transformer能够训练的比其他的架构都要快很多，而且在实际的结果上确实是效果更好



作者对于这种纯基于注意力机制的模型感到非常激动，作者想要把它用在文本以外的别的数据上面，使得生成不那么时序化也是另外的一个研究方向



这篇文章所有的代码放在tensor2tensor这个库里面，作者将整个代码放在了结论的最后（如果有代码的话，通常会把这个代码放在摘要的最后一句话，因为现在神经网络的文章里面细节通常是比较多的，简简单单的一篇文章很难把所有的细节都讲清楚，所以最好第一时间公布代码，让别人能够很方便地复现文章，然后这样就能够扩大文章的影响力）



### 4、结论



这里的导言写的比较短，基本可以认为是前面摘要的前面一半的扩充



在时序模型里面，2017最常用的是RNN、LSTM、GRU，有两个比较主流的模型：

- 一个叫做语言模型
- 另一个是当输出结构化信息比较多的时候会使用**编码器和解码器架构**



**RNN的特点**

- RNN中给定一个序列，它的计算是把这个序列从左到右一步一步往前做，假设序列是一个句子的话，就是一个一个词，对第t个词会计算出一个输出叫做ht（也叫做它的隐藏状态），ht是由前面一个词的隐藏状态（h（t-1））和当前第t个词本身决定的，这样就可以把前面学到的历史信息通过h（t-1）放到当下，然后和当前的词做一些计算，然后得到输出



**RNN如何有效处理时序信息的关键**：他将之前的信息全部放在隐藏状态里，然后一个一个放下去。他的问题也来自于这里：

1. <font color=red>他是一个一步一步计算（时序）的过程，比较难以并行，在时间上难以并行，使得在计算上性能比较差</font>
2. 历史信息是一步一步地往后传递的，如果时序比较长的话，那么很早期的时序信息在后面的时候可能丢掉，如果不想丢掉的话，==可能需要ht要比较大，如果要做比较大的ht，在每一个时间步都得把他存下来，导致内存开销比较大==。



**attention在RNN上的应用**

- 在这篇文章之前，attention已经成功地用在编码器和解码器里面了，主要是用在怎么样把编码器的东西有效地传给解码器



这篇文章提出来的transformer是一个新的模型，不再使用之前被大家使用的循环神经层，而是纯基于注意力机制，并行度比较高，这样的话它能够在比较短的时间之内做到比之前更好的结果



NeurIPS是一个篇幅比较短的会议，单列八页



### 5、相关工作



如何使用卷积神经网络替换循环神经网络，减少时序的计算



主要问题：用卷积神经网络的话，对于比较长的序列难以建模

- ==卷积做计算的时候每次是去看一个比较小的窗口，如果两个像素相隔比较远的话，需要用很多层卷积堆积起来==，最后才能够把这两个隔得比较远的像素融合起来。但是如果是使用transformer里面的注意力机制的话，每一次都能够看到所有的像素，使用一层就能看到整个序列
- 卷积的好处是可以做多个输出通道，<font color=blue>一个输出通道可以认为是识别不一样的模式，作者也想要实现这种效果，所以提出了一个叫做multi-headed attention（多头注意力机制），用于模拟卷积神经网络多输出通道的一个效果</font>



自注意力机制



memory networks



### 6、模型



序列模型里面比较好的是编码器和解码器的架构

- 编码器：将一个长为n的x1到xn的输入，编码器会把它表示成为一个也是长为n但是其中每一个zt对应xt的向量的表示，这就是编码器的输出，就是将原始的输入变成机器学习可以理解的一系列向量

- 解码器：拿到编码器的输出，然后生成一个长为m的一个序列（n和m是不一样长的，可以一样，也可以不一样）。他和编码器最大的不同之处在于，在解码器中，词是一个一个生成的（**因为对于编码器来讲，很可能是一次性能看全整个句子，但是在解码的时候只能一个一个的生成**），即自回归（auto-regressive）,在这里面输入又是输出，在过去时刻的输出也会作为当前时刻的输入

  18:13

- transformer是使用了编码器和解码器的架构，具体来讲它是将自注意力和point-wise、fully connected layers一个一个堆在一起

![img](https://i0.hdslb.com/bfs/note/0e3d20df953dae975726187ccb0f650bdd0ffa7a.png@630w_!web-note.webp)

- 写论文的话，有一张比较漂亮的能够把整个全局话清楚的图是非常重要的

![img](https://i0.hdslb.com/bfs/note/b0776a09bed44621f07c49727b13b23b5068864a.png@630w_!web-note.webp)

- 上图是一个编码器和解码器的架构

![img](https://i0.hdslb.com/bfs/note/810dcd1555f3667928075a49d562ecb201d75090.png@630w_!web-note.webp)

- 左边圈出来的部分是编码器，右边圈出来的部分是解码器
- 左下角的input是编码器的输入，如果是中文翻英文的话，输入就是中文的句子
- 右下角的outputs是解码器的输入，解码器在做预测的时候是没有输入的，<font color=red>实际上就是解码器在之前时刻的一些输出作为输入</font>
- shifted right就是一个一个往右移
- input embedding：嵌入层。进来的是一个一个的词，需要将它们表示成向量
- poositional encoding：
- Nx：N代表层数

![img](https://i0.hdslb.com/bfs/note/e51891cb1c295c0dd3e694e67e8bf482e2ee8201.png@630w_!web-note.webp)

- 红色圆圈圈出来的部分可以看作*transformer block*
- multi-headed attention
- feed forward：前馈神经网络
- add&norm：**add表示残差的连接**

![img](https://i0.hdslb.com/bfs/note/1a2a1c716c249a06927cef2dd67c068bd2e4ea26.png@630w_!web-note.webp)

- 编码器的输出会作为解码器的输入

![img](https://i0.hdslb.com/bfs/note/4a4253d3db880e386c30502213c33ba0c8e11761.png@630w_!web-note.webp)

- 解码器和编码器有点像，红色圆圈圈出来的部分是一样的，但是多了下面的masked multi-headed attention（多头注意力机制）

![img](https://i0.hdslb.com/bfs/note/7451688c71b3902744ff608362c07c0c4248b62b.png@630w_!web-note.webp)

- 解码器可以认为是红色圆圈中的三部分组成的一个块重复n次
- 解码器的输出进入一个输出层，然后做一个softmax就能得到最终的输出

![img](https://i0.hdslb.com/bfs/note/53b444cad5e6bf92cc4583be019995733d10aac8.png@630w_!web-note.webp)

- 红色方括号所扩起来的部分就是标准的神经网络的做法
- 上图所示的确是一个标准的编码器解码器的架构，只是说中间的每一块有所不同，然后是编码器和解码器之间的连接有所不同





**具体模块的实现**



**编码器**



编码器是用一个n等于6的一个完全一样的层，下图中的红色阴影部分算作一层，然后重复6次

- 每一个layer中会有2个sub-layer
- 第一个sub-layer叫做multi-headed self-attention
- 第二个sub-layer是一个simple position-wise fully connected feed-forward network，说白了就是一个MLP
- 对于每一个子层采用了一个残差连接，最后再使用layer normalization

![img](https://i0.hdslb.com/bfs/note/8135b4566e409c3da99494834984b8b26c2128d5.png@630w_!web-note.webp)

- 子层的公式如下图中黄色高亮部分所示

![img](https://i0.hdslb.com/bfs/note/aecfc2f2f4340d1f4b542272d3b39c32ed3a579e.png@630w_!web-note.webp)

- sublayer（x）：输入进来以后先进入子层
- x+sublayer（x）：因为是残差连接，所以将输入和输出加在一起，最后进入他的layernorm
- 因为残差连接需要输入和输出是一样大小的，如果大小不一样的话，需要做投影，为了简单起见，讲么一个层的输出维度变成512（固定长度表示是的模型相对来说是比较简单的，调参的话只需要调一个参数就行了，就是模型的输出维度，另外一个参数是要复制多少块n）





### batch norm对比layer norm

- 与 Batch Normalization 不同的是，Layer Normalization 是<font color=red>在单个样本的维度上进行归一化的，因此更加适合处理变长的序列数据，比如自然语言处理任务中的文本</font>。

考虑一个最简单的二维输入的情况，在二维输入的情况下输入是一个矩阵，**每一行是一个样本，每一列是一个特征**

![img](https://i0.hdslb.com/bfs/note/7e9f1aed6a40e400cb368bdab253ee1a70ea3d2e.png@630w_!web-note.webp)

batch norm所干的事情就是每一次将每一列（特征）在它的一个小mini-batch里面的均值变成零，方差变成1

- 把这个向量本身的均值减掉，然后再除以他的方差就可以了
- 在计算均值的时候，是在每个小批量里面（一条向量里面算出他的均值和方差）
- 在训练的时候可以做小批量，在预测的时候会把全局的均值算出来
- 在预测的时候会把全局的均值算出来，整个数据扫一遍之后，在所有数据上平均的均值方差存起来，在预测的时候再使用

![img](https://i0.hdslb.com/bfs/note/4295a86121344721f7c408f8f68f07fabed9e5b6.png@630w_!web-note.webp)

- batchnorm还会学 lambda和beta 出来：可以把向量通过学习放成一个方差为任意某个值，均值为任意某个值的东西



**layernorm和batpchnorm在很多时候几乎是一样的**

- 对于一个同样的二维输入来说（最简单的情况），layer norm对每个样本做normalization而不是对每个特征做normalization（之前是将每个列的均值变为0，方差变为1，现在是把每一个行变成均值为0，方差为1，这里的每一个行表示一个样本，<font color=red>所以可以认为layernorm就是把整个数据转置一下放到batchnorm里面出来的结果，再转置回去）</font>
- 在transformer或者正常的RNN里面，输入是一个三维的东西，输入的是一个序列的样本，每一个样本其实里面有很多元素（比如一个句子里面有n个词，每个词有个向量hebatch的话，就是一个3D的东西）。最大的正方形表示batch（样本），但是列不再是特征了，而是序列的长度（sequence），对每个sequence（每个词）都有自己的向量，即feature

![img](https://i0.hdslb.com/bfs/note/3dacd4d421cb79e1730f1fab0379416e0b436db3.png@630w_!web-note.webp)

- 如果还是用batchnorm的话，每次是取一根特征，然后把他的每个样本里面的所有元素，以及他的整个batch取出来，如下图立方体中蓝色正方形所示，然后把他的均值变为0方差变成1，就相当于是切一块下来然后拉成一个向量，然后再进行运算

![img](https://i0.hdslb.com/bfs/note/4f45e9a27d369ab5e70da71b8f23e5b7271c5665.png@630w_!web-note.webp)

- 如果是laynorm的话，就是对每个样本进行横切，如上图立方体中橘黄色正方形所示 



切法不一样会带来不同的效果，为什么layernorm用的多一点？

- 在时序序列模型中，每个样本的长度可能会发生变化，如下图中红色阴影所示，没有的东西一般是会放零进去

![img](https://i0.hdslb.com/bfs/note/3054b67ce539f6bfdc90a1f08935d1aaaebd6e5d.png@630w_!web-note.webp)

- 如果是用batchnorm的话，切出来的效果如下图中所示，其余地方补零

![img](https://i0.hdslb.com/bfs/note/330c7dbd93d170182fc29136ca76d42ad6753d64.png@630w_!web-note.webp)

- 如果是layernorm的话，切出来的效果如下图所示

![img](https://i0.hdslb.com/bfs/note/7cabc373f27ccd8bb867abc27ef85518c2d8179a.png@630w_!web-note.webp)

- 这里主要的问题是在算均值和方差上面，对于batchnorm来说，会对上图中切出来的阶梯形的部分进行求解（只有这部分是有效值，其他地方因为是补零，所以其实没有太多作用），如果样本长度变化比较大的时候，每次做小批量的时候，算出来的均值和方差的抖动相对来说是比较大的
- 另外，在做预测的时候要把全局的均值和方差记录下来，这个全局的均值和方差如果碰到一个新的预测样本，如果碰到一个特别长的，因为在训练的时候没有见过这种长度的，那么在之前计算的均值和方差可能就不那么好用了。
- 相反，对于layernorm相对来说没有太多的问题，因为他是按照每个样本来进行均值和方差的计算，同时也不需要存下一个全局的均值和方差（不管样本的长短，均值和方差的计算都是以样本为单位的），这样的话相对来讲更稳定一些





**解码器**



解码器跟编码器很像，跟编码器一样是由（n=6）个同样的层构成的，每个层里面跟编码器一样有两个子层



解码器和编码器的不同之处在于解码器里面用了第三个子层，他同样是多头的注意力机制，跟编码器一样同样用了残差连接，用了layernorm



解码器中做的是自回归，也就是说当前的输出的输入集是上面一些时刻的输出，**意味着在做预测的时候不能看到之后的那些时刻的输出**

- 在注意力机制中，每一次能够看到完整的输入，这里要避免这个情况的发生，也就是说在解码器训练的时候，在预测第t个时刻的输出时候不应该看到t时刻以后的那些输入，<font color=blue>他的做法是通过一个带掩码的注意力机制，如下图中的masked所示，这也是与解码器其他地方的不同之处，这个masked的作用是保证输入进来的时候，在t时间是不会看到t时间以后的那些输入，从而保证训练和预测的时候行为是一致的</font>

![img](https://i0.hdslb.com/bfs/note/dfe1b6afe0df3580ac9da156feb63028e340e173.png@630w_!web-note.webp)





**子层**



注意力层



注意力：注意力函数是一个将 query 和一些 key-value对 映射成输出的函数

- 里面所有的query、key-value、输出都是向量

- 输出是value的加权和，所以输出的维度和value的维度是一样的

- 对于每一个value的权重，他是value对应的key和query的相似度（compatibility function，不同的注意力机制有不同的算法，不同的相似度函数导致不一样的注意力的版本）计算得来的

  34:45



transformer中使用到的注意力机制：

scaled dot-product attention

- **query和key长度是等长的，都等于dk（可以不等长的，不等长有别的计算方法）**
- value的长度等于dv（输出的长度也应该是dv）
- 具体的计算方法：对每一个query和key最内积，然后将其作为相似度（如果两个向量做内积的话，如果这两个向量的 long 是一样的，那么内积的值（余弦值）越大，就表示这两个向量的相似度就越高，如果内积等于零（两个向量正交），就是说这两个向量没有相似度），**算出来之后再除以根号dk（即向量的长度**），然后再用softmax来得到权重。因为对于一个query，假设给n个key、value对的话，就会算出n个值，因为这个query会跟每个key做内积，算出来之后再放进softmax就会得到n个非负的和为1的权重（对于权重来说，非负、和为1是比较好的权重），然后将这些权重作用在value上面，就能得到输出了。
- 在实际中不能一个一个做运算，运算起来比较慢，文章提出query可以写成矩阵，可能不只是一个query，也可能有n个query，query的个数和key value的个数可能是不一样的，但是长度一定是一样的，这样才能做内积
- 给定query和key这两个矩阵，相乘就会得到一个n*m的矩阵，如下图所示，他的每一行（如图中蓝色的线所示），就是一个query对所有key的内积值，再除以根号dk后做softmax（对每一行做softmax，行与行之间是独立的），这样就能得到权重，然后再乘V（V是一个m行dv列的矩阵），得到一个n*dv的矩阵（这个矩阵的每一行就是所需要的输出）

![img](https://i0.hdslb.com/bfs/note/041d075f36729f3cf7dad1ea1b6412c724e5068c.png@630w_!web-note.webp)

- 对于key、value对和n个query，可以通过两次矩阵乘法来做整个计算，key和value在实际中对应的就是序列，这样就等价于是在并行地计算里面的每个元素（矩阵乘法便于并行）





文中所提出的注意力机制和其他注意力机制的区别

一般有两种比较常见的注意力机制

- 加型注意力机制：可以处理query和key不等长的情况
- 点积注意力机制：点积注意力机制和文中所提出的注意力机制是一样的（唯一的区别就是文中所提出来的注意力机制多除以了一个根号dk，这个根号dk就是命名中提到的scaled）

这两种注意力机制其实都差不多，文章选用的是点积注意力机制，==因为实现起来比较简单，而且效果比较好，两次矩阵乘法就能算好==

这里为什么要除以根号dk？

- 当dk不是很大的时候除不除都没关系，但是当dk比较大的时候，也就是两个向量长度比较长的时候，做点积的时候值可能比较大也可能比较小
- 当值比较大的时候，向量之间相对的差距就会变大，就导致值最大的那个值进行softmax操作后就会更接近1，剩下的值就会更靠近于0，值就会向两极分化，当出现这种情况后，在算梯度的时候，梯度会比较小（softmax最后的结果是所希望的预测值置信的地方尽量靠近1，不置信的地方尽量靠近0，这样就差不多收敛了，梯度就会变得比较小，就会跑不动）
- 在transformer里面一般用的dk比较大（512），所以除以根号dk是一个不错的选择





整个注意力机制的计算过程如下图左图所示

![img](https://i0.hdslb.com/bfs/note/5873f1135c233f4bf9ca53377f2d444a52e99ea4.png@630w_!web-note.webp)

- Q代表query矩阵
- K代表key矩阵
- mask主要是为了避免在第t时刻的时候看到以后时间的东西，具体来说，假设query和key是等长的，长度都为n，而且在时间上是能对应起来的，对第t时刻的qt在做计算的时候，应该只是看到k1一直到k（t-1），而不应该看到kt和它之后的东西，因为kt在当前时刻还没有。
- 但是在做注意力机制的时候，会发现其实qt在跟所有k里面的东西全部做运算，从k1一直算到kn，只要保证在计算权重的时候，**不要用到后面的东西就可以了**
- mask是说对于qt和kt之后的用于计算的那些值，**把他们替换成非常大的负数**，这些大的负数在进入softmax做指数的时候就会变成0，所以导致softmax之后出来对应的权重都会变成0，而kt之前所对应的值会有权重
- 这样在计算输出的时候就只用到了v对应的v1一直到v（t-1）的结果，而vt后面的东西并没有用到
- 所以mask的效果是在训练的时候，让第t个时间的query只看到对应的前面的那一些key、value对，使得在做预测的时候能够进行一一对应





multi-head

与其做一个单个的注意力函数，不如把整个query、key、value投影到一个低维，**投影h次，然后做h次的注意力函数，再将每一个函数并在一起，再投影回来得到最终的输出，如下图右图所示**

![img](https://i0.hdslb.com/bfs/note/235b323c873517333e90e3da0329e7aab3ff5da6.png@630w_!web-note.webp)

- 原始的value、key、query进入一个线性层（线性层将其投影到比较低的维度），然后再做一个scaled dot-product attention（如上图左图所示），做h次，得到h个输出，再把这些输出向量全部合并到一起，最后做一次线性的投影，然后回到multi-head attetion



为什么要做多头注意力机制？

- dot-product的注意力中没有什么可以学的参数，具体函数就是内积。有时候为了识别不一样的模式，希望有一些不一样的计算像素的办法
- 如果是用加型attention的话，里面其实是有一个权重可以学习到的，但是本文使用的是内积，它的做法是先投影到低维，这个投影的w是可以学的，也就是说，有h次机会希望可以学到不一样的投影方法，使得再投影进去的度量空间中能够匹配不同模式所需要的相似函数，然后最后把所得到的东西再做一次投影（这里有点像在卷积神经网络里面有多个输出通道的感觉）
- 具体的计算（公式如下图）：在multi-head的情况下，还是以前的Q、K、V，但是输出已经是不同头的输出做contact运算再投影到一个WO里面，对每一个头，就是把Q、K、V通过不同的可以学习的WQ、WK、WV投影到dv上面，再做注意力函数，然后再出来就可以了

![img](https://i0.hdslb.com/bfs/note/524ad9abd746aa1f05c549e42bb0ba567128b273.png@630w_!web-note.webp)

- 实际上h是等于8的，就是用8个头
- 注意力的时候，因为有残差连接的存在，使得输入和输出的维度是一样的，所以他投影的时候，投影的就是输出的维度除以h（因为输出维度是512，除以8之后，就是每一次把它投影到64维的维度，然后在这个维度上面计算注意力函数，最后再投影回来）
- 虽然公式中看起来有很多小矩阵的乘法，实际上在实现的时候也可以通过一次矩阵的乘法来实现（可以作为一个练习题来练习如何实现）





在transformer模型中是如何实现注意力的？

三种实现情况

![img](https://i0.hdslb.com/bfs/note/e5eabdd645598be803f6fe2e83b53866807351b0.png@630w_!web-note.webp)

- 上图中三个阴影表示三个注意力层，这三个注意力层各不相同



第一个注意力曾的使用：

![img](https://i0.hdslb.com/bfs/note/adda262c75f3b2b6214c7cd8027aaca973aa9139.png@630w_!web-note.webp)

- 上图中红色圈出来的部分是编码器的注意力层。编码器的输入（假设句子长度是n的话，他的输入其实是n个长为d的向量，每一个输入的词对应的是一个长为d的向量，一共有n个）

- 这个注意力层有三个输入，分别表示的是key、value、query。这里一根线复制成三根线表示同样一个东西，既作为key，也作为value和query，这个东西叫做自注意力机制，就是说key、value和query其实是一个东西，就是他自己本身

- 这里输入了n个query，每个query会得到一个输出，最终会得到n个输出，而且这个输出和value因为长度是一样的，所以输出的维度其实也是d，即输入和输出的大小其实是一样的，输出长也为n。

- 输出其实就是value的加权和，权重来自query和key

  49:48

- 假设不考虑多头和有投影的情况，输出其实是输入的加权和，权重来自于自己本身跟各个向量之间的相似度。如果有多头的话，因为有投影，会学习出h个不一样的距离空间出来，使得得出来的东西会有点不一样



第二个注意力层的使用：

![img](https://i0.hdslb.com/bfs/note/eb85c3be8b1398c59e37455e34eecf875cbdbbb6.png@630w_!web-note.webp)

- 如上图中红色圆圈圈出的部分所示
- 解码器也是一样的，也是同一个东西复制了三次
- 解码器的输入也是一样的，只是长度可能变成了m，维度其实也是一样的，所以它跟编码器一样的也是自注意力，唯一不一样的是里面有一个mask（mask的作用：在解码器计算query对应的输出的时候，不应该看到第t时刻后面的东西，意味着后面的东西要设为0）





第三个注意力层：

![img](https://i0.hdslb.com/bfs/note/372e3f43acc43abc457f882067609ef56ecc7255.png@630w_!web-note.webp)

- 如上图中红线箭头所指的部分

- 它不再是自注意力了，key和value来自编码器的输出，query来自解码器下一个attention的输入

- 编码器最后一层的输出就是n个长为d的向量

- 解码器的masked attention的输出是m个长为d的向量

- 编码器的输出作为key和value传入到这个注意力层中，解码器的下一层输出作为query传入到这个注意力层中，意味着对于解码器的每一个输出，作为query要计算出一个所要的输出，这个输出是来自于value的一个加权和（来自于编码器的输出的加权和，权重的粗细程度取决于query与编码器输出的相似度，如果相似度比较高的话，权重就会大一点，相反，如果相似度比较低的话，权重就会小一点）

- 这个attention中所要做的其实就是有效地把编码器里面的输出根据需要截取出来。例如

  53:20

- attention如何在编码器和解码器之间传递信息的时候起作用：根据在解码器输入的不同，根据当前的向量在编码器的输出里面挑选感兴趣的东西，也就是去注意感兴趣的东西，那些不那么感兴趣的东西就可以忽略掉





feed forward：



其实就是一个fully connected feed-forward network，就是一个MLP，但是不同之处在于他是applied to each position seperately and identically（就是把同一个MLP对每个词作用一次，即position-wise，说白了就是MLP只是作用在最后一个维度）

- position：输入序列中有很多个词，每一个词就是一个点，这些点就是position
- 具体公式如下图所示，xW1+b1就是一个线性层，max就是relu激活层，最后再有一个线性层

![img](https://i0.hdslb.com/bfs/note/aa19f938abdb89ea54e7f388d97a379c2c80b1f1.png@630w_!web-note.webp)

- 在注意力层的输入（每一个query对应的输出）的长为512，x就是一个512的向量，W1会把512投影成2048（等价于将他的维度扩大了4倍），以为最有有残差连接，所以还需要投影回去，所以W2又把2048投影回512
- 这其实就是一个单隐藏层的MLP，中间的隐藏层将输入扩大4倍，最后输出的时候又回到输入的大小（如果用pytorch来实现的话其实就是把两个线性层放在一起，而不需要改任何参数，因为pytorch在输入是3d的时候，默认就是在最后一个维度做计算）





整个transformer是如何抽取序列信息，然后把这些序列信息加工成最后所想要的语义空间向量？



56:53



首先考虑一个最简单的情况（没有残差连接、attention也是单头、没有投影），如下图所示

![img](https://i0.hdslb.com/bfs/note/288e6174d7c9e420fd77c7cda728e3c15ae09297.png@630w_!web-note.webp)

输入就是长为n的向量，在进入attention之后，就会得到同样长度的输出，最简单的attention其实就是对输入进行加权求和，加权和之后进入MLP，每个MLP对每一个输入的点做运算会得到输出，最后就得到整个transformer块的输出（输入和输出的大小都是一样的）

- 在整个过程中attetion所起到的作用就是把整个序列里面的信息抓取出来做一次汇聚（aggregation），因为已经抓取序列中感兴趣的信息，所以在做投影、MLP、映射成为更想要的语义空间的时候，因为加权之后的结果已经包含了序列种的信息，所以每个MLP只要再每个点独立进行运算就行了，因为序列信息已经汇聚完成，所以在做MLP的时候是可以分开做的



作为对比，RNN（没有隐藏层的MLP，纯线性）的实现过程

RNN的输入也是向量，对于第一个点也是做一个线性层

对于下一个点，如何利用序列信息，还是用之前的MLP（权重跟之前是一样的），但是时序信息（下图中绿色曲线表示之前的信息，蓝色曲线表示当前的信息）方面，是将上一个时刻的输出放回来作为输入的一部分与第二个点一起作为输入，这样就完成了信息的传递



RNN和transformer都是用一个线性层或者MLP来进行语义空间的转换，但是不同之处在于传递序列信息的方式：RNN十八上一时刻信息的输出传入下一时候做输入，但是在transformer中是通过attention层全局地拉取整个序列里面的信息，然后再用MLP做语义转换

- 他们的关注点都是如何有效地使用序列信息





embeding

因为输入是一个一个的词（或者叫词源，token），需要将其映射成向量。embeding的作用就是给任何一个词，学习一个长为d的向量来表示它（本文中d等于512）

- 编码器的输入需要embeding
- 解码器的输入也需要embeding
- 在softmax前面的线性也需要embeding

本文中这3个embeding是一样的权重，这样子训练起来会简单一点

另外还将权重乘了根号d：维度一大的话，权重值就会变小，之后要加上positional encoding，他不会随着长度变长把他的long固定住，因此乘上了根号d之后，使得他们在scale差不多

01:01:15







positional encoding

attention不会有时序信息，输出是value的加权和，权重是query和key之间的距离，和序列信息无关（也就是说给定一句话，把顺序任意打乱之后，attetion出来的结果都是一样的，顺序会变，但是值不会变，这样是存在问题的，所以需要把时序信息加入进来）

RNN是如何添加时序信息的？RNN将上一个时刻的输出作为下一个时刻的输入来传递历史信息

attention是在输入里面加入时序信息，将输入词所在的位置信息加入到输入里面（positional encoding），具体公式如下图所示

![img](https://i0.hdslb.com/bfs/note/c7647aebdd77684ce356c7e89d8ea81e28e0b913.png@630w_!web-note.webp)



01:03:23



- 在计算机中，数字是用一定长度的向量来表示的
- 词在嵌入层会表示成一个长为d的向量，同样用一个长为d的向量来表示数字，也就是词的位置，这些数字的不同计算方法是用周期不一样的sin和cos函数的值来算出来的，所以说任何一个值可以用长为d的向量来表示，然后这个长为d的记录了时序信息的向量和嵌入层相加，就完成了将时序信息加进数据中的操作，如下图中的红色线团所示，因为是cos和sin的函数所以是在+1和-1之间抖动的，所以乘了一个根号d，使得每个数字也是差不多在正负1的数值区间里面

![img](https://i0.hdslb.com/bfs/note/d23b1523d97fb0363e9e71f8a27f5d4aa3d41728.png@630w_!web-note.webp)









**4、为什么要用自注意力？**

相对于使用循环层或者卷积层，使用自注意力有多好

下表比较了四种不一样的层

![img](https://i0.hdslb.com/bfs/note/9c7cf0185f83d58bc0105949ec377ad7c8d8af49.png@630w_!web-note.webp)

- 第一个是自注意力层
- 第二个是循环层
- 第三个是卷积层
- 第四个是构造出来的受限的自注意力
- 第一列是计算复杂度，越低越好
- 第二列是顺序的计算，越少越好。指的是在算layer的时候，下一步计算必须要等前面多少步计算完成。相当于是说非并行度
- 第三列说的是信息从一个数据点走到另一个数据点要走多远，越短越好
- n是序列的长度
- d是向量的长度
- 整个自注意力就是几个矩阵做运算，其中一个矩阵运算时query矩阵（n行，n个query，列数是d，也就是维度是d）乘以key矩阵（也是n*d），两个矩阵相乘，算法复杂度就是n平方乘以d，别的矩阵运算复杂度也是一样的。因为只是牵涉到矩阵的运算，矩阵中可以认为并行度是比较高的，所以是o（1），最大长度是说从一个点的信息想跳到另一个点要走多少步，在attention里面，一个query可以跟所有的key做运算，而且输出是所有value的加权和，所以query跟任何一个很远的key、value对只要一次就能过来，所以长度是比较短的
- 对循环层来说，如果序列是乘了n的话就一个一个做运算，每个里面主要的计算就是n*n的矩阵的dense layer然后再乘以长为d的输入，所以是n平方，然后要做n次，所以是n*d平方
- 对比自注意力和RNN的计算复杂度，其实是有一定的区别的，取决于n大还是d大。在本文中d是512，n也差不多是几百，现在比较大的模型d可以做到2048甚至更大，n相对来说也会做的比较多，所以现在看起来其实这两种的计算复杂度是差不多的（n和d在差不多的数据上面），但是在循环的时候因为要一步一步做运算，当前时刻的值需要等待前面完成，所以导致了是一个长为n的序列化的操作，在并行上比较吃亏。在最初点的那个历史信息到最后一个点的时候需要走过n步才能过去，所以循环的最长距离是o（n），即RNN对特别长的序列的时候做的不够好，因为信息从一开始走，走着走着就走丢了，而不是像attention一样直接一步就能到
- 卷积在序列上的具体做法是用一个1d的卷积，所以它的kernel就是k，n是长度，d就是输入的通道数和输出的通道数，所以这里是k乘n乘d的平方。k一般不大（一般是3或者5，所以一般可以认为是常数），所以导致卷积的复杂度和RNN的复杂度差不多，但是卷积的好处在于只用卷积就完成了，并行度很高，所以卷积做起来通常比RNN要快一点，另外卷积每一次一个点是由一个长为k的窗口来看，所以信息在k距离内是能够一次完成传递的，如果超过k的话，传递信息就需要通过多层一层一层上去，但是由于是log的操作，所以也不会有太大的麻烦
- 最后一个层是在做注意力的时候，query只跟最近的r个邻居做运算，这样就不用算n平方了，但是问题在于这样的话，两个比较长的远一点的点需要走几步才能过来
- 一般来说使用attetion主要是关心对于特别长的序列是否能将整个信息揉的比较好一点所以在实际过程中，带限制的自注意力使用的不是特别多，一般都是用最原始的版本，而不做受限，所以基本就是考虑前三种层
- 实际中，当序列的长度和整个模型的宽度差不多而且深度都一样的话的时候，基本上前三个模型的算法复杂度都是差不多的，attention和卷积相对来说计算会好一点，attention在信息的糅合性上会好一点，所以用了self-attention看上去对长数据处理更好一些，但是实际上attention对整个模型做了更少的假设，导致需要更多的数据和更大的模型才能训练出来跟RNN和CNN同样的效果，所以导致现在基于transformer的模型都特别大

### 7、实验



训练的一些设置



训练数据集和batching

使用了两个任务：

- 英语翻德语标准的WMT 2014的数据，里面有4.5w个句子，使用了byte-pair encoding（bpe，不管是英语还是德语，一个词里面有很多种变化，如果直接把每个词做成一个token的话，会导致字典里面的东西会比较多，而且一个动词可能有好几种不同的变化形式，在做成不一样的词的时候，它们之间的区别模型是不知道的，bpe相对来说就是把词根给提出来，这样的好处是整个字典比较小），这里使用的是37000个token的字典，他是在英语和德语之间共享的，也就是说不再为英语构造一个字典也不再为德语构造一个字典，这样的好处是整个编码器和解码器的embeding就可以使用同一个东西，而且整个模型变得更加简单，也就是之前说的编码器和解码器的embeding是权重共享的
- 英语翻法语：使用了一个更大的数据集



硬件和schedule

- 训练使用了8个P100的GPU，后来使用tpu（tpu更适合做大的矩阵乘法）
- base模型使用小一点的参数，每一个batch训练的时间是0.4秒，一共训练了10w步，一共在8台gpu上训练了12个小时（基本上一个模型训练12个小时也是一个不错的性能）
- 这个大的模型，一个batch训练需要一秒钟，一共训练了30W步，最后一台机器训练了3.5天，其实也是一个可以承受的范围





在训练器上使用的是Adam

学习率是根据以下公式计算出来的：学习律是根据模型宽度的-0.5次方（就是说当模型越宽的时候，学习的向量越长的时候，学习率会低一点）

![img](https://i0.hdslb.com/bfs/note/de02350ab647617c8db3ff593db62d2e2ff8af3e.png@630w_!web-note.webp)

- 存在一个warmup，就是从一个小的值慢慢地爬到一个高的值，爬到之后，再根据步数按照0.5次方衰减，最后warmup是4000
- 学习率几乎是不用调的：第一，adam对学习率不敏感；学习率已经把模型考虑进来了，schedule也已经算是不错的schedule了，所以学习率是不需要调的





正则化

总共使用了三个正则化

residual dropout：对每一个子层（包括多头的注意力层和之后的MLP），在每个层的输出上，在进入残差连接之前和进入到layernorm之前，使用dropout（dropout率为0.1，也就是把输出的10%的那些元素值乘0.1，剩下的值乘1.1）。另外在输入加上词嵌入再加上position encoding的时候，也用了一个dropout。也就是说，基本上对于每一个带权重的层，在输出上都使用了dropout，虽然dropout率不是特别高，但是使用了大量的dropout层来对模型做正则化





label smoothing（inception v3）

使用softmax去学东西的时候，正确的标号是1，错误的标号是0，对于正确的label的softmax值，让他去逼近于1，但是softmax的值是很难去逼近于1的，因为他里面是指数，比较soft（需要输出接近无限大的时候，才能逼近于1），这样使得训练比较困难。

一般的做法是不要搞成特别难的0和1，可以把1的值往下降一点，比如降成0.9，本文中是直接降成了0.1，就是说对于正确的那个词，只需要softmax的输出（置信度）到0.1就可以了，而不需要特别高，剩下的值就可以是0.9除以字典大小，这里会损失perplexity（log lost做指数），基本上可以认为是模型的不确信度（正确答案只要10%是对的就行了，不确信度会增加，所以这个值会变高），但是在模型不那么确信的情况下会提升精度和BLUE的分数（这两个才是真正所要关心的点）





下表表示的是不同的超参数之间的对比

![img](https://i0.hdslb.com/bfs/note/ffd017934f4674fdc9a2fb7ad5610768e2e45d32.png@630w_!web-note.webp)

- n表示要堆多少层

- d表示模型的宽度，即token进来要表示成一个多长的向量

- dff表示MLP中间隐藏层的输出的大小

- h表示注意力层的头的个数

- dk、dv分别是一个头里面key和value的维度

- Pdrop表示dropout的丢弃率

- els表示最后label smoothing的时候要学的label的真实值是多少

- train steps表示要训练多少个batch

  01:19:54

- 整个模型参数相对来说还是比较简单的，基本上能调的就是上面的这些超参数剩下的东西基本上都是可以按比例算过来的，这也是transformer的一个好处，虽然看上出模型比较复杂，但是实际上没有太多可以调的东西，这个设计对后面的人来说相对更加方便一点



### 8、评价



写作

- 非常简洁，每一句上基本上在讲一个事情
- 没有使用太多的写作技巧
- 这种写法并不是很推荐，因为对一篇文章来说，需要讲一个故事来让读者有代入感，能够说服读者
- 一般说可以选择把东西减少一点，甚至把一些不那么重要的东西放在附录里面，但是在正文的时候，最好还是讲个故事：为什么做这些事情以及设计的理念、对整个文章的一些思考，这让会使得文章更加有深度



transformer模型

- 现在不仅仅是用在机器翻译上，也能够用在几乎所有的NLP任务上面，在bert，gpt，后续能够训练很大的预训练模型，能够极大提升所有NLP任务的性能，类似于CNN对整个计算机视觉的改变：能够训练一个大的CNN模型，使得别的任务也能够从中受益，CNN使得整个计算机视觉的研究者提供了一个同样的框架，使得只要学会CNN就行了，而不需要去管以前跟任务相关的海量专业知识（比如特征提取、任务建模等）
- 对于transformer来说，之前需要做各种各样的数据文本的预处理，然后根据NLP的任务设计不也一样的架构，现在不需要了，使用了整个transforner架构就能够在各个任务上取得非常好的成绩，而且它的预训练模型也让大家的训练变得更加简单
- 现在transformer不仅仅是用在自然语言上面，也在图片、语音、video上面取得了很大的进展
- 之前计算机视觉的研究者使用CNN，而在语言处理使用RNN，在别的方面用别的模型，现在发现同样一个模型能够在所有领域都能用，让大家的语言的一样了，任何一个领域的研究做的一些突破能够很快地在别的领域被使用，能够极大地减少一个新的技术在机器学习里面各个领域被应用的时间
- 人对世界的感知是多模态的：图片、文字、语音，现在transformer能够把这些所有不同的数据给融合起来，因为大家都用一个同样的架构抽取特征的话，就可以抽取到一个同样的语义空间，使得我们可以用文本、图片、语音、视频等训练更好更大的模型
- 虽然transformer这些模型取得了非常好的实验性的结果，但是对它的理解还是处于比较初级的阶段。
- 虽然标题说只需要attention就够了，但是最新的研究表明，attention只是在transformer里面起到把整个序列的信息聚合起来的作用，但是后面的MLP以及残差连接是缺一不可的，如果把这些东西去掉的话，attention基本上什么东西都训练不出来所以模型也不只是说只需要attention就够了
- attention根本就不会对数据的顺序做建模，为什么能够打赢RNN呢？RNN能够显示建模的序列信息理论上应该比MLP效果更好现在大家觉得它使用了一个更广泛的归纳偏置，使得他能够处理一些更一般化的信息，这也是为什么说attetion并没有做任何空间上的一些假设，它也能够跟CNN甚至是比CNN取得更好的结果，但是他的代价是因为他的假设更加一般，所以他对数据里面抓取信息的能力变差了，以至于说需要使用更多的数据、更大的模型，才能训练出想要的效果，这也是为什么现在transformer模型越来越大
- attention也给了研究者一些鼓励，原来CNN和RNN之外也会有新的模型能打败他们。现在也有一些工作说，就用MLP或者就用一些更简单的架构也能够在图片或者文本上面取得很好的结果
- 未来肯定会有很多新的架构出现，让整个领域更加有意思一些



## 四、GAN  阅读



**GAN**



generative Adversarial  Nets



###  1、标题 + 作者



机器学习中有两大类

- 分辨模型：判断数据的类别或者预测实数值
- 生成模型（generative）：生成数据



adversarial：对抗



Nets：networks



一作 Ian J.Goodfellow ,也是《深度学习》（花书）的作者



### 2、摘要



写法简洁，重点在于讲清楚GAN



文章提出了一个新的framework（framework通常是一个比较大的模型）用来估计生成模型，通过对抗的过程，同时会训练两个模型

- <font color=red>生成模型G</font>：用来抓取整个数据的分布（生成模型就是要对整个数据的分布进行建模，使得能够生成各种分布，这里的分布就是指的生成图片、文字或者电影等，在统计学中，整个世界是通过采样不同的分布来得到的，所以如果想要生成东西，就应该抓取整个数据的分布）
- 辨别模型D：用来估计样本到底是从真正的数据生成出来的还是来自生成模型生成出来的，(警察)

生成模型尽量想让辨别模型犯错（生成模型一般是尽量使数据分布接近，但是这个地方有所不同，它是想让辨别模型犯错）



这个framework对应的是minmax two-player game（博弈论中一个很有名的两人对抗游戏）



在任何函数空间的 G 和 D 中存在一个独一无二的解（ G 能够将数据的真实分布找出来，如果已经把真实数据发掘出来的，辨别模型 D 就做不了什么事情了），如果 G 和 D 是MLP的话，那么整个系统就可以通过误差的反向传播来进行训练



不需要使用任何马尔科夫链或着说对一个近似的推理过程展开，相比其他方法更加简单，而且实验效果非常好



### 3、导言



深度学习是用来发现一些丰富的有层次的模型，这些模型能够对AI中各种应用的各种数据做概率分布的表示

- 深度学习不仅仅是深度神经网络，更多的是对整个数据分布的特征表示，深度神经网络只是其中的一个手段



虽然深度学习在辨别模型上取得了很大进展，但是在生成模型上做的还是比较差（难点在于在最大化似然函数的时候要对概率分布进行很多近似，这个近似的计算比较困难）



<font color=red>深度学习在生成模型上进展不大，是因为要去近似概率分布分布来计算似然函数，这篇文章的关键是不用近似似然函数而可以用别的方法来得到一个计算上更好的模型</font>



==这个框架中每一个模型都是MLP==，所以取名叫GAN。这个框架中有两类模型

- 生成模型：类似于造假的人，他的目的是去生产假币
- 辨别模型：类似于警察，他的任务是将假币找出来，和真币区分开开来

造假者和警察会不断地学习，造假者会提升自己的造假技能，警察会提升自己判别真币和假币的性能，最终希望造假者能够赢得这场博弈，也就是说警察无法区分真币和假币，这时就能生成跟真实一样的数据了



框架下面的生成模型是一个MLP，它的输入是一个随机的噪音，MLP能够把产生随机噪音的分布（通常是一个高斯分布）映射到任何想要拟合的分布中。同理，如果判别模型也是MLP的情况下，在这个框架下的特例叫做adversarial nets，因为两个模型都是基于MLP，所以在训练的时候可以直接通过误差的反向传递而不需要像使用马尔科夫链类似的算法来对一个分布进行复杂的采样，从而具有计算上的优势



### 4、相关工作



视频中所读的版本是GAN在neurips上的最终版本，它是一个比较新的版本



在搜索的时候可能会搜索到arxiv版本，它是一个比较早期的版本，作者没有将最新的版本上传到arxiv上面



以上两个版本的主要区别就是相关工作是不不一样的

- arxiv版本上面相关工作的部分其实什么都没写，写的并不相关
- neurips的版本上写了很多真正相关的工作



首先阐述了其他方法所存在的问题：之前的方法是想要构造出一个分布函数出来，然后提供一些参数让他可以学习，通过最大化这些参数的对数似然函数来做这样的坏处是采样一个分布的时候计算比较困难（尤其是维度比较高的时候）



因为这些方法计算比较困难，所以开展了generative machines的相关工作，*不再去构造这样一个分布出来，而是去学习一个模型去近似这个分布，这两种方法是有区别的*

- 前一种方法明确知道分布是什么，包里面的均值、方差等
- 后一种方法不用去构造分布，只需要一个模型去近似想要的结果就可以了，缺点是不知道最后具体的分布是什么样的，好处是计算起来比较容易



![img](https://i0.hdslb.com/bfs/note/343eb63f72af165b00a6841fa24af0e3b91ddd4d.png@640w_!web-note.webp)

- 上式中对 f 的期望求导等价于对 f 自身求导，==这也是为什么可以通过误差的反向传递对GAN进行求解==



VAEs跟GAN非常类似



通过一个辨别模型来帮助生成模型，比如说NCE也用了这样的思路，但是NCE相对来说损失函数更加复杂一点，在求解上没有GAN的性能那么好



和predictability minimization算法的区别

- 其实GAN就是predictability minimization反过来



一个真实有用的技术会在不同的领域不断被人重新发现给予新的名词，大家通常会将功劳归功于那个教会了大家这个技术的人，而不是最早发明他的人



adversarial examples和GAN的区别

- adversarial examples是说通过构造一些和真实样本很像的假样本，能够糊弄到分类器，从而测试整个算法的稳定性

### 5、模型



==这个框架最简单的应用是当生成器和辨别器都是MLP的时候==，生成器需要去学一个在数据x上的Pg分布，x中每个值的分布都是由pg这个分布来控制的



生成模型如何输出x

- 首先在一个输入分布为Pz的噪音变量 z 上定义一个先验，z可以认为是一个100维的向量，每一元素是均值为0，方差为1的高斯噪音
- ==生成模型就是把z映射成x，生成模型是MLP，他有一个可以学习的参数 θg== 



假设想要生成游戏的图片

- 第一种办法是反汇编游戏代码，然后利用代码就知道游戏是如何生成出来的，==这就类似于构造分布函数的方法，在计算上比较困难==
- 第二种办法是不管游戏程序是什么，假设用一个若干维的向量就足以表达游戏背后隐藏的逻辑，再学一个映设（MLP，MLP理论上可以拟合任何一个函数，所以可以通过构造一个差不多大小的向量，然后==利用MLP强行将z映射成x==，使得他们相似就可以了），这种方法的好处是计算比较简单，坏处是MLP不在乎背后真正的分布是什么，而是只是每次生成一个东西，看起来相似就行了



辨别器D也是一个MLP，它也有自己可以学习的参数 θd ，它的作用是将数据放进来之后输出一个标量，这个标量用来判断x到底是来自真实采样的数据还是生成出来的图片（以游戏为例，就是这个图片到底是来自游戏中的截图，还是生成器自己生成的图片），因为知道数据的来源，所以会给数据一个标号（如果来自真实的数据就是1，来自生成的数据就是0）



所以就采样一些数据来训练一个两类的分类器



<font color=red>在训练D的同时也会去训练G，G用来最小化log(1-D(G(z)))</font>

- z 代表随机噪音，放到G中就会生成图片，假设辨别器正确的话，辨别器的输出应该为0，表示是生成的数据，这个式子最终为log1等于0
- 但是如果辨别器没有做好，会输出一个大于0的数，在极端情况下输出1，即辨别器百分之百地确信生成模型所生成的辨别器来自真实的数据，即判断错误。则无论无何log（1-一个大于零小于1 的数）的最终结果就会变成一个负数，在极端情况下，log0是负无穷大
- 所以如果要训练G来最小化log(1-D(G(z)))就意味着，训练一个G使得辨别器尽量犯错，无法区分出来数据到底是来自真实数据还是生成模型所生成的数据



总结

![img](https://i0.hdslb.com/bfs/note/c90034d3d3af722239b39d2012e719092e47bb44.png@640w_!web-note.webp)



20:35

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。



<font color=red>符号 $\mathbb{E}_{z \sim p_z(z)}$ 表示的是对随机变量 \(z\) 的期望值，其中 \(z\) 是从先验分布 \(p_z(z)\) 中采样得到的。这里的 \(\mathbb{E}\) 是期望（Expectation）的缩写，而 $z \sim p_z(z)$ 表示 \(z\) 是遵循概率分布 \(p_z(z)\) 的一个随机变量</font>。



1. **随机变量 \(z\)**：
   \(z\) 通常在机器学习和统计学中用来表示一个随机变量，它可能代表数据点、特征向量、或者在生成对抗网络（GANs）中的输入噪声。

2. **概率分布 \(p_z(z)\)**：
   \(p_z(z)\) 是定义在 \(z\) 上的概率分布，描述了 \(z\) 可能取值的概率。在GAN中，这通常指的是输入到生成器的随机噪声的分布，常见的选择是高斯分布。

3. **期望 \(\mathbb{E}_{z \sim p_z(z)}\)**：
   期望值是一种度量随机变量平均取值的方法。对于离散随机变量，期望值是每个可能值乘以其发生的概率的总和。对于连续随机变量，期望值是每个可能值乘以其概率密度的积分。



数学表达：

对于离散随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \sum_{z} z \cdot p_z(z) $
其中，\( p_z(z) \) 是 \( z \) 取特定值的概率。

对于连续随机变量，期望值可以表示为：
$\mathbb{E}[Z] = \int_{-\infty}^{\infty} z \cdot p_z(z) \, dz $
其中，\( p_z(z) \) 是 \( z \) 的概率密度函数。

在GAN中的应用：

在GAN的上下文中，$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$这一项是训练生成器 \( G \) 的一部分，其中 \( G(z) \) 是生成器生成的数据，\( D \) 是判别器。生成器 \( G \) 试图生成尽可能接近真实数据的样本，以使判别器 \( D \) 难以区分真假。这里的期望值是对所有可能的噪声 \( z \) 进行平均，以评估生成器的整体性能。

公式（1）定义了生成对抗网络（GAN）中生成器 \( G \) 和判别器 \( D \) 之间的最小最大游戏的价值函数 \( V(D, G) \)。公式如下：

$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$

这里，\( p_{\text{data}}(x) \) 是训练数据的真实分布，\( p_z(z) \) 是输入噪声的先验分布，\( G(z; \theta_g) \) 是生成器，它将噪声 \( z \) 映射到数据空间，\( D(x; \theta_d) \) 是判别器，它输出一个标量，表示 \( x \) 来自真实数据分布而不是生成器分布的概率。

公式（1）可以分为两部分：

1. **判别器的期望**：$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$
   - 这部分衡量判别器识别真实数据的能力。判别器 \( D \) 被训练以最大化这个项，这意味着它应该尽可能准确地识别出真实数据样本。

2. **生成器的期望**：$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$
   - 这部分衡量生成器欺骗判别器的能力。生成器 \( G \) 被训练以最小化这个项，这意味着它应该生成尽可能接近真实数据的样本，以使判别器难以区分真假。

最小最大游戏的目标是找到生成器和判别器的平衡点，在这个点上，判别器无法区分真实数据和生成数据。当这个平衡点达到时，生成器 \( G \) 能够生成与真实数据分布 \( p_{\text{data}}(x) \) 无法区分的数据。

在实践中，我们不能同时优化 \( G \) 和 \( D \) 到它们的最优值，因为这将需要无限次的迭代和计算资源。相反，我们使用交替优化策略，其中我们迭代地更新判别器 \( D \) 多次，然后更新生成器 \( G \) 一次。这种方法被称为交替优化或小批量随机梯度下降，如论文中算法1所示。

==总结来说，公式（1）定义了GAN训练过程中生成器和判别器之间的竞争目标，生成器试图生成越来越真实的数据，而判别器试图区分真实和生成的数据。这种对抗性的训练过程最终导致生成器学习到真实数据的分布。== 



- 目标函数如上图中公式所示，是一个两人的minimax游戏
- ==V（G，D）是一个价值函数==
- 公式右边第一项是期望，x是采样真实分布
- 公式右边第二项是期望，x是采样噪音分布
- 在D是完美的情况下，公式右边的两项应该都是等于0的
- 如果D不完美、有误分类的情况下，这两项因为log的关系，都会变成一个负数值
- 所以如果想要辨别器完美地分类这两类的话，就应该最大化D的值，最小化G，目标函数中有两个东西，一个是min，一个是max，和一般的训练步骤有所区别，一般只有一个min，或者只有一个max，这里既有min又有max，就是两个模型在相互对抗：D是尽量把数据分开，G是尽量使生成数据分不开，这个在博弈论中叫两人的minimax游戏
- 如果达到了一个均衡，就是D不能往前进步，G也不能往前进步了，就认为达到了均衡，这个均衡叫做纳什均衡





![img](https://i0.hdslb.com/bfs/note/49f4369d54a493101193b6b59f8a9453dde54ecd.png@640w_!web-note.webp)

- 上图中一共有四张图，分别表示GAN在前面三步和最后一步所做的工作
- z是一个一维的标量
- x也是一个一维的标量
- 噪音是均匀分布采样来的
- **所要真实拟合的x如图中黑色圆点所示，是一个高斯分布**
- （a）表示第一步的时候，生成器将均匀分布进行映射，图中绿色的线就是把z映射成了一个高斯分布，此时辨别器视图中蓝色的线，表现一般
- （b）表示更新辨别器，尽量把这两个东西分开，**两个高斯分布的最高点表示真实分布和噪声最有可能出现的地方，辨别器需要在真实分布的地方值为1**，在噪音分布的地方值为0，这样就可以尽量将来自真实分布的x和来自于生成器的x尽量分别开来
- （c）表示尽量更新生成器，使得能够尽量糊弄到辨别器（就是将生成器生成的高斯分布的峰值尽量左移，向真实数据的高斯分布进行靠拢），让辨别器犯错，这时候辨别器就需要尽量调整来把这两个细微的区别区别开来。
- （d）表示通过不断地调整生成器和辨别器，直到最后生成器的模型能够将来自均匀分布的随即噪音z映射成几乎跟真实分布差不多融合的高斯分布，即从真实的黑点中采样还是从生成器的绿线采样，辨别模型都是分辨不出来的（不管来自于哪个分布，辨别器对这每个值的输出都是0.5，这就是GAN最后想要的结果：生成器生成的数据和真实数据在分布上是完全分别不出来的，辨别器最后对此无能为力）





![img](https://i0.hdslb.com/bfs/note/848589b90e335f754444ab0dbcdfeb9ef1b4116e.png@640w_!web-note.webp)

- 上图表示的是算法一
- 第一行是一个for循环，每一次循环里面是做一次迭代，迭代的另一部分也是一个k步的for循环，每一步中先采样m个噪音样本，再采样m个来自真实数据的样本，组成一个两个m大小的小批量，将其放入价值函数中求梯度（就是将采样的真实样本放入辨别器，将采样的噪音放进生成器得到的生成样本放进辨别器**，放进去之后对辨别器的参数求梯度来更新辨别器**），这样子做k步，做完之后再采样m个噪音样本放进第二项中，把它对于生成器的模型的梯度算出来，然后对生成器进行更新，这样就完成了一次迭代
- 每次迭代中，==先更新辨别器，再更新生成器==
- k是一个超参数，k不能取太小，也不能取太大，需要辨别器有足够的更新但也不要更新的太好。如果没有足够好的更新，对新的数据，生成器生成的东西已经改变了，如果辨别器没有做相应的变化，那么再更新生成器来糊弄D其实意义不大；反过来讲如果将D训练到足够完美，log(1-D(G(z)))就会变成0，对0进行求导，生成模型的更新就会有困难（**如果辨别器是警察，生成器是造假者，假设造假者一生产假币，警察就将其一锅端了，造假者也就不会赚到钱，就没有能力去改进之后的工艺了；反过来讲，如果警察没有能力，造假者随便造点东西，警察也看不出来，也抓不到造假者，那么造假者也不会有动力去改进工艺，使得假钞和真钞真的长得差不多，所以最好是两方实力相当，最后大家能够一起进步）**
- **k就是一个超参数，使得D的更新和G的更新在进度上差不多**
- 外层循环迭代N次直到完成，如何判断是否收敛，这里有两项，一个是往上走（max），一个是往下走（min），有两个模型，所以如何判断收敛并不容易。整体来说，GAN的收敛是非常不稳定的，所之后有很多工作对其进行改进









在上面的公式中，等式右边的第二项存在一定的问题：在早期的时候G比较弱，生成的数据跟真实的数据差得比较远，这就很容易将D训练的特别好（D能够完美地区分开生成的数据和真实的数据），就导致log(1-D(G(z)))会变成0，它变成0的话，对他求梯度再更新G的时候，就会发现求不动了。所以在这种情况下建议在更新G的时候将目标函数改成最大化log(D(G(z)))就跟第一项差不多了，这样的话就算D能够把两个东西区分开来，但是因为是最大化的话，问题还是不大的，但是这也会带来另外一个问题，如果D(G(z))等于零的话，log(D(G(z)))是负无穷大，也会带来数值上的问题，在之后的工作中会对其进行改进



### 6、理论



当且仅当生成器学到的分布和真实数据的分布式相等的情况下，目标函数有全局的最优解



算法一确实能够求解目标函数



第一个结论：当G是固定，即生成器是固定的情况下，最优的辨别器的计算如下图公式中所示

![img](https://i0.hdslb.com/bfs/note/596ad2ec9c620805028558d7978a49070965ef1c.png@640w_!web-note.webp)

- \* 表示最优解
- Pdata表示将x放进去之后，在真实产生数据的分布中的概率是多少
- ==Pg表示将x放进去之后，生成器所拟合的分布的概率是多少==
- 分布是在0和1之间的数值，所以上式中的每一项都是大于等于0、小于等于1的，因此上式中分子、分母中所有的项都是非负的，所以整个式子右式的值是在0到1之间的
- 当Pdata和Pg是完全相等的情况下（即对每一个x，两个p给出来的结果是一样的），右式的值是1/2，即不管对什么样的x，最优的辨别器的输出概率都是1/2，<font color=red>表示这两个分布是完全分不开的</font>
- 这里可以看到D是如何训练出来的，从两个分布中分别采样出数据，用之前的目标函数训练一个二分类的分类器，这个分类器如果说给的值都是1/2，即什么值都分辨不出来，就表示这两个分布是重合的，否则的话就能够分辨出来，这个东西在统计学中非常有用，这叫做two sample test：判断两个数据是不是来自同一个分布在统计上其实有很多工具，比如说用T分布检测（在数据科学中经常使用，可以完全不管分布是什么样子的，可以无视在高维上很多统计工序不好用，就训练一个二分类的分类器，如果这个分类器能够分开这两个数据，就表示这两个数据是来自于不同分布，如果不能分开，就表示这个数据是来自同一分布的，这个技术在很多实用的技术中经常会用到它，比如说在一个训练集上训练一个模型然后把它部署到另外一个环境，然后看新的测试数据跟训练数据是不是一样的时候，就可以训练一个分类器把它分一下就行了，这样就可以避免训练一个模型部署到一个新的环境，然后新的环境和模型不匹配的问题）





期望的计算如下图所示

![img](https://i0.hdslb.com/bfs/note/6d803a867e81ecb80f5a459a5f2c52c07019135d.png@640w_!web-note.webp)



33:08



- 等式右边第一项是在Pdata上面对函数求均值
- 等式右边第二项是在Pz上面对函数求均值
- 已知x=g(z)，x是由g（z）生成出来的，假设Pg就是生成器对应的数据映射，就将g（z）替代成x，替代之后，右边第二项对z的概率求期望就变成了对x求期望，x的分布来自于生成器所对应的Pg。
- 一旦完成替代之后，第一项和第二项是可以合并了，合并之后，积分里面的东西抽象出来经过替换变量就可以得到一个关于y的函数，如果y是一个值的话，它其实是一个凸函数，取决于a、b不一样，它的形状不一样。因为它是一个凸函数，所以他会有一个最大值，因为要求最大值，所以会求导，结果是y=a/（a+b），意味着对于任何的x，最优解的D对他的输出等于y等于Pdata(x)/(Pdata(x) + Pg(x))，就证明了之前的结论

![img](https://i0.hdslb.com/bfs/note/5ae22246d6ecb609ae61062433a45d47e8c755c3.png@640w_!web-note.webp)

- 将所求到的最优解代入到上图所示的价值函数中，最大化D，就是将D*直接代进去然后展开，就能得到如上图所示的结果，就能得到之前的结论，再把得到的结果写成一个关于G的函数，因为D已经求得最优解并带入了，所以整个式子就只跟G相关，所以将他记成C(G)，到此对整个价值函数求解就只需要对C(G)进行最小化就行了，因为D的最优解已经算出来了
- 定理一是说当且仅当生成器的分布和真实数据的分布是相等的情况下，C(G)取得全局最小值的时候
- KL散度：用来衡量两个分布。如下图左侧红色公式所示，它表示的在知道p的情况下至少要多少个比特才能够将q描述出来

![img](https://i0.hdslb.com/bfs/note/de4bcc1019ef3c5ce818ce2cfebff93102ef73c1.png@640w_!web-note.webp)



37:11



- 上式中最终结果中的两项实际上就是两个KL散度如下图中的公式所示

![img](https://i0.hdslb.com/bfs/note/175320a3dd312c17a161296dc5a96f3c984886e5.png@640w_!web-note.webp)

- ==KL散度一定是大于等于零的，KL要等于0，那么p和q要相等==
- 如果C(G)要取得最小值，所以需要两个KL散度等于零，又因为p=q，所以Pdata=(Pdata+Pg)/2，所以C(G)的最优解就等价于Pdata=Pg，这就证明了D在已经取得了最优解的情况下，如果想要对G取最优解的话一定是Pg=Pdata，具体来说，对于写成这种形式的两个分布又叫做JS散度
- JS散度和KL散度的区别：<font color=red>JS散度是对称的，而KL不是对称的</font>，不能将p和q进行互换，但是对于JS散度，将p和q进行互换也是可以保持不变的，所以说它是一个对称的散度，而KL是一个不对称的散度
- 也有评论说因为GAN是一个对称的散度，所以使得它在训练上更加容易。但是也可以取一个更好的目标函数使得训练更加艰难
- 到此就证明了目标函数的选择还是很不错的





结论二是说算法一是能够优化目标函数的



当G和D有足够的容量的时候而且算法一允许在中间的每一步D是可以达到它的最优解的时候，如果对G的优化是去迭代下图所示的步骤（式中G已经换成最优解了），那么最后的Pg会收敛到Pdata

![img](https://i0.hdslb.com/bfs/note/d48abdd39dfef7525a35ecb0c8509a938f333080.png@640w_!web-note.webp)

- 将目标（价值函数）看成是一个关于Pg（模型或者分布）的函数，Pg其实是一个函数，那么目标函数就是一个关于函数的函数
- 一个函数的输入可以是标量或者是向量
- 这里目标函数是一个函数的函数：输入不再是一个值，而是一个值加上了计算（等于是说在python中写一个函数，本来是接收一个x，x是一个vector，然后现在需要接收一个clousure，clousure就包括了计算和数），之前是在高维的值的空间里面做迭代，现在需要在一个函数空间里面做梯度下降
- Ex~Pg其实是关于Pg的一个很简单的函数，这个东西展开之后就是把Pg写出来，是一个积分，积分里面有一个Pg(x)，后面一项跟Pg无关，所以他其实就是一个线性函数，而且是一个凸函数
- 在每一步中把D求到最优，就是说一个凸函数的上限函数还是一个凸函数，所以这个凸函数做梯度下降的时候会得到一个最优解
- 虽然假设了每一次会对D优化到极致，但实际上在算法上只是迭代了k步，所以说这个证明并不能说算法一是工作的，但是实际上算法一跑的还是挺好的（其实算法一跑的并不好，还是挺难收敛的，经常会出现各种问题）

### 7、实验+总结



下图是生成的一些图片

![img](https://i0.hdslb.com/bfs/note/d0cd6b6e8cd4349ec8082e2411ac3497d8eb2209.png@640w_!web-note.webp)

- 数字生成的还行，但是后面的图片效果不太好，分辨率特别低，需要很长的时间才能生成稍微能看的图片



总结

- 坏处是整个训练是比较难的，G和D需要比较好的均衡，如果没有均衡好的话会导致生成的图片比较差
- 优势是因为生成器并没有看真正样本上的数据，没有试图去拟合真实数据的特征，使得它能够生成一些比较锐利的边缘，但是这个说法在后面发现并不是这样的



未来的工作

- conditional GAN：现在生成的时候是不受控制的，随便给定一个z，然后看最终出来的是什么东西，但最好是说控制一下去偏向所想要生成的东西



### 8、评论



写作

- 总的来说，写作还是比较明确的，主要关注GAN在干什么
- 摘要中主要讲述了GAN在干什么事情
- intro非常短，首先写了一点故事性（为什么要做这个事情），然后接下来就是写GAN在干什么
- 在相关工作中，虽然第一个版本写的比较糟糕，基本上就是在说与别人不一样，但是后来的版本也基本承认了很多想法前面的人工作都已经做过了（真正伟大的工作不在乎你的那些想法在别的地方已经出现过还是没有，**关键是说你能够给大家展示用这个东西在某个应用上能够取得非常好的效果，能够让别人信服跟着你继续往下做，然后把整个领域做大，这个是伟大工作的前提）**
- 第三章讲的是GAN的目标函数以及如何做优化
- 第四章证明了为什么目标函数能得到最优解以及求解算法在一定程度上能够得到最优解
- 最后一章简单介绍了一些实验和未来的工作

这样的写法比较i清楚，想读的东西可以一路读下来



但是如果工作的开创性并不是很高的时候就一定要写清楚跟别人的区别是什么和贡献是什么





对于GAN本身这个算法而言

- 它开创了一个领域
- 从一般化的角度来及那个，它影响了之后的很多工作（不仅仅是关于GAN）：1、他是无监督学习的，不需要使用标号；2、他用一个有监督学习的损失函数来做无监督学习的，他的标号（来自于采样的还是生成的）来自于数据，用了监督学习的损失函数，所以在训练上确实会高效很多，这也是之后自监督学习（比如说BERT）的灵感的来源



### 9、shuhuai008  讲解

![image-20241001222738993](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241001222738993.png)



1. **P(x ; θg)**: 这里，$P(x ; \theta_g)$表示条件概率分布，表示数据 \(x\) 在给定参数 $\theta_g$ 下的概率。这个分布可以是高斯分布、伯努利分布、或其他形式，取决于具体上下文中的模型。
   - **\(x\)** 是观测到的或生成的数据。
   - $\theta_g$通常是模型的参数（可能是均值、方差等），它控制生成数据的分布。
   
2. **Z ~ Pz(Z)**: 这个表示 Z 是随机变量，并服从 \(P_Z(Z)\) 的概率分布。
   - **\(Z\)** 是一个随机变量，它可能是潜在变量或者从先验分布中抽取的样本。
   - **\(P_Z(Z)\)** 通常是 Z 的概率分布，可能是标准正态分布、均匀分布等。



这个表达式可能出现在生成模型（例如**变分自编码器 (VAE)** 或 **GANs**）中。具体含义可以是：

- $Z \sim P_Z(Z)$ 表示从先验分布 \(P_Z(Z)\) 中抽取潜在变量 \(Z\)。
- 然后使用生成模型根据 \(Z\) 和模型参数 \(\theta_g\) 生成观测数据 \(x\)，即 \(P(x ; \theta_g)\)。

因此，整个表达式可以解释为：**从潜在分布 \(P_Z(Z)\) 中采样 \(Z\)，并通过生成分布 \(P(x ; \theta_g)\) 来生成观测数据 \(x\)。**

这种方法通常用于深度生成模型（如VAE）中，来描述从潜在空间生成数据的过程。

如果你有更详细的背景或任务说明，可以更具体地讨论这个概念。

![image-20241002093928249](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002093928249.png)

![image-20241002103134188](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241002103134188.png)



## 五、Bert 阅读

**BERT**



pre-training of deep bidirectional transformers for language understanding



自然语言中中近三年最重要的文章



在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能



但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，<font color=red>在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练</font>



BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃



### 1、标题 + 作者



**pre-training**：在一个数据集上训练好一个模型，这个模型主要的目的是用在另外一个任务上面，所以如果另外一个任务叫training的话，那么在大的数据集上训练的这个任务（模型）就叫做pre-training，即training之前的任务



**deep**：更深的神经网络



**bidirectional**：双向的



**transformers**：



**language understanding**：transformer主要是用在机器翻译这个小任务上，这里使用的是一个更加广义的词，就是对语言的理解



这篇文章是关于BERT模型，它是一个深的双向的transformer，是用来做预训练的，针对的是一般的语言的理解任务



作者来自**Google AI**语言团队



### 2、摘要



BERT是一个新的语言表示模型，BERT的名字来自于：

- Bidirectional
- Encoder
- Representation
- Transformer

它的意思是transformer这个模型双向的编码器表示，这四个词跟标题是不一样的



**它的想法是基于ELMo**

- ELMo来自于芝麻街中人物的名字，芝麻街是美国的一个少儿英语学习节目
- BERT是芝麻街中另外一个主人公的名字
- 这篇文章和之前的ELMo开创了NLP的芝麻街系列文章



**BERT和最近的一些语言的表示模型有所不同**

- Peters 引用的是ELMo
- Radfor 引用的是GPT



**BERT和ELMo、GPT的区别：**

- **BERT是设计用来训练深的双向表示，使用的是没有标号的数据，再联合左右的上下文信息**
- 因为这样的设计导致训练好的BERT只用加一个额外的输出层，就可以在很多NLP的任务（比如问答、语言推理）上面得到一个不错的结果，而且不需要对任务做很多特别的架构上的改动

GPT考虑的是单向（用左边的上下文信息去预测未来），BERT同时使用了左侧和右侧的信息，它是双向的（Bidirectional）

<font color=blue>ELMO用的是一个基于RNN的架构，BERT用的是transformer，所以ELMo在用到一些下游任务的时候需要对架构做一点点调整，但是BERT相对比较简单，和GPT一样只需要改最上层就可以了</font>



分别用两句话讲清楚了BERT和GPT、ELMo的区别，在摘要的前面讲清楚了文章和哪两个工作相关，以及和这两个工作的区别是什么，在一定程度上表示了本文的工作是基于上述的两个工作之上然后做了一些改动





**BERT的好处**

- 模型概念上更加简单而且效果更好。它在11个NLP的任务上得到了新的最好的结果（包括GLUE、MultiNLI、SQvAD v1.1、SQvAD v2.0等绝对精度都有一定的提升、相对好处（具体提升了多少））



摘要整体上有两段话，一段话是跟另外两篇相关工作的区别，第二段话是说结果特别好。先写改进再写结果比别人好在什么地方



### 3、导言





第一段一般是交代论文所研究方向的上下文关系



在语言模型中，**预训练可以用来提升很多自然语言的任务**



自然语言任务包括两类

- 句子层面的任务（sentence-level）：主要是用来建模句子之间的关系，比如说对句子的情绪识别或者两个句子之间的关系
- 词元层面的任务（token-level）：包括实体命名的识别（对每个词识别是不是实体命名，比如说人名、街道名），这些任务需要输出一些细腻度的词元层面上的输出



预训练在NLP中已经流行了有一阵子了，在计算机视觉里面已经用了很多年了，同样的方法用到自然语言上面也不会很新，但是在介绍BERT的时候，很有可能会把NLP做预训练归功于BERT，BERT不是第一个提出来的而是BERT让这个方法出圈了让后面的研究者跟着做自然语言的任务



导言的第二段和之后一般是摘要的第一段的扩充版本



在使用预训练模型做特征表示的时候，一般有两类策略

- 一个策略是**基于特征**的，代表作是==ELMo==，对每一个下游的任务构造一个跟这个任务相关的神经网络，它使用的==RNN==的架构，然后将预训练好的这些表示（比如说词嵌入也好，别的东西也好）作为一个额外的特征和输入一起输入进模型中，希望这些特征已经有了比较好的表示，所以导致模型训练起来相对来说比较容易，这也是NLP中使用预训练模型最常用的做法（把学到的特征和输入一起放进去作为一个很好的特征表达）
- 另一个策略是**基于微调**的，这里举的是GPT的例子，就是把预训练好的模型放在下游任务的时候不需要改变太多，只需要改动一点就可以了。这个模型预训练好的参数会在下游的数据上再进行微调（所有的权重再根据新的数据集进行微调）



介绍别人的方法的目的通常来讲是为了铺垫自己的方法，别人哪些地方做的不好，自己的方法有所改进



上述两个途径在预训练的时候都是使用一个相同的目标函数，都是使用一个单向的语言模型（给定一些词去预测下一个词是什么东西，说一句话然后预测这句话下面的词是什么东西，属于一个预测模型，用来预测未来，所以是单向的）



第三段讲述了本文的主要想法**：现在这些技术会有局限性，特别是做预训练的表征的时候，主要的问题是标准的语言模型是单向的，这样就导致在选架构的时候会有局限性**

- 在GPT中使用的是一个从左到右的架构（在看句子的时候只能从左看到右），这样的坏处在于如果要做句子层面的分析的话，比如说要判断一个句子层面的情绪是不是对的话，从左看到右和从右看到左都是合法的，另外，就算是词元层面上的一些任务，比如QA的时候也是看完整个句子再去选答案，而不是一个一个往下走

因此如果将两个方向的信息都放进去的话，应该是能够提升这些任务的性能的



在指出了相关工作的局限性和提出了自己的想法之后，接下来就开始讲作者是如何解决这个问题的：提出了BERT，<font color=red>BERT是用来减轻之前提到的语言模型是一个单向的限制，使用的是一个带掩码的语言模型（masked language model）MLM，这个语言模型是受Cloze任务的启发</font>（引用了一篇1953年的论文）

- 这个带掩码的语言模型每一次随机地选一些资源，然后将它们盖住，目标函数就是预测被盖住的字，等价于将一个句子挖一些空完后进行完形填空
- 跟标准的语言模型从左看到右的不同之处在于：带掩码的语言模型是允许看到左右的信息的（相当于看完形填空的时候不能只看完形填空的左边，也需要看完形填空的右边），<font color=blue>这样的话它允许训练深的双向的transformer模型</font>
- 在带掩码的语言模型之外还训练了一个任务，预测下一个句子，核心思想是给定两个句子，然后判断这两个句子在原文里面是相邻的，还是随机采样得来的，这样就让模型学习了句子层面的信息



**这篇文章的贡献**

1. 展示了双向信息的重要性，**GPT只用了单向，之前有的工作只是很简单地把一个从左看到右的语言模型和一个从右看到左的语言模型简单地合并到一起，类似于双向的RNN模型（contact到一起），这个模型在双向信息的应用上更好**
2. 假设有一个比较好的预训练模型就不需要对特定任务做特定的模型改动。BERT是第一个在一系列的NLP任务上（包括在句子层面上和词元层面上的任务）都取得了最好的成绩的基于微调的模型
3. 代码和模型全部放在：https://github.com/google-research/bert



### 4、结论



最近一些实验表明，使用无监督的预训练是非常好的，这样使得资源不多（训练样本比较少的任务也能够享受深度神经网络）,本文主要的工作就是把前人的工作扩展到深的双向的架构上，使得同样的预训练模型能够处理大量的不同的自然语言任务



简单概括一下：本文之前的两个工作一个叫**ELMo**，它使用了双向的信息但是它网络架构比较老，用的是RNN，另外一个工作是**GPT**，它用的是transformer的架构，但是它只能处理单向的信息，因此本文将ELMo双向的想法和GPT的transformer架构结合起来就成为了BERT

- 具体的改动是在做语言模型的时候不是预测未来，而是变成完形填空



很多时候我们的工作就是把两个东西缝合到一起，或者把一个技术用来解决另外领域的问题，如果所得到的东西确实简单好用，别人也愿意使用，就朴实地将它写出来也没有问题



### 5、相关工作



**非监督的基于特征的一些工作**

- 词监督
- ELMo



**非监督的基于微调的一些工作**

- 代表作是GPT



**在有标号的数据上做迁移学习**

- 在NLP中有标号而且比较大的数据（包括自然语言的推理和机器翻译这两块中都有比较大的数据集）
- 然后在这些有标号的数据集上训练好了模型然后在别的任务上使用

在计算机视觉中这一块使用比较多：经常在ImageNet上训练好模型再去别的地方使用，但是在NLP这一块不是特别理想（可能一方面是因为这两个任务跟别的任务差别还是挺大的，另一方面可能是因为数据量还是远远不够的），BERT和他后面的一系列工作证明了在NLP上面使用没有标号的大量数据集训练成的模型效果比在有标号的相对来说小一点的数据集上训练的模型效果更好，同样的想法现在也在慢慢地被计算机视觉采用，就是说在大量的没有标号的图片上训练出的模型也可能比在ImageNet这个100万数据集上训练的模型可能效果更好

### 6、BERT模型



主要介绍了实现的一些细节



BERT中有两个步骤：

- **预训练**：**在预训练中，这个模型是在一个没有标号的数据集上训练的**
- **微调**：在微调的时候同样是用一个BERT模型，但是它的权重被初始化成在预训练中得到的权重，所有的权重在微调的时候都会参与训练，用的是有标号的数据



每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早预训练好的BERT模型作为初始化，但是每个下游任务都会根据自己的数据训练好自己的模型



虽然预训练和微调不是BERT独创的，在计算机视觉中用的比较多，但是作者还是做了一个简单的介绍（在写论文的时候遇到一些技术需要使用的时候，而且可能应该所有人都知道，最好不要一笔带过，论文是需要自洽的，后面的人读过来可能不太了解这些技术，但是这些技术又是论文中方法不可缺少的一部分的话，最好还是能够做一个简单的说明）





下图中左图表示预训练，右图表示微调

![img](https://i0.hdslb.com/bfs/note/576a684529ef23d69c4ad806b7c4e81c9d4f19af.png@630w_!web-note.webp)

- ==预训练的时候输入是一些没有标号的句子对==
- 这里是在一个没有标号的数据上训练出一个BERT模型，把他的权重训练好，对下游的任务来说，对每个任务创建一个同样的BERT模型，但是它的权重的初始化值来自于前面预训练训练好的权重，对于每一个任务都会有自己的有标号的数据，然后对BERT继续进行训练，这样就得到了对于某一任务的BERT版本





**模型架构**

![img](https://i0.hdslb.com/bfs/note/2850d9e7574d552db3406e49b4e946a6fef775d8.png@630w_!web-note.webp)

BERT模型就是一个多层的双向transformer编码器，而且它是直接基于原始的论文和它原始的代码，没有做改动

三个参数

- L：transformer块的个数
- H：隐藏层的大小
- A：自注意力机制中多头的头的个数

两个模型

- BERT base：它的选取是使得跟GPT模型的参数差不多，来做一个比较公平的比较
- BERT large：用来刷榜
- BERT中的模型复杂度和层数是一个线性关系，和宽度是一个平方的关系



**怎样把超参数换算成可学习参数的大小**

模型中可学习参数主要来自两块

- 嵌入层：就是一个矩阵，输入是字典的大小（假设是30k），输出等于隐藏单元的个数（假设是H）
- transformer块：==transformer中有两部分：一个是自注意力机制（它本身是没有可学习参数的，但是对多头注意力的话，他会把所有进入的K、V、Q分别做一次投影，每一次投影的维度是等于64的，因为有多个头，头的个数A乘以64得到H，所以进入的话有key、value、q，他们都有自己的投影矩阵，这些投影矩阵在每个头之间合并起来其实就是H*H的矩阵了，同样道理拿到输出之后也会做一次投影，他也是一个H*H的矩阵，所以对于一个transformer块，他的自注意力可学习的参数是H的平方乘以4），一个是后面的MLP（MLP里面需要两个全连接层，第一个层的输入是H，但是它的输出是4*H，另外一个全连接层的输入是4*H，输出是H，所以每一个矩阵的大小是H*4H，两个矩阵就是H的平方乘以8），这两部分加起来就是一个transformer块中的参数，还要乘以L（transformer块的个数）==

所以总参数的个数就是30k乘以H（这部分就是嵌入层总共可以学习的参数个数）再加上L层乘以H的平方再乘以12

![img](https://i0.hdslb.com/bfs/note/ff210a3cba166eaecd3db56e6242a725a3e45994.png@630w_!web-note.webp)





**输入和输出**

对于下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使BERT模型能够处理所有的任务，它的输入既可以是一个句子，也可以是一个句子对

- 这里的一个句子是指一段连续的文字，不一定是真正的语义上的一段句子
- 输入叫做一个序列，可以是一个句子，也可以是两个句子
- 这和之前文章里的transformer是不一样的：transformer在训练的时候，他的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是BERT只有一个编码器，所以为了使它能够处理两个句子，就需要把两个句子变成一个序列





**序列的构成**

<font color=green>这里使用的切词的方法是WordPiece</font>，核心思想是：

- 假设按照空格切词的话，一个词作为一个token，因为数据量相对比较大，所以会导致词典大小特别大，可能是百万级别的，那么根据之前算模型参数的方法，如果是百万级别的话，就导致整个可学习参数都在嵌入层上面
- WordPiece是说假设一个词在整个里面出现的概率不大的话，那么应该把它切开看它的一个子序列，它的某一个子序列很有可能是一个词根，这个词很有可能出现的概率比较大话，那么就只保留这个子序列就行了。这样的话，可以把一个相对来说比较长的词切成很多一段一段的片段，而且这些片段是经常出现的，这样的话就可以用一个相对来说比较小的词典就能够表示一个比较大的文本了

切好词之后如何将两个句子放在一起

- 序列的第一个词永远是一个特殊的记号[CLS]，CLS表示classification，这个词的作用是BERT希望最后的输出代表的是整个序列的信息（比如说整个句子层面的信息），因为BERT使用的是transformer的编码器，所以它的自注意力层中每一个词都会去看输出入中所有词的关系，就算是词放在第一的位置，它也是有办法能够看到之后的所有词

---

**[CLS]**：用于分类任务的特殊标记，出现在输入序列的第一个位置。



在 BERT 中，**[CLS]** 是一个专门为==分类任务设计的标记==。当输入文本经过 BERT 编码器时，BERT 会生成每个词（token）对应的隐藏层向量，**[CLS]** 的输出向量被认为是对整个输入序列（句子或段落）的综合表示。这个表示向量可以用于处理下游任务，如文本分类、情感分析、问答系统等。



​	•	**位置**：在 BERT 的输入序列中，**[CLS]** 始终位于第一个位置，无论输入的是一个句子还是两个句子。

​	•	**输出向量**：在 BERT 模型的最后一层，**[CLS]** 对应的向量被用作输入序列的全局表示，用于下游任务的决策。这个向量可以被送入一个全连接层进行分类或其他任务。



<font color=red>**[CLS]** 对应的输出向量被视为整个输入序列的综合表示</font>

---

把两个句子合在一起，但是因为要做句子层面的分类，所以需要区分开来这两个句子，这里有两个办法：

1. 在每一个句子后面放一个特殊的词：[SEP]表示separate
2. 学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子

下图中红线画出来的粉色方框表示输入的序列，[CLS]是第一个特殊的记号表示分类，中间用一个特殊的记号[SEP]分隔开，每一个token进入BERT得到这个token的embedding表示（对BERT来讲，就是输入一个序列，然后得到另外一个序列），最后transformer块的输出就表示这个词元的BERT表示，最后再添加额外的输出层来得到想要的结果

![img](https://i0.hdslb.com/bfs/note/4135badc3a7e7d87d7a74b989c37d29da545e14b.png@630w_!web-note.webp)





对于每一个词元进入BERT的向量表示，它是这个词元本身的embedding加上它在哪一个句子的embedding再加上位置的embedding，如下图所示

![img](https://i0.hdslb.com/bfs/note/42f0d13adc6530b7cae7e2e5018affdef470729b.png@630w_!web-note.webp)

- 上图演示的是BERT的嵌入层的做法，即由一个词元的序列得到一个向量的序列，这个向量的序列会进入transformer块
- 上图中每一个方块是一个词元
- token embedding：这是一个正常的embedding层，对每一个词元输出它对应的向量
- segment embedding：表示是第一句话还是第二句话
- position embedding：输入的大小是这个序列的最大长度，它的输入就是每个词元这个序列中的位置信息（从零开始），由此得到对应的位置的向量
- 最终就是每个词元本身的嵌入加上在第几个句子的嵌入再加上在句子中间的位置嵌入
- 在transformer中，位置信息是手动构造出来的一个矩阵，但是在BERT中不管是属于哪个句子，还是具体的位置，它对应的向量表示都是通过学习得来的





**预训练和微调的不同之处**

在预训练的时候，主要有两个东西比较关键

- 目标函数
- 用来做预训练的数据



**带掩码的语言模型**

对于输入的词元序列，如果词元序列是由WordPiece生成的话，那么它有15%的概率会随机替换成掩码，但是对于特殊的词元（第一个词元和中间的分割词元不做替换），如果输入序列长度是1000的话，那么就要预测150个词

这里也会存在问题：因为在做掩码的时候会把词元替换成一个特殊的token（[MASK]），在训练的时候大概会看到15%的词元，但是在微调的时候是没有的，因为在微调的时候不用这个目标函数，所以没有mask这个东西，导致在预训练和微调的时候所看到的数据会有多不同

- 解决方法：对这15%的被选中作为掩码的词有80%的概率是真的将它替换成这个特殊的掩码符号（[MASK]），还有10%的概率将它替换成一个随机的词元（其实是加入了一些噪音），最后有10%的概率什么都不干，就把它存在那里用来做预测（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/397b25bd813702907a728a46ba47747f263c8c97.png@630w_!web-note.webp)





**预训练中的第二个任务就是预测下一个句子**

在QA和自然语言推理中都是句子对，如果让它学习一些句子层面的信息也不错，具体来说，一个输入序列里面有两个句子：a和b，有50的概率b在原文中间真的是在a的后面，还有50%的概率b就是随机从别的地方选取出来的句子，这就意味着有50%的样本是正例（两个句子是相邻的关系），50%的样本是负例（两个句子没有太大的关系），加入这个目标函数能够极大地提升在QA和自然语言推理的效果（附录中有例子）

![img](https://i0.hdslb.com/bfs/note/8e4c09357f2fb5173215cc5b91710b19e6dbfdbc.png@630w_!web-note.webp)

- 上图中高亮部分的##：在原文中 flightless 是一个词，但是由于这个词出现的概率不高，所以在WordPiece中把它砍成了两个词 flight 和 less ，他们都是比较常见的词，##表示后面的词在原文中其实是跟在前面那个词后面的意思





**预训练数据**

![img](https://i0.hdslb.com/bfs/note/df1a2fac07b73131eccdc47b80795d118ead5ce0.png@630w_!web-note.webp)

使用了两个数据集

- BooksCorpus
- English Wikipedia

应该使用文本层面的数据集，即数据集里面是一篇一篇的文章而不是一些随机打乱的句子，因为transformer确实能够处理比较长的序列，所以对于整个文本序列作为数据集效果会更好一些





**用BERT做微调**

BERT和一些基于编码器解码器的架构有什么不同

- transformer是编码器解码器架构
- 因为把整个句子对都放在一起放进去了，所以自注意力能够在两端之间相互能够看到，但是在编码器解码器这个架构中，编码器一般是看不到解码器的东西的，所以BERT在这一块会更好一点，但是实际上也付出了一定的代价（不能像transformer一样能够做机器翻译）



在做下游任务的时候会根据任务设计任务相关的输入和输出，好处是模型其实不需要做大的改动，主要是怎么样把输入改成所要的那个句子对

- 如果真的有两个句子的话就是句子a和b
- 如果只有一个句子的话，比如说要做一个句子的分类，b就没有了

然后根据下游的任务要求，要么是拿到第一个词元对应的输出做分类或者是拿到对应的词元的输出做所想要的输出，不管怎么样都是在最后加一个输出层，然后用一个softnax得到想要的标号



跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU跑一个小时就可以了，使用GPU的话多跑几个小时也行



### 7、实验



介绍了BERT怎么样用在各个下游任务上



**GLUE**

它里面包含了多个数据集，是一个句子层面的任务

BERT就是把第一个特殊词元[CLS]的最后的向量拿出来，然后学习一个输出层w，放进去之后用softmax就能得到标号，这就变成了一个很正常的多分类问题了

下图表示了在这个分类任务上的结果

![img](https://i0.hdslb.com/bfs/note/30a0222074f444f72f042dfdeb0f05e43c3e3f9d.png@630w_!web-note.webp)

- average表示在所有数据集上的平均值，它表示精度，越高越好
- 可以发现就算是BERT就算是在base跟GPT可学习参数差不多的情况下，也还是能够有比较大的提升



**SQuAD v1.1**

斯坦福的一个QA数据集

QA任务是说给定一段话，然后问一个问题，需要在这段话中找出问题的答案（类似于阅读理解），答案在给定的那段话中，只需要把答案对应的小的片段找出来就可以了（找到这个片段的开始和结尾）

- 就是对每个词元进行判断，看是不是答案的开头或者答案的结尾

具体来说就是学两个向量S和E，分别对应这个词元是答案开始的概率和答案最后的概率，它对每个词元（也就是第二句话中每个词元）的S和Ti相乘，然后再做softmax，就会得到这个段中每一个词元是答案开始的概率，公式如下图所示，同理也可以得出是答案末尾的概率

![img](https://i0.hdslb.com/bfs/note/7cca66ff2bb525232c965370ccf17888bd7ace21.png@630w_!web-note.webp)

- Ti表示第 i 个输入词元对应的最后一个隐藏向量



**在做微调的时候的参数设置**

- 使用了3个epoch，扫描了3遍数据
- 学习率是5e-5
- batchsize是32

用BERT做微调的时候结果非常不稳定，同样的参数、同样的数据集，训练十遍，可能会得到不同的结果。最后发现3其实是不够的，可能多学习几遍会好一点



BERT用的优化器是adam的不完全版，当BERT要训练很长时间的时候是没有影响的，但是如果BERT只训练一小段时间的话，它可能会带来影响（将这个优化器换成adam的正常版就可以解决这个问题了）



**SQuAD v2.0**



**SWAG数据集**

它用来判断两个句子之间的关系

跟之前的训练没有太多区别，BERT的结果比别的模型要好很多





对这些不同的数据集，BERT基本上只要把这些数据集表示成所要的句子对的形式，最后拿到一个对应的输出然后再加一个输出层就可以了，所以BERT对整个NLP领域的贡献还是非常大的，大量的任务可以用一个相对来说比较简单的架构，不需要改太多的东西就能够完成了





**ablation study**



介绍了BERT中每一块最后对结果的贡献

![img](https://i0.hdslb.com/bfs/note/ec4e09e283db95be14d7c2037af29101264d5d16.png@630w_!web-note.webp)

- No NSP：假设去掉对下一个句子的预测
- LTR & No NSP：使用一个从左看到右的单向的语言模型（而不是用带掩码的语言模型），然后去掉对下一个句子的预测
-  \+ BiLSTM：在上面加一个双向的LSTM

从结果来看，去掉任何一部分，结果都会打折扣





**模型大小的影响**



BERT base中有1亿的可学习参数

BERT large中有3亿可学习的参数

相对于之前的transformer，可学习参数数量的提升还是比较大的

当模型变得越来越大的时候，效果会越来越好，这是第一个展示将模型变得特别大的时候对语言模型有较大提升的工作

虽然现在GPT3已经做到1000亿甚至在向万亿级别发展，但是在三年前，BERT确实是开创性地将一个模型推到如此之大，引发了之后的模型大战





假设不用BERT做微调而是把BERT的特征作为一个静态特征输进去会怎样

结论是效果确实没有微调好，所有用BERT的话应该用微调



**8、评论**



**写作**

- 先写了BERT和GPT的区别
- 然后介绍了BERT模型
- 接下来是在各个实验上的设置
- 最后对比结果，结果非常好

这篇文章认为本文的最大贡献就是双向性（写文章最好能有一个明确的卖点，有得有失，都应该写出来）

- 但是今天来看，这篇文章的贡献不仅仅只有双向性，还有其它东西
- 从写作上来说，至少要说选择双向性所带来的不好的地方是什么，做一个选择，会得到一些东西，也会失去一些东西：和GPT比，BERT用的是编码器，GPT用的是解码器，得到了一些好处，但是也有坏处（比如做机器翻译和文本摘要比较困难，做生成类的东西就没那么方便了）
- 分类问题在NLP中更加常见，所以NLP的研究者更喜欢用BERT，会更容易一些



BERT所提供的是一个完整的解决问题的思路，符合了大家对于深度学习模型的期望：在一个很大的数据集上训练好一个很深很宽的模型，这个模型拿出来之后可以用在很多小问题上，通过微调可以全面提升这些小数据上的性能

### 8、推荐拓展阅读



- [Bert输入部分详细解读_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ey4y1874y/?p=2&spm_id_from=pageDriver&vd_source=81e5007efea018d7c2e8c28374fcdf34)



- [图解BERT_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1uF411Y7F7/?spm_id_from=333.337.search-card.all.click&vd_source=81e5007efea018d7c2e8c28374fcdf34)





NSP  任务  用于处理两个句子之间的任务 Next Sentence Prediction

MLM：Mask Language Model



## 六、VIT  阅读





**Vit**



**an image is worth 16\*16 words:transformers for image recognition at scale**



过去一年在计算机视觉领域影响力最大的工作

- <font color=red>它挑战了自从2012年AlexNet提出以来卷积神经网络在计算机视觉里绝对统治的地位</font>
- **结论：如果在足够多的数据上做预训练，也可以不需要卷积神经网路，直接使用标准的transformer也能够把视觉问题解决的很好**
- *它打破了CV和NLP在模型上的壁垒，开启了CV的一个新时代*



papers with code

- 可以查询现在某个领域或者说某个数据集表现最好的一些方法有哪些
- 图像分类在ImageNet数据集上排名靠前的全是基于Vision Transformer

![img](https://i0.hdslb.com/bfs/note/c23dff43fa425da9d6961166754c100c3fad3c80.png@712w_!web-note.webp)

- 对于目标检测任务在COCO数据集上，排名靠前都都是基于Swin Transformer（Swin Transformer是ICCV 21的最佳论文，可以把它想象成一个多尺度的Vit（Vision Transformer））
- 在其他领域（语义分割、实例分割、视频、医疗、遥感），基本上可以说Vision Transformer将整个视觉领域中所有的任务都刷了个遍





另一篇论文

**Intriguing Properties of Vision Transformer**

Vision Transformer一些有趣的特性



下图中展示了一些在卷积神经网络CNN中工作得不太好但是用Vision Transformer都能处理的很好的例子

![img](https://i0.hdslb.com/bfs/note/23434d4d1b30827a946f257d7c9851ba7262b40d.png@712w_!web-note.webp)

- a表示的是遮挡，在这么严重的遮挡情况下，不管是卷积神经网络，人眼也很难观察出图中所示的是一只鸟
- b表示数据分布上有所偏移，这里对图片做了一次纹理去除的操作，所以图片看起来比较魔幻
- **c表示在鸟头的位置加了一个对抗性的patch**
- d表示将图片打散了之后做排列组合

上述例子中，卷积神经网络很难判断到底是一个什么物体，**但是对于所有的这些例子Vision Transformer都能够处理的很好**



### 1、标题 + 作者



一张图片等价于很多16*16大小的单词

- 为什么是16*16的单词？将图片看成是很多的patch，假如把图片分割成很多方格的形式，每一个方格的大小都是16*16，那么这张图片就相当于是很多16*16的patch组成的整体



使用transformer去做大规模的图像识别



作者团队来自于google research和google brain team



### 2、摘要



虽然说transformer已经是NLP（自然语言处理）领域的一个标准：BERT模型、GPT3或者是T5模型，但是用transformer来做CV还是很有限的



在视觉领域，<font color=red>自注意力要么是跟卷积神经网络一起使用，要么用来把某一些卷积神经网络中的卷积替换成自注意力，但是还是保持整体的结构不变</font>

- 这里的整体结构是指：比如说对于一个残差网络（Res50），它有4个stage：res2、res3、res4、res5，上面说的整体结构不变指的就是这个stage是不变的，它只是去取代每一个stage、每一个block的操作



这篇文章证明了这种对于卷积神经网络的依赖是完全不必要的，一个纯的Vision Transformer直接作用于一系列图像块的时候，也是可以在图像分类任务上表现得非常好的，尤其是当在大规模的数据上面做预训练然后迁移到中小型数据集上面使用的时候，Vision Transformer能够获得跟最好的卷积神经网络相媲美的结果



这里将ImageNet、CIFAR-100、VATB当作中小型数据集

- 其实ImageNet对于很多人来说都已经是很大的数据集了



Transformer的另外一个好处：它只需要更少的训练资源，而且表现还特别好

- 作者这里指的少的训练资源是指2500天TPUv3的天数
- 这里的少只是跟更耗卡的模型去做对比（类似于一个小目标）



### 3、引言



自注意力机制的网络，尤其是Transformer，已经是自然语言中的必选模型了，现在比较主流的方式，就是先去一个大规模的数据集上去做预训练，然后再在一些特定领域的小数据集上面做微调（这个是在BERT的文章中提出来的）



得益于transformer的计算高效性和可扩展性，现在已经可以训练超过1000亿参数的模型了，比如说GPT3



随着模型和数据集的增长，目前还没有发现任何性能饱和的现象

- 很多时候不是一味地扩大数据集或者说扩大模型就能够获得更好的效果的，==尤其是当扩大模型的时候很容易碰到过拟合的问题，但是对于transformer来说目前还没有观测到这个瓶颈==
- 最近微软和英伟达又联合推出了一个超级大的语言生成模型Megatron-Turing，它已经有5300亿参数了，还能在各个任务上继续大幅度提升性能，没有任何性能饱和的现象





回顾transformer

06:17



- transformer中最主要的操作就是自注意力操作，自注意力操作就是每个元素都要跟每个元素进行互动，两两互相的，然后算得一个attention（自注意力的图），用这个自注意力的图去做加权平均，最后得到输出
- 因为在做自注意力的时候是两两互相的，这个计算复杂度是跟序列的长度呈平方倍的。
- 目前一般在自然语言处理中，硬件能支持的序列长度一般也就是几百或者是上千（比如说BERT的序列长度也就是512）







**将transformer运用到视觉领域的<font size=6>难处</font>**



首先要解决的是如何把一个2D的图片变成一个1D的序列（或者说变成一个集合）。**最直观的方式就是把每个像素点当成元素，将图片拉直放进transformer里，看起来比较简单，但是实现起来复杂度较高**。

- 一般来说在视觉中训练分类任务的时候图片的输入大小大概是224x224，如果将图片中的每一个像素点都直接当成元素来看待的话,他的序列长度就是224x24=50176个像素点，也就是序列的长度，这个大小就相当于是BERT序列长度的100倍，这还仅仅是分类任务，对于检测和分割，现在很多模型的输入都已经变成600x600或者800x800或者更大，计算复杂度更高，所以在视觉领域，卷积神经网络还是占主导地位的，比如AlexNet或者是ResNet



所以现在很多工作就是在研究如**何将自注意力用到机器视觉中：一些工作是说把卷积神经网络和自注意力混到一起用；另外一些工作就是整个将卷积神经网络换掉，全部用自注意力。这些方法其实都是在干一个事情：因为序列长度太长，所以导致没有办法将transformer用到视觉中，所以就想办法降低序列长度**



Wang et al.,2018：既然用像素点当输入导致序列长度太长，就可以不用图片当transformer的直接输入，可以把网络中间的特征图当作transformer的输入

- 假如用残差网络Res50，==其实在它的最后一个stage，到res4的时候的featuremap的size其实就只有14*14了==，再把它拉平其实就只有196个元素了，即这个序列元素就只有196了，这就在一个可以接受的范围内了。所以就通过用特征图当作transformer输入的方式来降低序列的长度



Wang et al.,2019;Wang et al.,2020a（Stand-Alone Attention&Axial Attention，孤立自注意力和轴自注意力）

- **孤立自注意力**：之所以视觉计算的复杂度高是来源于使用整张图，所以不使用整张图，就用一个local window（局部的小窗口），这里的复杂度是可以控制的（通过控制这个窗口的大小，来让计算复杂度在可接受的范围之内）。==这就类似于卷积操作（卷积也是在一个局部的窗口中操作的）==
- **轴自注意力**：之所以视觉计算的复杂度高是因为序列长度N=H*W，是一个2D的矩阵，将图片的这个2D的矩阵想办法拆成2个1D的向量，所以先在高度的维度上做一次self-attention（自注意力），然后再在宽度的维度上再去做一次自注意力，相当于把一个在2D矩阵上进行的自注意力操作变成了两个1D的顺序的操作，这样大幅度降低了计算的复杂度



最近的一些模型，这种方式虽然理论上是非常高效的，但事实上因为这个自注意力操作都是一些比较特殊的自注意力操作，所以说无法在现在的硬件上进行加速，所以就导致很难训练出一个大模型，所以截止到目前为止，孤立自注意力和轴自注意力的模型都还没有做到很大，跟百亿、千亿级别的大transformer模型比还是差的很远，因此在大规模的图像识别上，传统的残差网络还是效果最好的



所以，自注意力早已经在计算机视觉里有所应用，而且已经有完全用自注意力去取代卷积操作的工作了，所以本文换了一个角度来讲故事



本文是被transformer在NLP领域的可扩展性所启发，本文想要做的就是直接应用一个标准的transformer直接作用于图片，尽量做少的修改（不做任何针对视觉任务的特定改变），看看这样的transformer能不能在视觉领域中扩展得很大很好



但是如果直接使用transformer，还是要解决序列长度的问题

- vision transformer将一张图片打成了很多的patch，每一个patch是16*16
- 假如图片的大小是224x224，则sequence lenth（序列长度）就是N=224x224=50176，如果换成patch，一个patch相当于一个元素的话，有效的长宽就变成了224/16=14，所以最后的序列长度就变成了N=14*14=196，所以现在图片就只有196个元素了，196对于普通的transformer来说是可以接受的
- 然后将每一个patch当作一个元素，通过一个fc layer（全连接层）就会得到一个linear embedding，这些就会当作输入传给transformer，这时候一张图片就变成了一个一个的图片块了，可以将这些图片块当成是NLP中的单词，一个句子中有多少单词就相当于是一张图片中有多少个patch，这就是题目中所提到的一张图片等价于很多16*16的单词



<font color=red>本文训练vision transformer使用的是==有监督的训练==</font>

- <font color=red>为什么要突出有监督？因为对于NLP来说，transformer基本上都是用无监督的方式训练的，要么是用language modeling，要么是用mask language modeling，都是用的无监督的训练方式但是对于视觉来说，大部分的基线（baseline）网络还都是用的有监督的训练方式去训练的</font>



到此可以发现，本文确实是把视觉当成自然语言处理的任务去做的，尤其是中间的模型就是使用的transformer encoder，跟BERT完全一样，这篇文章的目的是说使用一套简洁的框架，transformer也能在视觉中起到很好的效果



这么简单的想法，之前其实也有人想到过去做，本文在相关工作中已经做了介绍，跟本文的工作最像的是一篇ICLR 2020的paper

- 这篇论文是从输入图片中抽取2*2的图片patch
- 为什么是2x2？因为这篇论文的作者只在CIFAR-10数据集上做了实验，而CIFAR-10这个数据集上的图片都是32x32的，所以只需要抽取2*2的patch就足够了，16*16的patch太大了
- 在抽取好patch之后，就在上面做self-attention

从技术上而言他就是Vision Transformer，但是本文的作者认为二者的区别在于本文的工作证明了如果在大规模的数据集上做预训练的话（和NLP一样，在大规模的语料库上做预训练），那么就能让一个标准的Transformer，不用在视觉上做任何的更改或者特殊的改动，就能取得比现在最好的卷积神经网络差不多或者还好的结果，同时本文的作者还指出之前的ICLR的这篇论文用的是很小的2x2的patch，所以让他们的模型只能处理那些小的图片，而Vision Transformer是能够处理224*224这种图片的



所以这篇文章的主要目的就是说，Transformer在Vision领域能够扩展的有多好，就是在超级大数据集和超级大模型两方的加持下，Transformer到底能不能取代卷积神经网络的地位



一般引言的最后就是将最想说的结论或者最想表示的结果放出来，这样读者不用看完整篇论文就能知道文章的贡献有多大



本文在引言的最后说在中型大小的数据集上（比如说ImageNet）上训练的时候，如果不加比较强的约束，Vit的模型其实跟同等大小的残差网络相比要弱一点

- 作者对此的解释是：这个看起来不太好的结果其实是可以预期的，因为transformer跟卷积神经网路相比，**它缺少了一些卷积神经网络所带有的归纳偏置**
- 这里的归纳偏置其实是指一种先验知识或者说是一种提前做好的假设



对于卷积神经网络来说，常说的有两个inductive bias（归纳偏置）：

- locality：因为卷积神经网络是以滑动窗口的形式一点一点地在图片上进行卷积的，所以假设图片上相邻的区域会有相邻的特征，靠得越近的东西相关性越强
- translation equivariance（平移等变性或平移同变性）：f(g(x))=g(f(x))，就是说不论是先做g这个函数，还是先做f这个函数，最后的结果是不变的。这里可以把f理解成卷积，把g理解成平移操作，意思是说无论是先做平移还是先做卷积，最后的结果都是一样的（因为在卷积神经网络中，卷积核就相当于是一个模板，不论图片中同样的物体移动到哪里，只要是同样的输入进来，然后遇到同样的卷积核，那么输出永远是一样的）



一旦神经网络有了这两个归纳偏置之后，他就拥有了很多的先验信息，所以只需要相对较少的数据来学习一个相对比较好的模型，但是对于transformer来说，它没有这些先验信息，所以它对视觉的感知全部需要从这些数据中自己学习

- 为了验证这个假设，作者在更大的数据集（14M-300M）上做了预训练，这里的14M是ImageNet 22k数据集，300M是google自己的JFT 300M数据集，然后发现大规模的预训练要比归纳偏置好



Vision Transformer只要在有足够的数据做预训练的情况下，就能在下游任务上取得很好的迁移学习效果。具体来说，就是当在ImageNet 21k上或者在JFT 300M上训练，Vit能够获得跟现在最好的残差神经网络相近或者说更好的结果，如下图所示

![img](https://i0.hdslb.com/bfs/note/dd370b2e1b3a909ea2b1ee3e936ff380c360bdc4.png@700w_!web-note.webp)

- VTAB也是作者团队所提出来的一个数据集，融合了19个数据集，主要是用来检测模型的稳健性，从侧面也反映出了Vision Transformer的稳健性也是相当不错的



总的来说，引言写的简洁明了

- 第一段先说因为Transformer在NLP中扩展的很好，越大的数据或者越大的模型，最后performance会一直上升，没有饱和的现象，然后提出：如果将Transformer使用到视觉中，会不会产生同样的效果
- 第二段开始讲前人的工作，讲清楚了自己的工作和前人工作的区别：之前的工作要么就是把卷积神经网络和自注意力结合起来，要么就是用自注意力去取代卷积神经网络，但是从来没有工作直接将transformer用到视觉领域中来，而且也都没有获得很好的扩展效果
- 第三段讲Vision Transformer就是用了一个标准的Transformer模型，只需要对图片进行预处理（把图片打成块），然后送到transformer中就可以了，而不需要做其他的改动，这样可以彻底地把一个视觉问题理解成是一个NLP问题，就打破了CV和NLP领域的壁垒
- 最后两段展示了结果，只要在足够多的数据做预训练的情况下，Vision Transformer能够在很多数据集上取得很好的效果



### 4、结论



这篇论文的工作是直接拿NLP领域中标准的Transformer来做计算机视觉的问题，跟之前用自注意力的那些工作的区别在于：

- **除了在刚开始抽图像块的时候，还有位置编码用了一些图像特有的归纳偏置**

除此之外就再也没有引入任何图像特有的归纳偏置了，这样的好处就是不需要对Vision领域有多少了解，可以直接把图片理解成一个序列的图像块，就跟一个句子中有很多单词一样，然后就可以直接拿NLP中一个标准的Transformer来做图像分类了



当这个简单而且扩展性很好的策略和大规模预训练结合起来的时候效果出奇的好：Vision Transformer在很多图像分类的benchmark上超过了之前最好的方法，而且训练起来还相对便宜



目前还没有解决的问题（对未来的展望）

如何用transformer来做cv

第一个问题：Vit不能只做分类，还有检测和分割

- DETR：去年目标检测的一个力作，相当于是改变了整个目标检测之前的框架



鉴于Vit和DETR良好的表现，所以作者说拿Vision Transformer做视觉的其他问题应该是没有问题的

- 事实上，在Vit出现短短的一个半月之后，2020年12月检测这块就出来了一个叫Vit-FRCNN的工作，就已经将Vit用到检测上面了
- 图像分割这一块也是一样的，同年12月就有一篇SETR的paper将Vit用到分割里了
- 紧接着3个月之后Swin Transformer横空出世，它将多尺度的设计融合到了Transformer中，更加适合做视觉的问题了，真正证明了Transformer是能够当成一个视觉领域的通用骨干网络



另外一个未来的工作方向就是说要去探索一下==自监督的预训练方案==，因为在NLP领域，所有的大的transformer全都是用自监督的方式训练的，Vit这篇paper也做了一些初始实验，证明了用这种自监督的训练方式也是可行的，但是跟有监督的训练比起来还是有不小的差距的



最后作者说，继续将Vision Transformer变得更大，有可能会带来更好的结果

- 过了半年，同样的作者团队又出了一篇paper叫做Scaling Vision Transformer，就是将Transformer变得很大，提出了一个Vit-G,将ImageNet图像分类的准确率提高到了90以上了





 ### 5、相关工作





transformer在NLP领域的应用

- 自从2017年transformer提出做机器翻译以后，基本上transformer就是很多NLP任务中表现最好的方法。
- 现在大规模的transformer模型一般都是先在一个大规模的语料库上做预训练，然后再在目标任务上做一些细小的微调，这当中有两系列比较出名的工作：BERT和GPT。BERT是用一个denoising的自监督方式（**其实就是完形填空，将一个句子中某些词划掉，再将这些词预测出来**）；GPT用的是language modeling（已经有一个句子，然后去预测下一个词是什么，也就是next word prediction，预测下一个词）做自监督。这两个人物其实都是人为定的，语料是固定的，句子也是完整的，只是人为的去划掉其中的某些部分或者把最后的词拿掉，然后去做完形填空或者是预测下一个词，所以这叫自监督的训练方式



自注意力在视觉中的应用

- 视觉中如果想简单地在图片上使用自注意力，最简单的方式就是将每一个像素点当成是一个元素，让他们两两做自注意力就好了，但是这个是平方复杂度，所以很难应用到真实的图片输入尺寸上。像现在分类任务的224*224，一个transformer都很难处理，更不用提人眼看的比较清晰的图片了，一般是1k或者4k的画质，它们的序列长度都是上百万，直接在像素层面使用transformer的话不太现实，所以如果想用transformer就一定得做一些近似
- 复杂度高是因为用了整张图，所以序列长度长，那么可以不用整张图，就用local neighborhood（一个小窗口）来做自注意力，那么序列长度就大大降低了，最后的计算复杂度也就降低了
- **另外也可以使用Sparse Transformer，就是只对一些稀疏的点去做自注意力，所以只是一个全局注意力的近似**
- 还有一些方法就是将自注意力用到大小不同的block上，或者说在极端的情况下使用轴注意力（先在横轴上做自注意力，然后再在纵轴上做自注意力），序列长度也是大大减小的
- 这些特制的自注意力结构其实在计算机视觉上的结果都不错，表现都是没问题的，但是它们需要很复杂的工程去加速算子，虽然在CPU或者GPU上跑得很快或者说让训练一个大模型成为可能



跟本文工作最相似的是一篇ICLR2020的论文，区别在于Vision Transformer使用了更大的patch更大的数据集



在计算机视觉领域还有很多工作是把卷积神经网络和自注意力结合起来的，这类工作相当多，而且基本涵盖了视觉里的很多任务（检测、分类、视频、多模态等）



还有一个工作和本文的工作很相近，叫image GPT

- GPT是用在NLP中的，是一个生成性的模型
- image GPT也是一个生成性模型，也是用无监督的方式去训练的，它和Vit相近的地方在于它也用了transformer
- image GPT最终所能达到的效果：如果将训练好的模型做微调或者就把它当成一个特征提取器，它在ImageNet上的最高的分类准确率也只能到72，Vit最终的结果已经有88.5了，远高于72
- 但是这个结果也是最近一篇paper叫做MAE爆火的原因。*因为在BEiT和MAE这类工作之前生成式网络在视觉领域很多任务上是没有办法跟判别式网络相比的，判别式网络往往要比生成式网络的结果高很多，但是MAE做到了，它在ImageNet-1k数据集上训练，用一个生成式的模型，比之前判别式的模型效果好很多，而且不光是在分类任务上，最近发现在目标检测上的迁移学习的效果也非常好*



**Vit其实还跟另外一系列工作是有关系的，用比ImageNet更大的数据集去做预训练，这种使用额外数据的方式，一般有助于达到特别好的效果**

- 比如2017年介绍JFT 300数据集的那篇paper研究了卷积神经网络的效果是怎么随着数据集的增大而提高的
- 还有一些论文是研究了在更大的数据集（比如说ImageNet-21k和JFT 300M）上做预训练的时候迁移学习的效果会怎样，就是迁移到ImageNet或者CIFAR-100上的效果如何

这篇论文也是聚焦于ImageNet-21k和JFT 300M，但是训练的并不是一个残差网络，而失去训练transformer



本文的相关工作写的非常彻底，而且列举了很多跟本文工作最相近的，比如说ICLR 2020的论文、iGPT还有之前研究大数据集的BiT等



写相关工作这个章节的目的就是让读者知道在你的工作之前别人做了哪些工作，你和他们的区别在哪里。写清楚之后其实对论文本身是非常有利的，并不会降低论文的创新性，反而让整个文章变得更加简单易懂





### 6、ViT模型



在模型的设计上是尽可能按照最原始的transformer来做的，这样做的好处就是可以直接把NLP中比较成功的Transformer架构拿过来用，而不用再去对模型进行改动，而且因为transformer因为在NLP领域已经火了很久了，它有一些写的非常高效的实现，同样ViT也可以直接拿来使用



下图是模型的总览图，模型的总览图对论文来说是非常重要的，画的好的模型总览图能够让读者在不读论文的情况下，仅仅通过看图就能够知道整篇论文的大致内容

![img](https://i0.hdslb.com/bfs/note/01dd08af4754c81fc11f34a820acd55a43351153.png@700w_!web-note.webp)

- 首先给定一张图，先将这张图打成了很多的patch（如上图左下角所示），这里是将图打成了九宫格
- ==然后再将这些patch变成了一个序列，每个patch通过线性投射层的操作得到一个特征（就是本文中提到的patch embedding）==
- 自注意力是所有元素之间两两做交互，所以本身并不存在顺序的问题，但是对于图片来说，图片本身是一个整体，这个九宫格是有自己的顺序的，如果顺序颠倒了就不是原来的图片了。所以类似于NLP，给patch embedding加上了一个position embedding，==等价于加上了一个位置编码==
- 在加上这个位置编码信息之后，<font color=red>整体的token就既包含了图片块原本有的图像信息，又包含了这个图片块的所在位置信息</font>
- 在得到了这个token之后，接下来就跟NLP中完全一样了，直接将它们输入进一个Transformer encoder，然后Transformer encoder就会得到很多输出
- 这么多输出，应该拿哪个输出去做分类？这里借鉴了BERT，BERT中有一个extra learnable embedding，它是一个特殊字符CLS（分类字符），所以这里也添加了一个特殊的字符，用*代替，而且它也是有position embedding，它的位置信息永远是0，如下图红色圆圈所示

![img](https://i0.hdslb.com/bfs/note/9ac3c99c417479e91b4b7c6cfd3d1a9791a7de67.png@700w_!web-note.webp)

- 因为**所有的token都在跟其它token做交互信息**，所以作者相信，class embedding能够从别的序列后面的embedding中学到有用的信息，从而只需要根据class embedding的输出做最后的判断就可以了
- MLP Head其实就是一个通用的分类头
- 最后用交叉熵函数进行模型的训练



模型中的Transformer encoder是一个标准的Transformer，具体的结构如下图右图所示

![img](https://i0.hdslb.com/bfs/note/e428897f1841695df81dde4d425ea910ac02f366.png@700w_!web-note.webp)

- Transformer的输入是一些patch
- 一个Transformer block叠加了L次



整体上来看Vision Transformer的架构还是相当简洁的，它的特殊之处就在于如何把一个图片变成一系列的token



**具体的模型的前向过程**

- 假如说有一个224x224x3的图片X，如果使用16x16的patch size大小，就会得到196个图像块，每一个图像块的维度就是16x16x3=768，到此就把原先224x224x3的图片变成了196个patch，每个patch的维度是768
- 接下来就要将这些patch输入一个线性投射层，这个线性投射层其实就是一个全连接层（在文章中使用E表示），这个全连接层的维度是768x768，第二个768就是文章中的D，D是可以变的，如果transformer变得更大了，D也可以相应的变得更大，第一个768是从前面图像的patch算来的（16x16x3），它是不变的。
- 经过了线性投射就得到了patch embedding（X*E），它是一个196x768的矩阵（X是196x768，E是768x768），意思就是现在有196个token，每个token向量的维度是768
- 到目前为止就已经成功地将一个vision的问题变成了一个NLP的问题了，输入就是一系列1d的token，而不再是一张2d的图片了
- 除了图片本身带来的token以外，这里面加了一个额外的cls token，它是一个特殊的字符，只有一个token，它的维度也是768，这样可以方便和后面图像的信息直接进行拼接。所以最后整体进入Transformer的序列的长度是197*768（196+1：196个图像块对应的token和一个特殊字符cls token）
- 最后还要加上图像块的位置编码信息，这里是将图片打成了九宫格，所以位置编码信息是1到9，但是这只是一个序号，并不是真正使用的位置编码，==具体的做法是通过一个表（表中的每一行就代表了这些1到9的序号，每一行就是一个向量，向量的维度是768，这个向量也是可以学的==）得到位置信息，然后将这些位置信息加到所有的token中（注意这里是加，而不是拼接，序号1到9也只是示意一下，实际上应该是1到196），所以加上位置编码信息之后，这个序列还是197*768
- 到此就做完了整个图片的预处理，包括加上特殊的字符cls和位置编码信息，也就是说transformer输入的embedded patches就是一个197*768的tensor
- 这个tensor先过一个layer norm，出来之后还是197*768
- 然后做多头自注意力，这里就变成了三份：k、q、v，每一个都是197x768，这里因为做的是多头自注意力，所以其实最后的维度并不是768，假设现在使用的是VIsion Transformer的base版本，即多头使用了12个头，那么最后的维度就变成了768/12=64，也就是说这里的k、q、v变成了197*64，但是有12个头，有12个对应的k、q、v做自注意力操作，最后再将12个头的输出直接拼接起来，这样64拼接出来之后又变成了768，所以多头自注意力出来的结果经过拼接还是197*768
- 然后再过一层layer norm，还是197*768
- 然后再过一层MLP，这里会把维度先对应地放大，一般是放大4倍，所以就是197*3072
- 然后再缩小投射回去，再变成197*768，就输出了

![img](https://i0.hdslb.com/bfs/note/be3d91c3c79025f2700aa445a9dbf097bffcdaf0.png@700w_!web-note.webp)

- 以上就是一个Transformer block的前向传播的过程，进去之前是197*768，出来还是197*768，这个序列的长度和每个token对应的维度大小都是一样的，所以就可以在一个Transformer block上不停地往上叠加Transformer block，最后有L层Transformer block的模型就构成了Transformer encoder





3.1 Vision Transformer



38:22



![img](https://i0.hdslb.com/bfs/note/4e85307dcadfc5ca5356d974ab11768f4aa4b348.png@700w_!web-note.webp)

- Transformer从头到尾都是使用D当作向量的长度的，都是768，这个维度是不变的
- 对于位置编码信息，本文用的是标准的可以学习的1d position embedding，它也是BERT使用的位置编码。作者也尝试了了别的编码形式，比如说2d aware（它是一个能处理2d信息的位置编码），但是最后发现结果其实都差不多，没有什么区别





**消融实验（附录）**

针对特殊的class token还有位置编码，作者还做了详细的消融实验，因为对于Vision Transformer来说，**怎么对图片进行预处理以及怎样对图片最后的输出进行后处理是很关键的**，因为毕竟中间的模型就是一个标准的Transformer

1、class token

因为在本文中，想要跟原始的Transformer尽可能地保持一致，所以也使用了class token，因为class token在NLP的分类任务中也有用到（也是当作一个全局的对句子的理解的特征），本文中的class token是将它当作一个图像的整体特征，拿到这个token的输出以后，就在后面接一个MLP（MLP中是用tanh当作非线性的激活函数来做分类的预测）

- 这个class token的设计是完全从NLP借鉴过来的，之前在视觉领域不是这么做的，比如说有一个残差网络Res50，在最后一个stage出来的是一个14*14的feature map，然后在这个feature map之上其实是做了一个叫做gap（**global average pooling，全局平均池化**）的操作，池化以后的特征其实就已经拉直了，就是一个向量了，这个时候就可以把这个向量理解成一个全局的图片特征，然后再拿这个特征去做分类



对于Transformer来说，如果有一个Transformer模型，进去有n个元素，出来也有n个元素，为什么不能直接在n个输出上做全局平均池化得到一个最后的特征，而非要在前面加上一个class token，最后用class token的输出做分类？

- 通过实验，作者最后的结论是：这两种方式都可以，就是说可以通过全局平均池化得到一个全局特征然后去做分类，也可以用一个class token去做。本文所有的实验都是用class token去做的，主要的目的是跟原始的Transformer尽可能地保持一致（stay as close as possible），作者不想人觉得某些效果好可能是因为某些trick或者某些针对cv的改动而带来的，作者就是想证明，一个标准的Transformer照样可以做视觉

两种方法的效果对比如下图所示

![img](https://i0.hdslb.com/bfs/note/9a97a02920ebeea844abe15d4de7f8e6f418cb41.png@700w_!web-note.webp)

- 绿线表示全局平均池化
- 蓝线表示class token
- 可以发现到最后绿线和蓝线的效果是差不多的，但是作者指出绿线和蓝线所使用的学习率是不一样的，如果直接将蓝线的学习率拿过来使用得到的效果可能如橙线所示，也就是说需要进行好好调参



2、位置编码

作者也做了很多的消融实验，主要是三种

- 1d：就是NLP中常用的位置编码，也就是本文从头到尾都在使用的位置编码
- 2d：比如1d中是把一个图片打成九宫格，用的是1到9的数来表示图像块，2d就是使用11、12、13、21等来表示图像块，这样就跟视觉问题更加贴近，因为它有了整体的结构信息。具体的做法就是，原有的1d的位置编码的维度是d，现在因为横坐标、纵坐标都需要去表示，横坐标有D/2的维度，纵坐标也有D/2的维度，就是说分别有一个D/2的向量去表述横坐标和纵坐标，最后将这两个D/2的向量拼接到一起就又得到了一个长度为D的向量，把这个向量叫做2d的位置编码
- relative positional embedding（相对位置编码）：在1d的位置编码中，两个patch之间的距离既可以用绝对的距离来表示，又可以用它们之间的相对距离来表示（文中所提到的offset），这样也可以认为是一种表示图像块之间位置信息的方式

但是这个消融实验最后的结果也是：三种表示方法的效果差不多，如下图所示

![img](https://i0.hdslb.com/bfs/note/4de5b129fc2ee760c7d8c039e489e358a51d6477.png@700w_!web-note.webp)

- No Pos表示不加任何的位置编码，效果不太好，但也不算特别差。transformer根本没有感知图片位置的能力，在没有位置编码的情况下，还能够达到61的效果其实已经相当不错了
- 对比以上三种位置编码的形式发现，所有的performance都是64，没有任何区别
- 对此作者给出了他认为合理的解释，他所做的Vision Transformer是直接在图像块上做的，而不是在原来的像素块上做的，因为图像块很小，14x14，而不是全局的那种224x224，所以在排列组合这种小块或者想要知道这些小块之间相对位置信息的时候还是相对比较容易的，所以使用任意的位置编码都无所谓





通过以上的消融实验可以看出，class token也可以使用全局平均池化替换，最后1d的位置信息编码方式也可以用2d或者相对位置编码去替换，但是为了尽可能对标准的transformer不做太多改动，所以本文中的vision transformer还是使用的是class token和1d的位置信息编码方式





transformer encoder

transformer在现在看来是一个比较标准的操作了，作者对于transformer（或者说多头注意力机制）的解释放在附录中了





作者用整体的公式将整个过程总结了一下，如下图中的公式所示

![img](https://i0.hdslb.com/bfs/note/8b80f5899da957faadc23d70edc2ab2154fc9b4c.png@700w_!web-note.webp)

- X表示图像块的patch，一共有n个patch
- E表示线性投影的全连接层，得到一些patch embedding
- 得到patch embedding之后，在它前面拼接一个class embedding（Xclass），因为需要用它做最后的输出
- 一旦得到所有的tokens，就需要对这些token进行位置编码，所以将位置编码信息Epos也加进去
- Z0就是整个transformer的输入
- 接下来就是一个循环，对于每个transformer block来说，里面都有两个操作：一个是多头自注意力，一个是MLP。在做这两个操作之前，都要先经过layer norm，每一层出来的结果都要再去用一个残差连接
- ZL’就是每一个多头自注意力出来的结果
- ZL就是每一个transformer block整体做完之后出来的结果
- L层循环结束之后将ZL（最后一层的输出）的第一个位置上的ZL0，也就是class token所对应的输出当作整体图像的特征，然后去做最后的分类任务





**归纳偏置**

vision transformer相比于CNN而言要**少很多图像特有的归纳偏置**，比如在CNN中，locality（局部性）和translate equivariance（平移等变性）是在模型的每一层中都有体现的，这个先验知识相当于贯穿整个模型的始终



但是对于ViT来说，只有MLP layer是局部而且平移等变性的，但是自注意力层是全局的，这种图片的2d信息ViT基本上没怎么使用（就只有刚开始将图片切成patch的时候和加位置编码的时候用到了，除此之外，就再也没有用任何针对视觉问题的归纳偏置了）



而且位置编码其实也是刚开始随机初始化的，并没有携带任何2d的信息，所有关于图像块之间的距离信息、场景信息等，都需要从头开始学习



这里也是对后面的结果做了一个铺垫：vision transformer没有用太多的归纳偏置，所以说在中小数据集上做预训练的时候效果不如卷积神经网络是可以理解的





混合模型

既然transformer全局建模的能力比较强，卷积神经网络又比较data efficient（不需要太多的训练数据），所以搞出了一个混合的网络，前面是卷积神经网络，后面是transformer



作者对此做了实验：

- 原先是假设有一个图片，将它打成16*16的patch，得到了196个元素，这196个元素和全连接层做一次操作，最后得到patch embedding
- 现在不将图片打成块了，就按照卷积神经网络的方式去进行处理，将一整张图输入一个CNN，比如说Res50，最后出来一个14*14的特征图，这个特征图拉直了以后恰好也是196个元素，然后用新的到的196个元素去和全连接层做操作得到新的patch embedding

以上就是两种不同的对图片进行预处理的方式

- 一种是将图片打成patch，然后直接经过全连接层
- 另外一种就是经过一个CNN

因为这两种方式得到的序列的长度都是196，所以后续的操作都是一样的，都是直接输入一个transformer，最后再做分类





**遇到更大尺寸图片的时候如何做微调**

之前有工作说如果在微调的时候，能用比较大的图像尺寸（不是用224x224，而是用256x256，甚至更大的320*320，就会得到更好的结果）就能够得到更好的效果



vision transformer也想在在更大的尺寸上做微调，但是用一个预训练好的vision transformer其实是不太好去调整输入尺寸的。当使用更大尺寸的图片的时候，如果将patch size保持一致，但是图片扩大了，那么序列长度就增加了，所以transformer从理论上来讲是可以处理任意长度的，只要硬件允许，任意长度都可以。



但是提前预训练好的位置编码有可能就没用了，因为原来的位置编码是有明确的位置信息的意义在里面的，现在图片变大了，如果保持patch size不变的话，patch增多了，

- 这个时候位置编码该如何使用？作者发现其实做一个简单的2d的插值就可以了（使用torch官方自带的interpolate函数就可以完成了）。
- 但是这里的插值也不是想插多长就插多长，当从一个很短的序列变成一个很长的序列时，简单的插值操作会导致最终的效果下降，所以说这里的插值只是一种临时的解决方案，这也算是vision transformer在微调的时候的一个局限性
- 因为使用了图片的位置信息进行插值，所以这块的尺寸改变和抽图像块是vision transformer里唯一用到2d信息的归纳偏置的地方



### 7、实验



主要是对比了残差网络、vit和它们混合模型的表征学习能力



为了了解训练好每个模型到底需要多少数据，在不同大小的数据集上做预训练，然后在很多的数据集上做测试



当考虑到预训练的时间代价（预训练的时间长短）的时候，vision transformer表现得非常好，能在大多数数据集上取得最好的结果，同时需要更少的时间进行训练



最后作者还做了一个自监督的实验，自监督实验的结果虽然没有最好，但是还是可以，还是比较有潜力

- 时隔一年之后，MAE就证明了自监督的方式去训练ViT确实效果很好



数据集的使用方面主要是用了

- ImageNet的数据集：ImageNet-1k（最常用的有1000个类别、1300张图片）、ImageNet-21k（有21000个类别、14000张图片）
- JFT数据集：Google自己的数据集（有3亿张图片）



下游任务全部是做的分类，用的也是比较常用的数据集

- CIFAR
- Oxford Pets
- Oxford Flowers





**模型的变体**

一共有三种模型，参数如下图所示

![img](https://i0.hdslb.com/bfs/note/73acce55ebf70f9ebf01c0e70f4c53df06b6c53a.png@700w_!web-note.webp)

- Base
- Large
- Huge
- Layers：transformer block的个数
- Hidden size D：向量维度
- MLP size：
- Heads：多头自注意力中头的数量



因为本文中的模型，不光跟transformer本身有关系，还和输入有关系。当patch size大小变化的时候，模型的位置编码就不一样，所以patch size也要考虑在模型的命名里面，所以模型的命名方式就是

- vit-l 16:表示用的是一个vit large的模型，输入的patch size是16*16



transformer的序列长度其实是跟patch size成反比的，因为patch size越小，切成的块就越多，patch size越大，切成的块就越少，所以当模型用了更小的patch size的时候计算起来就会更贵，因为序列长度增加了



结果如下图所示，下表是说当它已经在大规模的数据上进行过预训练之后，在左边这一列的数据集上去做fine-tune（微调）的时候得到的表现

![img](https://i0.hdslb.com/bfs/note/9d47e062866abbd0a73c177308f19064b8d63702.png@838w_!web-note.webp)

- 上表对比了几个vit的变体和卷积神经网络（bit和noisy student）
- 和bit做对比的原因是因为bit确实是之前卷积神经网络里做得比较大的，而且也是因为他是作者团队自己本身的工作，所以正好可以拿来对比
- 和noisy student做对比是因为它是ImageNet之前表现最好的方法，它所采用的方法是用pseudo-label（伪标签）去进行self training，也就是常说的用伪标签也取得了很好的效果
- 从上表中可以看出，vit huge用比较小的patch 14*14能取得所有数据集上最好的结果。



但是因为这些数值都太接近了，仅仅相差零点几个点或者一点几个点，没有特别大的差距，所以作者觉得没有展示出vision transformer的威力，所以作者就得从另外一个角度来体现vit的优点：因为训练起来更便宜

- 作者所说的更便宜是指最大的vit huge这个模型也只需要训练2500天tpuv3天数，正好bit和noisy student也都是google的工作，也都是用tpuv3训练的，所以刚好可以拿来比较，bit用了9900天，noisy student用了一万多天，所以从这个角度上来说，vit不仅比之前bit和noisy student要训练的快，而且效果要好，所以通过这两点可以得出vit真的是比卷积神经网络要好的结论





**分析**

vision trasformer到底需要多少数据才能训练的比较好？

下图中图三是整个vision trasformer论文最重要的take home message，是作者最想让读者知道的，这张图基本上把所有的实验都快概括了

![img](https://i0.hdslb.com/bfs/note/24ec144dfcd98cfb8595688dc8a7a3285f630140.png@838w_!web-note.webp)

- 图三表示当时用不同大小的数据集的时候，比如说ImageNet是1.2m，而ImageNet-21k是14m，JFT是300m，当数据集不断增大的时候，resnet和vit到底在ImageNet的fine-tune的时候效果如何
- 图三的主要意思是说，灰色代表bit，也就是各种大小的resnet，**<font color=red size=5>最下面表示50，最上面表示152</font>**，如下图所示，他所想要展示的是在中间的灰色区域就是resnet能达到的效果范围，剩下的圆点就是各种大小不一的vision transformer

![img](https://i0.hdslb.com/bfs/note/88678bd2ad92d0f85d752cbc5e8792f666476a2e.png@838w_!web-note.webp)

- 在最小的ImageNet上做预训练时，vision transformer是完全不如resnet，vision transformer基本上所有的点都在灰色区域的下面。这说明vision transformer在中小型数据集上做预训练的时候的效果是远不如残差网络的，原因就是因为vision transformer没有使用先验知识（归纳偏置），所以它需要更大的数据去让网络学得更好
- 在ImageNet-21k上做预训练的时候，vision transformer和resnet已经是差不多了，vision transformer基本上所有的点都落在灰色区域内
- 只有当用特别大的数据集JFT-300M时，vision transformer是比bit对应的res152还要高的

总之这个图所要表达的是两个信息

- 如果想用vision transformer，那么得至少准备差不多和ImageNet-21k差不多大小的数据集，如果只有很小的数据集，还是选择使用卷积神经网络比较好
- 当已经拥有了比ImageNet-21k更大的数据集的时候，用vision transformer就能得到更好的结果，它的扩展性更好一些

其实整篇论文所讲的就是这个scaling



图四如下图右图所示，因为作者在图三中要用vision transformer跟resnet做比较，所以在训练的时候用了一些强约束（比如说dropout、weight decay、label smoothing），所以就不太好分析vision transformer模型本身的特性,所以在图四中做了linear few-shot evaluation（在拿到预训练的模型之后，直接把它当成一个特征提取器，不去fine-tune，而是直接拿这些特征做了一个just take a regression就可以了），同时作者选择了few-shot，图示中标出了5-shot，就是在ImageNet上做linear evaluation的时候，每一类随机选取了5个sample，所以这个evaluation做起来是很快的，作者用这种方式做了大量的消融实验

![img](https://i0.hdslb.com/bfs/note/50925ec9c3601353871b07864a9e1a6a91f019f9.png@838w_!web-note.webp)

- 图四中横轴表示预训练数据集的大小，这里就使用了JFT，没有用别的数据集，但是他取了一些JFT的子集：10M、30M、100M、300M，这样因为所有的数据都是从一个数据集里面得来的，就没有那么大的distribution gap，这样比较起来模型的效果就更加能体现出模型本身的特质
- 图四中的结果其实跟图三差不多，图中浅灰色的线是res50，深灰色的线是res152，当用很小的预训练的数据集的时候vision transformer是完全比不过resnet的
- 本文给出的解释是因为缺少归纳偏置和约束方法（weight decay、label smoothing等），所以就导致在10M数据集的情况下vision transformer容易过拟合，导致最后学到的特征不适合做其他任务，但是随着预训练数据集的增大，vision transformer的稳健性就提升上来了
- 但是因为这里的提升也不是很明显，作者也在最后一段写了如何用vision transformer去做这种小样本的学习是一个非常有前途的方向





由于vision transformer这篇论文之前说了，它的预训练比用卷积神经网络便宜，所以这里就需要做更多的实验来支持它的论断，因为大家对transformer的印象都是又大又贵，很难训练，下图图五中画了两个表

图五
左图的average-5就是他在五个数据集（ImageNet real、pets、flowers、CIFAR-10、CIFAR-100）上做了evaluation，然后把这个数字平均了

因为ImageNet太重要了，所以作者将ImageNet单独拎出来又画了一张表，如右图所示

但是其实这两张表的结果都差不多

蓝色的圆点表示vit

灰色的圆点表示resnet

橙色的加号表示混合模型（前面是卷积神经网络，后面是transformer）

图中大大小小的点就是各种配置下大小不一样的vision transformer的变体，或者说是resnet的变体

左右两张图中所有的模型都是在JFT 300M数据集上训练的，作者这样训练的目的不想让模型的能力受限于数据集的大小，所以说所有的模型都在最大的数据集上做预训练

上图中几个比较有意思的现象

如果拿蓝色的圆圈所表示的vit去跟灰色圆圈的resnet作比较，就会发现，在同等计算复杂度的情况下，一般transformer都是比resnet要好的，这就证明了：训练一个transformer是要比训练一个卷积神经网络要便宜的

在比较小的模型上面，混合模型的精度是非常高的，它比对应的vision transformer和resnet都要高，按道理来讲，混合模型都应该是吸收了双方的优点：既不需要太多的数据去做预训练，同时又能达到跟vision transformer一样的效果

但是当随着模型越来越大的时候，混合模型就慢慢的跟vision transformer差不多了，甚至还不如在同等计算条件下的vision transformer，为什么卷积神经网络抽出来的特征没有帮助vision transformer更好的去学习？这里作者对此也没有做过多的解释，其实怎么预处理一个图像，怎么做tokenization是个非常重要的点，之后很多论文都去研究了这个问题

如果看整体趋势的话，随着模型的不断增加，vision transformer的效果也在不停地增加，并没有饱和的现象（饱和的话一般就是增加到一个平台就不增加了），还是在不停的往上走的。但是但从这个图中来看的话，其实卷积神经网络的效果也没有饱和

分析完训练成本以后，作者也做了一些可视化，希望通过这些可视化能够分析一下vit内部的表征

vision transformer的第一层（linear projection layer，E），下图展示了E是如何embed rgb value，这里主要展示了头28个主成分，其实vision transformer学到了跟卷积神经网络很像，都是这种看起来像gabor filter，有颜色和纹理，所以作者说这些成分是可以当作基函数的，也就师叔，它们可以用来描述每一个图像块的底层的结构


位置编码是如何工作的？如下图所示，这张图描述的是位置编码的相似性，数字越大相似性越高（-1到1，cos），横纵坐标分别是对应的patch，如果是同一个坐标，自己和自己相比，相似性肯定是最高的。从图中可以发现，学到的位置编码是可以表示一些距离信息的，同时它还学习到了一些行和列的规则，每一个图像块都是同行同列的相似性更高，也就意味着虽然它是一个1d的位置编码，但是它已经学到了2d图像的距离概念，这也可以解释为什么在换成2d的位置编码以后，并没有得到效果上的提升，是因为1d已经够用了


最后作者想看一下自注意力是否起作用了，只为之所以想用transformer，就是因为自注意力的操作能够模拟长距离的关系。在NLP中，一个很长的句子里开头的一个词和结尾的一个词也能互相有关联，类比在图像里很远的两个像素点也能够做自注意力，所以作者就是想看一下自注意力到底是不是想期待的一样去工作的。下图展示的是vit large 16这个模型，vit large有24层，所以横坐标所表示的网络深度就是从0到24，图中五颜六色的点就是每一层的transformer block中多头自注意力的头，对于vit large来说一共有16个头，所以每一列其实有16个点。纵轴所表示的是mean attention distance（平均注意力的距离：假如说图上有两个点，平均注意力距离表示的就是整两个点真正的像素之间差的距离乘以他们之间的attention weights，因为自注意力是全局都在做，所以说每个像素点跟每个像素点都会有一个自注意力权重，平均注意力的距离就能反映模型到底能不能注意到两个很远的像素）。图中的规律还还是比较明显的：投机层中，有的自注意力中的头注意的距离还是挺近的，能达到20个像素，但是有的头能达到120个像素，这也就证明了自注意力真的能够在网络最底层，也就是刚开始的时候就已经能够注意到全局上的信息了，而不是像卷神经网络一样，刚开始第一层的receptive field（感受野）非常小，只能看到附近的一些像素；随着网络越来越深，网络学到的特征也会变得越来越高级，越来越具有语义信息；大概在网络的后半部分，模型的自注意力的距离已经非常远了，也就是说它已经学到了带有语义性的概念，而不是靠邻近的像素点去进行判断


为了验证上面所得到的结论，作者又画了另外一个图，如下图所示。图中是用网络中最后一层的out token所作的图，从图中可以发现，如果用输出的token的自注意力折射回原来的输入图片，可以发现模型确实是学习到了这些概念。对于全局来说，因为输出的token是融合了所有的信息（全局的特征），模型已经可以关注到与最后分类有关的图像区域






在文章的最后，作者还做了如何用自监督的方式去训练vision transformer的测试

这篇论文算上附录22页，在这么多的结果中，作者把别的结果都放到了附录里，而把自监督放到了正文中，可见它的重要性。它之所重要主要是因为在nlp领域，transformer这个模型确实起到了很大的推动作用，但另外一个真正让transformer火起来的原因其实是大规模的自监督训练，二者缺一不可。NLP中的自监督无非就是完形填空或者是预测下一个词，但是因为这篇论文主要仿照的是BERT，所以作者就想能不能也借鉴BERT这个目标函数去创建一个专属于vision的目标函数，BERT使用的就是完形填空（mask language modeling，给定一个句子，然后将一些词mask掉，然后通过一个模型，最后将它预测出来），同理，本文就仿造了一个mask patch prediction，意思就是给定一张图片，将它打成很多patch，然后将某些patch随机抹掉，通过这个模型以后，再将这些patch重建出来。

但是最后vit base 16在ImageNet只能达到80的左右的准确率，虽然相对于从头来训练vision transformer已经提高了两个点，但是跟最好的有监督的训练方式比差了4个点，所以作者将跟对比学习的结果当作是未来的工作（对比学习是去年CV圈最火的人们话题，是所有自监督学习中表现最好的，所以紧接着vit MoCo v3和DINO就出现了，这两篇论文都是用对比学习的方式去训练了一个vision transformer）





###  8、评价

这篇论文写的还是相当简洁明了的，在有这么多内容和结果的情况下，做到了有轻有重，把最重要的结果都放到了论文里，图和表也都做的一目了然

从内容上来说，可以从各个角度来进行分析、提高或者推广vision transformer

如果从任务角度来说，vision transformer只是做了分类，所以还可以拿他去做检测、分割甚至别的领域的任务

如果从改变结构的角度来讲，可以去改变刚开始的tokenization，也可以改中间的transformer block，后来就已经有人将自注意力换成了MLP，而且还是可以工作得很好（几天前，甚至有一篇论文叫做mataformer，他认为transformer真正工作的原因是transformer这个架构而不是因为某些算子，所以他就将自注意力直接换成了池化操作然后发现，用一个甚至不能学习的池化操作（文中提出了一个pool former模型）也能在视觉领域取得很好的效果），所以在模型的改进上也大有可为

如果从目标函数来讲，可以继续采用有监督，也可以尝试很多不同的自监督训练的方式

最重要的是vit打破了NLP和CV之间的鸿沟，挖了一个更大的多模态的坑，可以用它去做视频、音频，甚至还可以去做一些基于touch的信号，也就是说各种modality的信号都可以拿来使用

 





## 七、MAE  阅读



### 1、与之前论文联系



**和之前精读论文的关系？**



Transformer

- 一个纯基于注意力机制的编码器和解码器
- 表现比 RNN 架构好，在机器翻译任务



BERT

- 使用 一个 Transformer 编码器
- 拓展到更一般的 <font color=red>NLP</font> 的任务
- 使用了 完型填空 的==自监督==的训练机制
- 不需要使用标号
- 去预测一个句子里面 不见 masked 的词 
- 从而获取对文本特征抽取的能力
- BERT 极大的扩展了 Transformer 的应用
- 在一个大规模的、没有标号的数据上 训练出非常好的模型出来



ViT 

- 将 Transformer 用到 <font color=red>CV</font> 上面
- 把整个图片分割成很多 16 * 16 的小方块
- 每一个方块 patch 做成一个词 token，然后放进 Transformer 进行训练
- 证明：训练数据足够大 （1,000万 或者 一个亿的训练样本）的时候，Transformer 的架构 精度 优于 CNN 架构





![img](https://i0.hdslb.com/bfs/note/11dcb4bc7dddbb255c9c10bb96b4c0598e90f489.png@620w_!web-note.webp)



MAE：

- BERT 的一个 CV 的版本
- 基于 ViT ，BERT化
- 把整个训练 拓展到没有标号的数据上面
- 通过完型填空来获取图片的一个理解
- 不是第一个将 BERT 拓展到 CV 上
- MAE 很有可能 未来影响最大
- ==BERT 加速了 Transformer 架构 在 NLP 的应用==
- ==MAE 加速 Transformer 在 CV 上的应用==



### 2、标题 + 作者



02:46



Masked Autoencoders **带掩码的自编码器** 是可扩展的视觉学习器 scalable vision learners



**scalable**：可扩展的，模型比较大

**efficient**：算法特别快



**vision learners**：≠ classifier，一个 backbone 的模型



**masked**：来源于 BERT

- BERT 是 masked language model，带掩码的语言模型；完形填空
- 每次挖掉一些东西，然后去预测挖掉的东西



**Auto-encoder**：

- auto “自”，ML模型 auto 自模型；i.e., 自回归模型
- 标号 y 和 样本 x 来自 同一个东西
- 语言模型中，每一次用前面那些词，去预测接下来的一个词；在另外一个样本里面，我们这个预测的词也是标号，也是成为另外一个样本的 x 本身
- 样本 x 和 标号 y 来自于同样的句子里面的词 --> auto 
- NLP 里，语言模型是一大类，有没有 auto 大家都理解
- Transformer、BERT 用的是 encoder，加不加 auto 大家都觉得没问题
- CV 里 auto 自任务很少
- 图片（像素）的标号很少来自于图片本身，图片的标号更多是文本
- encoder-decoder 在图片里用得很多
- ==加 auto 在 encoder之前，MAE 的图片标号是图片本身，区分于其它工作==



**标题模板很不错！**

强有力的句式：文章的结论总结成一句话，XX 是 好 XX

结论放在 title 里，句式相对客观

i.e., GPT 系列 3篇文章

GPT1: Improving Language Understanding by Generative Pre-Training (Generative Pre-Train Model 就是GPT模型的名字由来）

GPT2: Language Models are Unsupervised Multitask Learners

GPT3: Language Models are Few-Shot Learners



**Example**：

我的手写笔是世界上 最好用来做笔记的笔 ❌

手写笔是一个做笔记很好的工具 ✔

- 虽然手写笔是我第一个发明的
- 从读者角度讲述，手写笔为什么它是一个好东西？



**作者**



06:03



FAIR

一作：Kaiming 恺明 ResNet 一作 + Project lead （从公司拿资源做项目、管理者；不是一般最后的大老板）

*：equal technial contribution

最后两位作者：CV 物体检测的大佬



### 3、摘要



07:10



**第一句话扩展 title MAE**

masked autoencoders are scalable vision learners --> Masked AutoEncoders are scalable self-supervised learners for computer vision. 



**第二句话 MAE 简单**

随机盖住图片里的一些块(patch, image 的一个块)，==再重构 缺失的像素==。

- 思想来自于 BERT 带掩码的语言模型
- 这里不是 masked 词，而是图片的一个块 image，重构这个缺失块里面的所有像素



**第三句话 2个 core designs**

**asymmetric encoder-decoder architecture**

- 虽然 MAE 说自己是 masked autoencoder
- 任何一个模型都有一个编码器解码器
- BERT 预测相对简单， 解码器：最后的一个全连接输出层
- MAE 预测一个块里面的所有像素
- MAE 的编码器 只关注 可见的 patches，节省计算时间
- 如果 一个 patch 被丢掉了，编码器不会对它进行编码



- MAE a lightweight decoder 重构原始的像素



**遮住大量的块 (i.e., 75%) 重构像素是一个非显然 nontrivial and meaningful 有意义的 self-supervisory task 自监督任务**

- ==如果只遮住几块的话，插值法就可以得到缺失的像素==，trivial 
- 遮住一大半的块，迫使模型学习更好的表征



**第四句话：asymmetric encoder-decoder + 75% masked --> train large models efficiently and effectively**

更有效的训练模型

- 大：比较有挑战的任务，不是求显然的解 nontrivial
- 块：不看遮住的块，训练量是 1 / 4，加速 3 倍 or 以上



**第五句话：结果好**

最简单的 a vanilla ViT-Huge 模型 在 ImageNet-1K 87.8% 准确率

- ViT 的自监督学习，效果不那么好，所以没怎么展开
- ViT 的作者认为还是需要 有标号的模型、用更大的训练集，效果更好
- MAE 使用小数据集  ImageNet-1K 100w 图片，self-supervise 效果很好



**第六句话：迁移学习效果**

MAE 主要用来做 迁移学习，在别的任务上表现好

shows promising scaling behavior





### 4、关键图



10:49



CV 最重要的一张图就是放在第一页的右上角



图1：MAE 的模型架构

**预训练流程**：input --> patches --> masked --> unmasked patches in encoder --> unmasked + masked 按位置排列 进 decoder --> decoder 重构 masked patches 的像素

- **patches + masked**：一张红色鸟图片进来，切成 patches，masked 块 (3/4) 是 灰色的。
- **unmasked patches，encoder**：没有 masked (1 / 4) 的块 进入 encoder (ViT)，得到每一块的特征（蓝色）。
- encoder 的输出 和 masked tokens 按照在图片中的原始位置排列成一长条向量 （包含位置信息）。
- 长条向量 进入 decoder，解码器尝试重构缺失的像素信息，还原原始图片





![img](https://i0.hdslb.com/bfs/note/7294f04595235a39b9b66a62ec67fb40510578e3.png@620w_!web-note.webp)



encoder 比 decoder 高：==计算量主要来自于 encoder，对图片的像素进行编码==



优化 encoder by 编码器只用处理 unmasked patches，i.e., 一张图里 1/4 的像素，--> 计算量降低

- Transformer 模型计算量特别大，几倍加速也很重要。





**Q：什么情况不需要解码器？**

用 MAE 做一个 CV 的任务，只需要用编码器。一张图片进来，不需要做掩码，直接切成 patches 格子块，然后得到所有 patches 的特征表示，当成是这张图片的特征表达，用来做 CV 的任务。



**图2：ImageNet 测试集图片**



三列分别是：80% masked tokens, MAE 重构的效果，ImageNet 真实图片



![img](https://i0.hdslb.com/bfs/note/506bd283de3d0601db9b2016dd9899f7193aa3a4.png@620w_!web-note.webp)



虽然细节有一点模糊，钟的指针、车的形状、狗、灯都还原的很好。

- 图片尺寸只有那么高，分辨率有限



Note: MAE 不一定对 所有图片的构造都那么好，图 2 是展示比较好的样例



**图3：COCO**

不同的数据集，效果也惊人。



![img](https://i0.hdslb.com/bfs/note/0a890bb24a5ebf5d7db4362aac5f18253927c360.png@620w_!web-note.webp)



图4 同一张图片、masked patches 的不同比例 的还原效果

95%效果惊人，蘑菇🍄、斑马🦓、车🚗、西红柿 都还原出来了。

![img](https://i0.hdslb.com/bfs/note/b1370ee2952a30ece18687fc49e5d7b9e41b2cc5.png@620w_!web-note.webp)





### 5、结论



15:19



3段



**Simple algorithms that scale well are the core of deep learning**. ==简单 + 可以拓展很好的算法是 DL 的核心==



**simple**：作者的简单是在 ViT 基础上，MAE 提出来的东西相对简单



**scale well**：能跑 大数据集



作者对 simple 和 scale well 是不是有什么误解？哈哈哈哈

- MAE 基于 ViT，ViT 基于 Transformer，整个 Transformer 模块里面有那么多东西，要那么多层堆起来 --> 比较复杂
- 很有钱的时候，scale well 无限加数据（无需标号）



In NLP, self-supervised learning methods enable benefits from exponentially scaling methods. NLP 自监督学习 火 🔥



CV 里 有标号的预训练数据是主流。



MAE 在 ImageNet 数据集上，通过自编码器学习到 可以媲美 有标号的 结果。



**第二段：图片和语言的差别**

- a word in a sentence：一个词是语义单元，包含较多语义信息
- a patch in an image：一定的语义信息，但不是一个语义的 segment
- 一个 patch 并不含有一个特定的物体
- 可能是多个物体的一小块 or 一个物体重叠的一块
- 即使图片和语言的 masked 的单元包含语义信息不同，MAE or Transformer 可以学到一个隐藏的比较好的语义表达



**第三段：broader impacts**

如果工作出圈，对社会的影响？

- 只用了图片本身信息学习
- 图片本身有 bias 的话，倾向于某一些图片 or  有一些不好的图片，可能会有负面的社会影响
- MAE 可以用来生成不存在的内容
- MAE 是生成模型，生成原始的像素
- 和 GAN 类似，有误导大家的可能
- 如果要使用这个工作，请一定要考虑潜在的影响



So far, MAE 在干什么？ 效果怎么样？



### 6、导言



18:33



**导言第一段：问题所在**

深度学习飞速发展，但 CV 仍然以来百万级别、甚至更多有标注的数据



**导言第二段：大量有标注数据是必须的吗？其它领域怎么解决的？**

NLP 的自监督学习很好

- ==GPT、BERT 可以使用 无标注 的数据及逆行训练，得到千亿级别可学习的参数模型==
- GPT 系列，一个标准的语言模型
- BERT 一个带掩码的自编码模型

CV 里已有的 maksed autoencoder 带掩码的自编码器

- denoising autoencoder，一张图片里加入很多噪音，通过去噪来学习对这张图片的理解
- 最近也有很多工作将 BERT 应用于 CV



但，作者认为 BERT 在 CV 领域的应用落后于 NLP

**What makes masked autoencoding different between vision and language？**

什么使得 带掩码的自编码器模型在 CV 和 NLP 处理上的不一样呢？



**1）==CV 使用 CNN，卷积窗口不好将 mask 放进去==**

**archtectural gap has been addressed by ViT**

- CNN 在一张图片上，使用一个卷积窗口、不断地平滑，来汇聚一些像素上面的信息 + 模式识别
- Transformer 的一个 mask 对应的是一个特定的词，会一直保留，和别的词区分开来
- **卷积上做掩码？**
- 图片的一块盖住 by 像素替换成一个特定的值，
- 卷积窗口扫过来、扫过去时，无法区分边界，无法保持 mask 的特殊性，无法拎出来 mask；最后从掩码信息很难还原出来
- **卷积不好加入位置编码？** 不那么充分
- Transformer 需要位置编码：attention 机制没有位置信息
- 卷积自带位置信息，不断平移时，不需要加入位置信息



**2）语言和图片的信息密度不同**

NLP 的一个词是一个语义的实体，一个词在字典里有很长的解释；一句话去掉几个词，任务很难，i.e., 完形填空 --> BERT 的 mask 比例不能过高



CV 的图片去掉一个块，通过对邻居的像素值进行插值还原。**怎么让任务不那么 trivial 呢？**

- **随机去掉很高比例的块**，极大降低图片的冗余性
- 这一块的周边块都被去掉了，这一块和很远的块的关系不那么冗余
- nontrivial 任务，使 模型去看 一张图片的 holistic 全局信息，而不仅关注局部





3）The autoencoder‘s decoder

CV 还原图片的原始像素：低层次的表示

NLP 还原句子里的词：语义层次更高，i.e., BERT 的一个全连接层还原词



图片分类、目标检测的 decoder：一个全连接层

语义分割（像素级别的输出）：一个全连接层不够，很有可能使用一个转置的卷积神经网络、来做一个比较大解码器。



**MAE 的想法：**

随机遮	住的像素信息，让它使用一个非对称的编码器和解码器的机制。



<font color=red>**非对称**：编码器和解码器看到的东西不一样</font>

- 编码器只看到可见块
- 解码器拿到编码器的输出之后，重构 masked patches
- 非对称的原因：
- 大量 masked 块
- 编码器只看可见块，极大降低计算开销、减少内存消耗



**导言最后一段：实验结果**

MAE预训练，只使用 ImageNet-1K 100w 无标号数据，ViT-Large/-Huge 达到 ViT 需要 100倍于 ImageNet-1K 的数据 的效果。



迁移学习效果也很好，预训练的模型在 目标检测、实例分割、语义分割 的效果都很好。



和 NLP 类似的效果：

在大量的没有标号的数据上，通过自监督学习训练出来模型，迁移学习效果不错



2页，图片 + 使用了 问题 - 回答问题 - 引出想法 的写法



**更本质的问题？**

把 BERT 从 NLP 用到 CV 有什么问题？

MAE 算法为什么要设计成这样？

- ViT 解决 图片中的 mask
- 大量随机 masked 块，降低图片冗余度
- 非对称的自编码器-解码器



**写作建议：**

<font color=blue>**讲清楚，你为什么要这么做？你对这个问题的动机？**</font>

- 没有动机 就是技术报告了，i.e. AlexNet





### 7、相关工作



26:58



**1）带掩码的语言模型**：BERT, GPT

**2）自编码器在 CV 的应用**

- MAE 也是一种形式的 带去噪的自编码
- masked patch 在这一个图片块里面加了很多噪声
- 和 传统的 DAE(Denoising autoencoder) 是很不一样的
- MAE 基于 ViT、transformer 架构

**3）带掩码的自编码器在 CV 的应用**

- iGPT，GPT 在 image 的应用
- ViT 最后一段，怎么用 BERT 方式训练模型
- BEiT，BERT 在 image 上的应用
- 给每一个 patch 离散的标号，更像 BERT 

MAE 直接重构 原始的像素信息



**4）self-supervised learning**

最近火 🔥 的 contrastive learning，==使用数据增强==

MAE 不需要数据增强（实验部分有讲）



相关工作总结：4个角度的对比，MAE 和它们都不同，但是**没有特别讲 MAE 和每一个不一样的地方在哪里**。

esp, iGPT 和 BEiT



**写作建议：**

相关工作介绍，写清楚和特别相关的工作的区别，不要让大家去猜





### 8、MAE模型



28:49



MAE 是一个简单的 自编码器（无监督）

- 看到了部分的观察的数据
- 用 观察到的部分数据 **重构** 完整的原始信号



所有自编码器的工作

- 将观察到的信号 映射到一个潜在 latent 表示里面
- 潜在表示：语义空间的一个表示
- 解码器 用 潜表示 latent 重构原始信号



**MAE 的自编码器 和 经典自编码器的不同？**

- asymmetric design 非对称结构
- 编码器只看 可见块
- 忽略不可见 masked 块，节省计算开销



**掩码 mask 如何工作？**



29:39





<font color=red>和 ViT 的一样图片 patches 化， i.e., 一张图片 九宫格分割，3 * 3，每一格 代表一个 patch，作为一个词 token</font>



![img](https://i0.hdslb.com/bfs/note/29d408076e5b05e55be18a35f38e3cb7ca360347.png@620w_!web-note.webp)



**random sampling?**

随机均匀采样块保留, 剩余块用 mask 掩码盖住。



**MAE 的关键技术？**

只采样少量的块，其余的块全覆盖，去掉图片 patches 之间的冗余度。--> 增加任务的复杂度



**MAE 的编码器**

- a ViT， 没有任何改动
- but applied only on visible, unmasked patches, 只作用于 可见块



**MAE 编码器如何作用于可见块呢？**



30:34



和 ViT 一样：

- 每一个 patch 块拿出来，做一个线性投影
-  \+ 位置信息 --> token 



和 ViT 不一样：

- masked 块不进入 MAE 编码器
- i.e., 随机采样概率 1 / 4， 25% 样本块进入 ViT，计算量减少



**MAE 的解码器**

要重构 masked 块的像素信息，需要看到 可见块（编码器对可见块的潜表示） 和 masked 块 （没有进入编码器）



a shared, learned vector 通过**一个** 共享的可以学到的向量来表示 each mask token 



==**每一个被盖住的块都表示成同样一个向量，**此向量值可学习==



解码器是 another series of Transformer blocks 另外一个 transformer

- 需要位置信息，不然无法区分对应哪一个 掩码masked tokens 



**可见块的位置信息 question？**

位置信息 要不要也对那些编码器过来的 潜在表示 也加上



==因为可见块的潜表示其实本来已经加过一次了，那么这个地方要不要再加上一次==？



**解码器什么时候用？**

pre-training；别的下游任务，解码器不需要，只需要编码器对图片编码得到潜表示 --》灵活，想用什么随便用



**解码器的架构 大 吗？**

相对小，计算开销不到编码器的 1 / 10





**怎么样重构出原始的像素？**



32:23



解码器的最后一层： a linear projection 

- 一个 patch 是 16 * 16 像素的话，线性层会投影到长为 256 的维度
- 再 reshape(16, 16), 还原原始像素信息
- ==损失函数： MSE，像素值相减，再平方和==
- 只作用于非可见块的损失，和 BERT 一样
- 可见块的图片编码器已经看到了，看到答案就不算正确率了





对预测的像素做一次 normalization，使像素均值为 0 方差为 1，数值更稳定。



**pre-training 的 normalization 可，但预测的时候怎么办呢？**

- pre-training 有标号的样本均值方差可算



**Simple implementation** 



33:31



- 对每一个输入 patch 生成 a token：一个一个 patch 的线性投影 + 位置信息
- 随机采样：==randomly shuffle== 随机打断序列，把最后一块拿掉。
- 从 头部均匀的、没有重置的 样本 采样
- 25% 意味着 随机 shuffle， 只保留前 25% 
- after encoding 解码时：append 跟以前长度一样的这些掩码的一些词源 mask tokens （一个可以学习的向量 + 位置信息），重新 unshuffle 还原到原来的顺序
- MSE 算误差时，跟原始图的 patches 对应



The decoder is applied to this full list (with positional embeddings added). 

**编码器处理可见块的潜表示需不需要再加位置信息？**



**shuffle 和 unshuffle 有什么好处？**

没有稀疏的操作，实现快，不影响 ViT 块的实现





总结：解码器怎么做？怎么还原像素信息？实现中随机采样怎么做？





### 9、实验



35:22





第四章：ImageNet 的实验



在 ImageNet-1K 100万张图片 数据集上

- 先做自监督的预训练（不用标号，只拿图片）
- 然后再在同样的数据集上做有标号的监督训练



**做法：**

- end to end 的微调，允许改整个模型 所有的可学习的参数；
- linear probing 允许改最后一层的线性输出层



结果：在验证集上报告 top-1 的精度，用 中心剪裁的 224*224 的图片



Baseline: ViT-Large / 16, 16 * 16 

ViT-Large 比 ResNet50 要大很多，很容易 overfitting 





**比较的 3 种情况：**

**scratch, original: 76.5,** ViT 所有的内容在 ImageNet-1K上训练, 结果不稳定 200 epoches



**scratch, our impl.: 82.5** 加入 strong regularization A.2

- ViT 文章说 ViT 需要很大的数据才行
- 小一点的数据 + 合适的正则化 ✔



**baseline MAE: 84.9** 先使用 MAE 做预训练，再在 ImageNet 上微调 50 epoches

- 数据集没变化，预训练和微调都是 ImageNet 

MAE 纯从图片上学到不错的信息



**主要结果 表1**



37:36



**ablation study**

**a: 解码器的深度**，多少个 Transformer 块; end to end fine-tuning 贵一点，但效果好

- 全都 ft，深度和效果关系不大 84.x
- 只调 lin, 深度深一点好



**b: 解码器的宽度**，每一个 token 表示成一个多长的向量

- 512 比较好



**c: 编码器要不要加入被盖住的 masked 块：**

- **不加**很好，精度高、计算量更少
- 非对称的架构 精度好、性能好



![img](https://i0.hdslb.com/bfs/note/7ecd309199ce7297bf0796d2cc3915ad4a3cbdd8.png@620w_!web-note.webp)



**d: 重构的目标**

- 每个像素的MSE
- **每个像素的MSE + normalization 均值为0 方差为 1 效果好**
- PCA 做一次降维
- dVAE: BEiT 的做法，通过 ViT 把每一个块映射到一个离散的 token，像 BERT 一样的去做预测





**e 怎么样做数据增强**

- 什么都不做
- 固定大小的裁剪
- **随机大小的裁剪**
- 裁剪 + 颜色变化

MAE 对数据增强不敏感



**f 怎么采样 被盖住的块**

- <font color=red>随机采样 最简单最好</font>
- 按一块块的采样 50 %
- 按一块块的采样 75 %
- 网格采样



**表1 主结果内容的展开图**



40:15





使用不同的掩码率的时候的效果：



10%的块被遮住： 83.2%

\> 40%的块被遮住：精度大幅提升

只调最后一层：更敏感



![img](https://i0.hdslb.com/bfs/note/0331e5654b0b4ca48e84d65cd98310fc1fcb0bbf.png@620w_!web-note.webp)





**表2 训练时间**

- ViT-Large + 解码器只使用一层 Transformer 的块：84.8% 精度不错，耗时最少
- 带掩码的块 + 大的解码器，加速 3.7倍
- ViT huge 加速也比较多



![img](https://i0.hdslb.com/bfs/note/d4c3e7e55fb932deda8df1fe19f8c31422fccc60.png@620w_!web-note.webp)



**绝对时间**

128个 TPU v3的 core， tensorflow 实现

训练时间是10个小时 和 大概是一天多，可以忍受





**图 6 表示的是不同的掩码采样策略的区别**

- 随机采样效果好
- 尽量的按照一块一块的来切
- 按照格点来切



![img](https://i0.hdslb.com/bfs/note/10cc8644d42c4897afa66ad8def2f342ba52b7a6.png@620w_!web-note.webp)





**图 7 预训练的轮数和微调的精度的对比**



![img](https://i0.hdslb.com/bfs/note/ca4bc214ce8cb2bec134ac1e09a9d33d8ac91f6d.png@620w_!web-note.webp)



 ImageNet-1K 上训练个 1,000 个数据轮，精度有提升，在一直训练一直学习，过拟合也没那么多严重，因为1,000轮是非常非常多的

- 一般在 ImageNet 上训练， 200轮 enough





**4.1 讲的是不同的超参数下的一些结果**

- 自己跟自己比



**4.2 就是比的是之前结果**

- 主要的结果在表3和图8里面





![img](https://i0.hdslb.com/bfs/note/5d30295cdab2f14896ff98d8981e3098123cf54b.png@620w_!web-note.webp)



表3：==跟前面结果比 MAE 效果是最好的==

图8：跟 ViT 里面的结果比

- 最上面的虚线：<font color=red>ViT 在 JFT 3亿标号的图片数据集合的效果</font>
- 排第二的线：只使用 ImageNet-1K 也就是1/300数据的效果
- 两根线很接近，不能说这是一个很公平的比较
- JFT数据集包括的类数远远大于 ImageNet
- 它们很多是一些 顾我自己 care 的一些目标，但是 ImageNet很多都是一些猫猫狗狗的图片
- 测试集也是 ImageNet，JFK 它多了很多很多 可能跟你那些标号不那么一样的图片



把验证集换到一个 不那么跟 ImageNet 相像的数据集上，可能这个差距会大一点



**主要比较的是算法，而不是数据集**





**4.3 调编码器所有层的参数和最后一层的参数效果差距大**

**到底调多少层：**

- 少，快，精度差
- 多，慢，精度好

![img](https://i0.hdslb.com/bfs/note/2907e839937588ab073242c7acfff98ca94c6106.png@620w_!web-note.webp)



调 4 - 5 层比较好

- 底层不调：底层学到的东西稍微是比较低层次一点，你可以换一个任务也不需要变太多
- 上面那些层，跟你的任务相关，要调



**5迁移学习实验结果**



45:08





COCO 的目标检测和分割

- MAE 做它的主干网络 效果最好
- 跟之前的 ViT 和 BEiT 比



重构像素的比较

- 像 BEiT 那样重构 or 用 dVAE 学出来的标号比 差别不大
- 重构原始的像素 简单、好





![img](https://i0.hdslb.com/bfs/note/8eeef4086553d00b340bfeabfdd6b893174356aa.png@620w_!web-note.webp)





### 10、评论



45:45



MAE 算法不难

- 利用 ViT 来做跟 BERT 一样的自监督学习
- ViT 文章已经做了这个事情了



**MAE 在 ViT 的基础提升点**

- 需要盖住更多的块，降低剩余块之间的冗余度，任务变得复杂一些
- 使用一个 Tranformer 架构的解码器，直接还原原始的像素信息，使得整个流程更加简单一点
- 加上在 ViT 工作之后的各种技术，训练更鲁棒



以上三点 MAE 使得在 ImageNet-1K 这个数据集上使用自监督训练的效果超过了之前的工作



**写作：简单，故事性好**



导言：为什么 MAE 这么做

非常详细的实验数据：整个 MAE 里面 所有可以调的超参数 它到底是什么意思？





**简单的想法、非常好的结果、详细的实验 ==》**

**一个非常高质量的工作**







**从一篇比较新的文章开始， 怎么样得到你自己的一些研究思路呢？**



vit 在小数据集上要多加正则化约束







我自己对其的总结





这篇论文的标题是《Masked Autoencoders Are Scalable Vision Learners》，由 Facebook AI Research (FAIR) 的团队撰写。论文主要探讨了在计算机视觉领域中，使用掩蔽自编码器（Masked Autoencoders, MAE）作为可扩展的自监督学习者的方法。以下是对论文内容的概要和主要章节内容的描述：



摘要

- 论文展示了==掩蔽自编码器==（MAE）是一种用于计算机视觉的可扩展**自监督学习者。**
- MAE 的方法很简单：随机掩蔽输入图像的块，并重建缺失的像素。
- 论文基于两个核心设计：开发了不对称的编码器-解码器架构，并发现掩蔽输入图像的高比例（例如75%）可以产生有意义且非平凡的自监督任务。
- 这些设计使得训练大型模型变得高效和有效，加速了训练过程，并提高了准确性。
- 论文的方法允许学习高容量模型，这些模型泛化能力强，例如，一个普通的 ViT-Huge 模型在仅使用 ImageNet-1K 数据的方法中取得了最佳准确性（87.8%）。



第1章 引言 (Introduction)

- 论文讨论了深度学习模型的增长趋势，以及在硬件快速发展的推动下，模型对数据的大量需求。
- 作者指出，在自然语言处理（NLP）中，自监督预训练方法（如 BERT）已成功应对这一挑战，而在计算机视觉（CV）中，自监督学习的方法却落后于 NLP。
- 论文提出了一种基于 Transformer 的掩蔽自编码器方法，通过掩蔽图像块并重建这些块来学习视觉表示。



第2章 相关工作 (Related Work)

- 论文回顾了在 NLP 中使用掩蔽语言建模和自回归语言建模的自监督预训练方法。
- 作者讨论了自动编码器在表示学习中的应用，并将其与计算机视觉中的去噪自编码器（Denoising Autoencoders, DAE）联系起来。
- 论文还探讨了自监督学习方法在计算机视觉中的研究进展，特别是在图像对比学习方面的工作。



第3章 方法 (Approach)

- 论文详细介绍了 MAE 的设计，包括编码器和解码器的不对称架构。
- 编码器仅对可见的未掩蔽图像块进行操作，而解码器则从潜在表示和掩蔽标记重建完整的信号。
- 论文还讨论了掩蔽策略、重建目标和简单实现等技术细节。



第4章 ImageNet 实验 (ImageNet Experiments)

- 论文描述了在 ImageNet 数据集上进行自监督预训练的实验设置，并评估了所提出方法的表示学习性能。
- 作者通过与监督预训练和自监督预训练的对比，展示了 MAE 在不同模型大小和掩蔽比例下的性能。
- 论文还探讨了训练计划、解码器设计和掩蔽策略对性能的影响。



第5章 迁移学习实验 (Transfer Learning Experiments)

- 论文评估了使用 MAE 预训练的模型在下游任务中的迁移学习能力，包括目标检测、实例分割和语义分割。
- 作者展示了 MAE 在这些任务中相比于监督预训练模型的性能优势。



第6章 讨论和结论 (Discussion and Conclusion)

- 论文总结了 MAE 的主要发现，并讨论了自监督学习在计算机视觉领域的潜力。
- 作者指出，尽管图像和语言在本质上是不同的信号，但 MAE 能够通过丰富的隐藏表示学习到复杂的视觉概念。



附录 (Appendix)

- 论文提供了实验的额外细节，包括训练设置、超参数选择、模型变体的比较等。

整体而言，这篇论文为自监督学习在计算机视觉领域的应用提供了有价值的见解，并展示了掩蔽自编码器作为一种有效方法的潜力。









## 八、MoCo 阅读





MoCo: CVPR 2020 最佳论文，视觉 + 对比学习的里程碑式的工作



对比学习：

- 简单、好用；
- 19年以来，视觉领域乃至整个 ML 领域最火的方向之一；
- 盘活了从17年很卷的CV领域，MoCo是其中的优秀工作之一



**MoCo：无监督的表征学习工作 在 CV上表现怎么样？**

- 分类：逼近了有监督的 baseline
- 检测、分割、人体关键点检测：大幅超越有监督预训练的模型 (ImageNet 上预训练的模型)
- CV 领域的定心丸，无监督学习真的可以，有可能真的不需要大规模、有标号的数据集做预训练。
- 侧面正面了 **Yann LeCun** 的 NeurIPS 2016 的演讲图



![img](https://i0.hdslb.com/bfs/note/c7a1dfae467d9e5e343fc62889daf30895f33af5.png@620w_!web-note.webp)



蛋糕🎂：ML

樱桃🍒：RL

蛋糕的糖霜 icing：有监督学习

**蛋糕本质：无监督学习**



现在无监督的进展：

- NLP 大模型都是自监督的预训练方式
- CV 的大模型用自监督预训练，也快了



Note: 多听大佬的 talk 报告有好处，找下一个研究方向



**什么是对比学习？+ 前人工作**



01:39



MoCo 动量对比学习：假设读者已经了解对比学习



对比学习：通过对比去学习模型，只需要知道图 1 和 图 2 相似，图 1、图 2 和 图 3 不相似；而不需要真的知道 图 1 和 图 2 代表的是人，图 3 代表的是狗。



![img](https://i0.hdslb.com/bfs/note/976897b4dd45be0ec39bb95b2c56f1d02670c32e.png@620w_!web-note.webp)





3 张图进入一个网络 M 得到特征 f1、f2、f3，在一个学习好的特征空间 embedding space 中，f1、f2 的特征尽量近，和 f3 的特征尽量远离。



![img](https://i0.hdslb.com/bfs/note/d7f2fb3a445b105dd1bee24e89c24ff56f19e18f.png@620w_!web-note.webp)



对比学习学到的很好的特征：类似物体在这个特征空间 相邻，不类似的物体在特征空间 远离



类似 meta-learning 的基于度量的学习？





**Q: 图 1 和 图 2 相似，和图 3 都不相似，难道不是有监督学习吗？Why 对比学习在 CV 领域被认为是无监督训练呢？**

==CV 领域 设计巧妙的代理任务 pre-text task，人为设立一些规则 —— 定义哪些图片相似、哪些图片不相似，为自监督学习提供监督信号，从而自监督训练==





**Example 代理任务 instance discrimination 个体判别**



04:30





一个无标注的数据集，n 张图片，x1, x2, ..., xn



**随机选取一张图片，做 transformation** 



以 x1 图片为例，x1 随机裁剪 + 数据增广 得到 xi1, xi2 （看起来和 x 1 有区别的 2 张照片，x1 的正样本），数据集中的其它图片 x_j, j ≠ i 是 x1 的负样本



i.e., ImageNet-1K 此时不是 1000 个类别，而是 100w 个类别。每个图片都是它自己的正样本，其余都是负样本。





![img](https://i0.hdslb.com/bfs/note/6f316ff053611f410064dc29bb20bf7e04759ca2.png@620w_!web-note.webp)



基于 图片和图片本身的变换是正样本，和其它图片是负样本 的代理任务， + 模型 得到特征，+ 对比学习的目标函数 i.e., NCE loss (正文有提)



![img](https://i0.hdslb.com/bfs/note/ca91999f2c17ed82d14dff4129bf45855c4aaa0e.png@620w_!web-note.webp)



==对比学习的框架：灵活性--定义正负样本的规则==

- 同一个视频里的任意两帧 是 正样本，和其它视频的所有帧是负样本
- NLP, simCSE 把同样的句子扔给模型，但是做 2 次 forward，通过不同的 dropout 得到一个句子的 2 个特征；和其它所有句子的特征都是负样本。 
- CMC 论文：一个物体的不同视角 view（正面、背面；RGB 图像、深度图像）作为不同形式的正样本。
- 多模态领域：Open AI 的 CLIP 模型





### 1、题目和作者



07:33



动量对比学习的方法做无监督视觉特征学习



Momentum Contrast: 动量对比学习

- 动量：(指数)加权移动平均值$ y_t = m * y_{(t - 1)} + (1 - m) * x_t$
- m: 动量的超参数
- y_(t - 1): 上一个时刻的输出
- x_t: 当前时刻的输入
- m 趋近于 1，y_t 改变缓慢，当前时刻的输入 x_t 没什么影响
- m 趋近于 0, y_t 更多依赖于当前时刻的输入。
- MoCo 利用动量的特性，缓慢的更新一个编码器，==从而让中间学习到的字典中的特征尽可能保持一致==。



作者来自 FAIR, 大佬 * 5



### 2、摘要



09:12



本文提出 MoCo (动量对比) 做无监督的表征学习。



**MoCo 从什么角度做对比学习呢？**

**dictionary look-up**, 字典查询任务, a dynamic dictionary with a queue and a moving-averaged encoder 动态字典

- **一个队列**：<font color=red>队列中的样本**无需梯度回传**，可以放很多负样本，让字典变得很大</font>
- **一个移动平均的编码器**：让字典的特征尽可能的保持一致
- 一个大的、一致的字典，有利于 无监督的对比学习 训练。



**本文的亮点是什么？**

结果 nice, MoCo （第一个）在 （分类、检测、分割）主流的 CV 任务证明 无监督学习 也不比 有监督学习 差

- ImageNet 分类：linear protocol 做测试，MoCo 和 之前最好的无监督学习方式差不多
- ==linear protocol （类似 MAE 的 linear probing）测试：freeze backbone 主干网络的参数不调整，只微调最后的分类全连接层（分类头）。把 backbone 当成特征提取器，可以证明 backbone 学习图片特征的好坏。==
- 容易迁移到下游任务。**满足了大家对 大规模、无监督训练 的想象：**
- 7 个 下游任务表现好 counterpart: 模型使用的一样的 i.e., Res50，只是训练方式不一样，i.e., 有监督的带标签数据训练，无监督的不带标签的数据训练。（控制-训练方式-变量法）
- 在大规模的数据集上进行无监督训练，模型学到一个很好的特征，而且学到的特征可以迁移，在小、少标注数据的下游任务取得好的结果。



**MoCo 有什么意义呢？**

==填补了 CV 领域的无监督学习和有监督学习的 gap==



### 3、引言



12:04



**第一段：无监督学习为什么在 CV 不成功？原始信号不一样** 

<font color=red>**NLP 的离散单词更具语义性，CV的连续、高维信号不好构建字典**</font>

引入无监督学习的成功：

- 无监督学习在 NLP 很成功, i.e., GPT, BERT
- 无监督学习在 CV 大幅落后于 主流的 有监督学习

无监督在 CV 不成功的原因是什么？

- 原始信号空间的不同
- NLP 原始信号是==离散的，词、词根、词缀==，容易构建 tokenized dictionaries 做无监督学习
- tokenized: 把一个词对应成某一个特征
- Why tokenized dictionaries 有助于无监督学习？
- **把字典的 key 认为是一个类别，有类似标签的信息帮助学习**
- NLP 无监督学习很容易建模，建好的模型也好优化
- CV **原始信号是连续的、高维的，不像单词具有浓缩好的、简洁的语义信息，不适合构建一个字**典
- 如果没有字典，无监督学习很难建模



**第二段：别人怎么用对比学习的方法在 CV 的无监督学习里？dynamic dictionaries**

近期结合 <font color=blue>对比学习和 CV 的无监督学习效果不错</font>，出发点motivation 不一样，但可以被归纳为 **“动态字典法”**



![img](https://i0.hdslb.com/bfs/note/4803a4afa50f2b1e833ed75914cb824b22e31c27.png@630w_!web-note.webp)



x_1^1: anchor

x_1^2: positive

x2, x3, ......, xn: negative  

编码器 E_11 和 E_12 可以一样，可以不一样 



**Q：负样本使用哪个编码器？**

E_12：因为 positive 和 negative 都是相对 anchor f_11 来说的。==正负样本使用同样的编码器==



![img](https://i0.hdslb.com/bfs/note/f8c8012f7ce51fbf339b746f70ee07909b74c501.png@630w_!web-note.webp)



**Q: 对比学习怎么做？**

f11 和 f12 相近，f11 和 f2, f3, ......, fn 远离



**Q: Why 对比学习可以归纳成 在做一个动态的字典 呢？**



15:25



==f11 当成 query 在 f12, f2, f3, ......, fn 组成的字典的 key 特征条目 k1, k2, ...... 里面查找，dictionary look-up 靠近 f12, 远离 f2, f3, ......==

- be similar to its matching key and dissimilar to others
- learning is formulated as minimizing a contrastive loss 最小化对比学习的目标函数



![img](https://i0.hdslb.com/bfs/note/85b651568e280324a74b1e21f79d3714eec2f1f7.png@630w_!web-note.webp)



**第三段：从动态字典的角度看对比学习，什么样的字典才适合呢？ 大 + 一致性**



17:09





- large 
- 从连续高维空间做更多的采样。字典 key 越多，表示的视觉信息越丰富，匹配时更容易找到具有区分性的本质特征。
- 如果 字典小、key 少，==模型可能学到 shortcut 捷径，不能泛化==
- consistent 
- 字典里的 key (k0, k1, k2, ......, kN) 应该由相同的 or 相似的编码器生成
- **如果字典的 key 是由不同的编码器得到的**，query q 做字典查询时，很有可能 找到和 query 使用同一个 or 相似编码器生成的 key，而不是语义相似的 key。另一种形式的 shortcut solution



![img](https://i0.hdslb.com/bfs/note/8c6f711014b2b167d0fbcd98c74d04cb63856f71.png@640w_!web-note.webp)



已有的 CV 对比学习 动态字典方法在 large or consistent 上有不足。



引言结构：介绍研究动机、相关研究工作的不足、想要达到的目标 ---> 本文的工作





**第四段：本文的 MoCo**



19:07





**为什么要提出 MoCo? 给CV 无监督对比学习 构建一个 大 (by queue)+ 一致 (momentum encoder) 的字典**



图1 MoCo 框架图 queue, momentum encoder





![img](https://i0.hdslb.com/bfs/note/23e8f83e2164097c3695e45d55c969d43e87bcab.png@640w_!web-note.webp)



- queue 数据结构: 剥离 字典的大小 和 显卡内存的限制，让字典的大小 和 模型每次做前向传播的 batch size 的大小 分开
- 字典很大（成千上万），意味着要输入很多很多的图片，显卡内存吃不消
- current mini-batch enqueued and the oldest mini-batch dequeued 当前 mini-batch 入队，最早进入队列的 mini-batch 出队
- 队列的大小 == 字典的大小，但是每次做 iteration 更新，并不需要更新字典中所有 key 元素的值。普通 GPU 训练



- momentum encoder: 
- Q：使用 queue，只有当前 mini-batch 的特征是由当前的编码器得到的；==之前的 key 是由不同时刻的编码器抽取的特征，如何保持 consistent 呢==？
- momentum encoder 由 当前时刻的 encoder 初始化而来	
- $theta_k = m * theta_(k-1) + (1-m) * theta_q$
- 动量参数 m 较大时，theta_k 的更新缓慢，不过多的依赖于 theta_q 当前时刻的编码器，即不随着当前时刻的编码器快速改变，尽可能保证 字典里的 key 都是由相似的编码器生成的特征，保证特征的 consistent





<font color=red>关键是保持特征的一致性</font>

基于 large + consistent dynamic dictionary，MoCo 可以很好的无监督学习视觉特征。



**第五段：MoCo 的代理任务 pretext task？ instance discrimination**



22:04



MoCo 建立模型的一种方式，很灵活，可以和很多代理任务使用



**instance discrimination**: （个体判别）query 和 key 匹配 如果它们来自于同一张图片的不同视角, i.e., 不同的裁剪



MoCo 用 instance discrimination 无监督训练 在 ImageNet 上可以和之前最好的结果打个平手 or 更好的表现 competitive results





**第六段：MoCo 的效果怎么样？ 卖结果**



23:05



**无监督学习的目的**：==在一个很大的无标注的数据集上训练，模型学到的特征可以很好的迁移到下游任务==。



MoCo 做到了。7个检测 or 分割的任务表现很不错。超越 ImageNet 有监督训练的结果，甚至有时大幅度超越 in some cases by nontrivial margins.





**无监督学习的期待：更多数据、更大的模型，性能会提升，不饱和。**



MoCo 在 10亿 Instagram 数据集（更糙 relatively curated 真实*******、一张图片有多个物体; ImageNet 数据集的图片大多只有一个图片、在图片中间） 上性能还有提升





中型 ImageNet or 大型 Instagram 数据集，MoCo 把 无监督学习和有监督学习的 坑🕳 填平。



应用展望：之前 ImageNet 预训练好的模型，可以尝试替换为 MoCo 预训练好的模型。



### 4、结论



25:33



==结论：MoCo在一系列的任务和数据集上效果很好 positive result==



- 1000 倍数据集数量的增加， MoCo 性能的提升不高
- 大规模数据集可能没有完全被利用
- 尝试开发其它的代理任务 pretext task
- <font color=red>除了 instance discrimination 代理任务，类似 NLP 的代理任务 masked auto-encoding</font>
- MAE, 大佬 2 年前就有了想法，做了实验；做研究急不来
- 像 NLP 的 BERT 使用 masked language model 完形填空做自监督预训练



==点题总结：MoCo 和 其它对比学习的 代理任务的解和==

- MoCo 设计的初衷：去构造一个大的字典，从而让正负样本能够更有效地去对比，提供一个稳定的自监督信号，最后去训练这个模型





### 5、相关工作



27:30





unsupervised / self-supervised learning: 

- 自监督学习是无监督学习的一种。
- 前人研究不怎么区分，MoCo使用 无监督学习 unsupervised learning (定义更广泛一些)



两个可以做的点：pretext tasks and loss functions

- **代理任务**：不是大家实际感兴趣的任务 (检测、分类、分割实际应用任务)，而是为了 学习一个好的数据特征表示
- **损失函数**：和代理任务可以分开研究。 <font color=red size=5>MoCo 的创新点在损失函数，又大又一致的字典 影响 info NCE 目标函数的计算</font>





28:30





==损失目标函数：衡量 模型的预测输出 和 固定的目标之间的 difference==。

- L1 or L2 losses 
- i.e., Auto-encoder（生成式网络的做法）, 输入一张原图 or 一张被干扰的图，经过编码器、解码器 重构输入的图，衡量是原图 和 重构图 之间的差异。



**判别式网络**：eight positions 2015

一张图片 打成 有序号的 9 宫格，给 中间的 第 5 格 和 剩下随机挑一格，**预测**随机挑的这一格是中间 第5 格 的**方位**（8个方位可选）。



pretext tasks：分类任务，因为每一个方格都自带序号，输出分到 8 个方位的哪一类。





**损失函数：判别式、生成式、对比学习、对抗学习**



- 对比学习的损失：**目标不固定，训练过程中不断改变。目标有编码器抽出来的特征（MoCo 的字典）而决定**
- 判别式：预测 8 个位置中的哪一个方位
- 生成式：重建整张图
- 对比学习的目标：==测量 样本对 在特征空间的相似性==。
- 相似样本离得近，不相似样本离得远
- 最近无监督表现好的文章都用了 contrastive learning (Sec. 3.1 讨论)





- 对抗学习的损失：衡量两个概率分布之间的差异，i.e., GAN
- unsupervised data generation 做无监督的数据生成
- 对抗性的方法做特征学习
- 如果可以生成很好、很真实的图片，模型应该学到数据的底层分布
- GAN 和 NCE 的关系 noise-contrastive estimation Ref. [24]





代理任务的生成：

- denoising auto-encoders 重建整张图
- context auto-encoders 重建某个 patch
- cross-channel auto-encoders (colorization) 给图片上色当自监督信号
- pseudo-labels 图片生成伪标签
- exemplar image 给同一张图片做不同的数据增广，它们都属于同一个类。
- patch ordering 九宫格方法：打乱了以后预测 patch 的顺序, or 随机选一个 patch 预测方位 eight positions 
- 利用视频的顺序做 tracking 
- 做聚类的方法 clustering features



对比学习和代理任务的关系：

- **不同的代理任务 可以和 某种形式的对比学习的目标函数 配对使用**
- MoCo 论文里 instance discrimination 个体判别方法  ++++ examplar based 代理任务很相关
- CPC contrastive predictive coding 用上下文信息预测未来 ++++ context auto-encoding 上下文自编码
- CMC contrastive multiview coding 利用一个物体的不同视角做对比 ++++ colorization 图片上色（同一个图片的 2 个视角：黑白 和 彩色）



**相关工作总结：**



32:38



简洁明了

从 代理任务 和 目标函数 （2 个和有监督学习不同的点）写相关工作



有监督学习的过程



无监督学习 or 自监督学习 缺少 ground truth，没有标签怎么办？

- 代理任务来帮忙，自己造标签。
- 代理任务生成自监督的信号，充当 ground truth 的标签信息



**有输出 y 和 标签信息 ground truth，还需要什么呢？**

==目标函数 L，衡量 输出 Y 和 标签 ground truth 的差异，让模型学到更好==



MoCo 从 目标函数 L 和 代理任务 pretext tasks 生成 ground truth 写相关工作





### 6、MoCo方法



33:44



**3.1 Contrastive learning as dictionary look-up** 



对比学习和最近的发展，都可以看成是一个训练一个 encoder 来做 字典查询 的任务





### 7、实验



01:06:10











### 8、总结



01:23:20



感谢 MoCo 论文和高效实现，普通 GPU 跑对比学习的实验，做激动人心的研究。

MoCo 激励学者研究 “MoCo 学出来的特征 和 有监督学习学出来的特征有什么区别？还能从什么方向提高对比学习？”



**期待对比学习的论文串烧**

第一阶段：Contrastive Predictive Coding (CPC), CMC Contrastive Multiview Coding, 

第二阶段：MoCo v1, simCLR v1, MoCo v2, simCLR v2 

第三阶段：不需要负样本的 BYOL, bootstrap your own latent, SimSiam

第四阶段： 用了 vision transformer 的 MoCo v3, 





## 九、CLIP 串烧





过去一年中，大家如何将CLIP模型和思想应用到其他领域中去。

![img](https://i0.hdslb.com/bfs/note/9088935963e04eddac34a9dda1f580c156b7d2f4.jpg@690w_!web-note.webp)



![img](https://i0.hdslb.com/bfs/note/83fb66ea6fb6cbfc8ae6f361644e5712878cade7.jpg@690w_!web-note.webp)



![img](https://i0.hdslb.com/bfs/note/a79f7e1aa1e092646bc1b6e7a380ba05caf014c3.jpg@690w_!web-note.webp)



CLIP： 给定图像文本对，分别通过对应编码器，对角线上元素是正样本，其他位置是负样本。结果是zero-shot能力非常强。推理的时候，对于一个图片，把所有可能候选都作为prompt丢进模型，计算相似度，最大相似度对应的文本标签就是类别。

### 1、Lseg





![img](https://i0.hdslb.com/bfs/note/2e59f44debf0cb71d1060c8655d59ebc3683f715.jpg@690w_!web-note.webp)

图片分类->像素级别分类。

![img](https://i0.hdslb.com/bfs/note/2c85013193c2abf8491b4262e7c358755c84f1a5.jpg@690w_!web-note.webp)

zero-shot 分割 零样本

![img](https://i0.hdslb.com/bfs/note/7c6fce0725f7e8ffc3cac847380f378d1685d8fe.jpg@690w_!web-note.webp)



dpt : vision transformer + decoder

文本编码器：用的CLIP的文本编码器，而且自始至终是锁住的。

文章意义是：将文本加入到传统图像任务中

![img](https://i0.hdslb.com/bfs/note/5cf6d66f38d96c8772714511a7cb3552ec7b5a3d.jpg@690w_!web-note.webp)



![img](https://i0.hdslb.com/bfs/note/1b6c279009f2287f1bf5609557d2734650851552.jpg@690w_!web-note.webp)

Lseg zero-shot还是比1-shot这种要差很多，有十几个点的提升空间。

- **LSeg可以直接应用于新的类别,而无需为这些类别重新训练模型,这是zero-shot模型的典型特征（为什么说LSeg是zero-shot？ perplexity回答）**
- One-shot，少样本，每个新类别至少有一个分类。

 

![img](https://i0.hdslb.com/bfs/note/f50a3345c999ec1116c555ed920701ae2c5282fd.jpg@690w_!web-note.webp)



failure cases: 

- CLIP本质上是选择相似性
- zero-shot做分割能提升的空间很大



### 2、group ViT

![img](https://i0.hdslb.com/bfs/note/87028fb13c45875766a7f72290b1ca1b433d7925.jpg@690w_!web-note.webp)

- LSeg虽然用了CLIP的预训练参数，但是不是无监督学习框架，也不是对比学习，没有对文本进行学习，还是依赖手工标注的segmentation mask。

- 如何摆脱掉手工标注，如何用文本来做监督信号，从而达到无监督训练？ GroupViT在这个方向上做出了贡献。

![img](https://i0.hdslb.com/bfs/note/8734010b5951b2d096d511abab54f96247ed5615.jpg@690w_!web-note.webp)

视觉grouping： **聚类中心点，从点开始发散，得到group，是一种自下而上的方式。**

groupViT贡献：已有的ViT框架中，加入grouping block，同时加入可学习的group tokens。

图像编码器输入：patch embedding + group tokens

group tokens : 64 * 384, 64是希望刚开始有比较多的聚类空间，384是为了和patch embedding同维。

经过6层transformer layer学习之后，加了一个grouping block，认为此时group token已经学的挺好的了，尝试来cluster一下，合并成为更大的group，学到更有语义的信息。此时加入grouping block，将patch embedding分配到group token上。此时整个ViT的输入长度从196 + 64 -> 64

![img](https://i0.hdslb.com/bfs/note/ed9e25d5b1d0591955bb07f4a0b8a9e7f51914cc.png@690w_!web-note.webp)

聚类分配的过程是不可导的，这里使用**gumbel softmax**将整个模型变成可导的，从而整个模型就可以做端到端的训练了。

到此为止完成了第一阶段的grouping。

由于一般segmentation中类别也不会很多，所以这里作者加了新的8个grouping tokens(8*384)，希望将64个再次映射到8个，本文作者在第9层transformer layer之后再做了一次groupign block。图像分成了8大块，每个块对应了一个特征。



challenge: 文本有一个特征，但是图像为序列长度为8的特征序列，如何将8打开特征融合成1块，变成整个图像的image level的特征，作者采用了average pooling。



总的来说，模型还是比较简单，所以scale性能可能比较好。



模型怎么做zero-shot推理？

![img](https://i0.hdslb.com/bfs/note/9245646a8579816da5d9422d976b01ca62ee37fa.png@690w_!web-note.webp)



局限性： 模型最后只有8个group embedding，图片分割最多只能检测8类。

问题： group token如何映射到原图片上？

![img](https://i0.hdslb.com/bfs/note/b89e2c07c71617bad5804a986550398bcf47d28a.jpg@690w_!web-note.webp)

上图对应groupin的效果



数值上的表现

![img](https://i0.hdslb.com/bfs/note/4cf87a3286871be4592fa78baba19707741bb858.png@690w_!web-note.webp)

无监督分割还是很难

两个limitations:

- ==现在groupViT更偏向图像编码器，没有很好使用dense特性==。
- 分割中的背景类，groupViT不光选择最大相似度，还设置了相似度的阈值。如果和所有类的相似度都达不到阈值，就认为是背景类。对于类别很多的情况，很容易出现前景物体置信度和背景物体置信度差不太多，如何设置阈值就很重要。分割做的很好，但是分类做的比较差。（这里将预测分割的部分和gt对上，所以只要分割做的好，分类就一定正确，发现这样性能提升了20多个点）。 这个问题的本质是因为CLIP的训练方式，只能学习到物体语义非常明确的东西，学不到非常模糊的（比如背景）的类。解决方案：可学习阈值？修改zero-shot推理方式？训练中增加约束将背景类融入其中。



### 3、ViLD

![img](https://i0.hdslb.com/bfs/note/bc1745f6d78ec200f1513054edd129f415cd7bc3.jpg@690w_!web-note.webp)

VilD

==看标题就知道是把clip当成一个teacher，从而去蒸馏模型，从而达到zero shot做目标检测==。



引言：

![img](https://i0.hdslb.com/bfs/note/c7f159478f0da00abf42e96749d48259b835ec06.png@690w_!web-note.webp)

是否可以在现有数据集上，不去做额外标注（黄鸭子），检测出新类别。

模型：

![img](https://i0.hdslb.com/bfs/note/dd76cc73b16449ceacd48099a5ef2360f5792451.jpg@690w_!web-note.webp)



a是有监督的baseline,bcd都是viLD方法。

baseline就是两阶段mask rcnn。

![img](https://i0.hdslb.com/bfs/note/dccec2278d7daeaeee5e696fef5234cd4eca873b.jpg@690w_!web-note.webp)

这里的文本类别还是base类，做有监督训练。

**ViLD-text只是把图像特征和文本特征连接到了一起。其他类别都塞给了背景类，专门有一个背景类embedding，在模型中学习。**

数学公式：

![img](https://i0.hdslb.com/bfs/note/fa392d85bfc6e388da5cf939687dd00eb55fc46d.jpg@690w_!web-note.webp)

图像I，phi(I)抽取图像特征，r是提前知道的proposal，经过额外的计算R，就得到region embedding e_r。

接下来定义了一个background embedding, e_{bg}，然后还有从文本中的t1,t2,..,t_{CB}, 分别点乘计算相似度，类似logits，然后和gt计算ce loss。

![img](https://i0.hdslb.com/bfs/note/34c84b4d674491e0496e9f9563eb6b13fdbff768.jpg@690w_!web-note.webp)

ViLD-image，想法： 希望region embedding尽可能和CLIP一致，最简单的方法是知识蒸馏。**粉色背景是teacher网络，左边是student网络。**使用L1 loss做蒸馏。因为现在的监督信号不再是人工标注，而是CLIP的编码，这里不再受基础类的限制，抽的proposal既可以是基础类的proposal，也可以是新类的proposal，都可以训练了，增强了做==open-vocabulary==的能力。 弊端：这里没有用上全部N个proposal，但是这里只用了M个，提前把每个图片，利用提前训练好的RPN(region proposal network)，预抽取M个proposal，这些proposal全部做crop & resize，过CLIP模型，把M个clip image embedding抽好，所以训练时候，只需要硬盘上load过来就行了。

![img](https://i0.hdslb.com/bfs/note/5552b29a9e4df78dfc69aa8b3cc8d05e9c3fb868.jpg@690w_!web-note.webp)

<font color=red>ViLD = VilD text + VilD image</font>

![img](https://i0.hdslb.com/bfs/note/209f5ad6cb489ebd7675f042403804c58dd88410.jpg@690w_!web-note.webp)

模型总览图



主要的表格

LVIS数据集：非常**长尾的目标检测数据集**，共1203个类，使用的还是COCO的图片，有一些类只标注了一次两次，所以这里有三个类，r(rare), c(common), f(frequent)。这样算AP的时候，就可以针对这几个分别算AP。

![img](https://i0.hdslb.com/bfs/note/911f84497dd2d637f68cd07142739c1490921f0e.jpg@690w_!web-note.webp)

common和frequent认为是基础类，rare是novel类。

但是这里LVIS数据集的特性导致有监督的模型也未必能够在rare上做的很好。





![img](https://i0.hdslb.com/bfs/note/19b549a59fecf4c74422545794913f625baf5b13.jpg@690w_!web-note.webp)

zero-shot方法，==直接拓展到其他数据集上，虽然比有监督上差，但在小规模数据集上，已经比较接近了，所以它 open-vocabulary上已经做的挺好了==。



### 4、GLIP



![img](https://i0.hdslb.com/bfs/note/ea598ae15026e842707fdf0d398d43f41dd7376f.jpg@690w_!web-note.webp)

研究动机：怎么利用更多数据（没有精心标注数据），==将图像文本对用上==。

vision grouding任务，给一句话，将这句话中的物体和当前图片中的物体找出来。

将detection和phrasing grounding合起来。



**Unified Formulation**

![img](https://i0.hdslb.com/bfs/note/e0ec70b9ea2048712a203eadf42e16496bd52c57.jpg@690w_!web-note.webp)

detection分类loss怎么算？vision grouding分类loss怎么算？如何合并到同一个框架下面。



01:03:1420240712





![img](https://i0.hdslb.com/bfs/note/e9082ce9b82848b2dd43cad540900809257667ea.png@690w_!web-note.webp)



detection的cls loss计算方式： bounding box，接一个分类头，然后得到分类logits，然后用nms把bounding box筛选一下，和gt计算cross entropy loss.

![img](https://i0.hdslb.com/bfs/note/1a76d2505aaf95bfe1b25040a5ab30bb94fde215.png@690w_!web-note.webp)



vision grounding cls loss计算方式：

匹配分数，图像中一样的处理，句子变成prompt，过一个文本的编码器，然后和图像embedding计算相似度（画图和VilDtext一致）



两种方式差不多，判断什么时候算positive match，什么时候算negative。



![img](https://i0.hdslb.com/bfs/note/fabd8625b3ea3a3eb40ecf48cccfd62ea9cfc682.png@690w_!web-note.webp)

Caption是self-training，用的伪标签peuodo label





![img](https://i0.hdslb.com/bfs/note/7d47a0b4875957f7fa710c7f472987c9951c8243.jpg@690w_!web-note.webp)

模型的总体框架

![img](https://i0.hdslb.com/bfs/note/2ecf0dd31576db3b47f130b95cd90bdf5fff7177.png@690w_!web-note.webp)

效果





![img](https://i0.hdslb.com/bfs/note/fbd2abe0c5a98957aa84644e462f4e5a090834cf.jpg@690w_!web-note.webp)

数值结果，zero-shot可以达到50AP, finetune一下，也可以达到60。

GLIP不论是zero-shot还是finetune，性能都非常强。

### 5、GLIP V2



![img](https://i0.hdslb.com/bfs/note/fde41d81dd001f7a5c487f6f5ce9f3e19097aa9e.jpg@690w_!web-note.webp)

GLIPv2: 分割检测、VQA、Visual grouding、Visual captioning都放进来了



![img](https://i0.hdslb.com/bfs/note/0550f45215f65ca53c20910163058088727736b9.jpg@690w_!web-note.webp)

思想和框架和GLIP差不多，但是加入了更多任务。

OFA， Unified-IO，提供统一框架囊括更多任务，争取把预训练任务学的又大又好。





### 6、CLIPasso



（CLIPasso: Semantically-Aware Object Skectching）  语义感知的对象偏移

**将CLIP做teach, 用它蒸馏自己的模型**

![img](https://i0.hdslb.com/bfs/note/f6ae4415e057030cd079937ae7ff142582a95fe0.png@690w_!web-note.webp)



- ​	semantic loss: <原始,生成>特征尽可能的接近
- ​	几何形状上的限制，==geomatric loss==: 
-  perceptual loss把模型前面几层的输出特征算<原始，生成i>的相似性，而不是最后的2048维的特征（<font color=red>因为前面的特征含有长宽的概念，对几何位置更加的敏感</font>）。保证几何形状，物体朝向 位置的一致性
- **基于saliency的初始化方式：**用一个训练好的VIT，把最后一层的多头自注意力加权平均得到一个saliency map，对saliency map显著的地方进行采点。（**在显著的地方采点其实就相当于自己已经知道了这个地方有物体或已经沿着这个物体的边界画贝兹曲线了）效果更稳定**

![img](https://i0.hdslb.com/bfs/note/f76c00ac85d431a3361e51d6e03acf4d3e28202d.png@690w_!web-note.webp)



- 一张V100 6min 2000 iters
- 后处理：一张input，三张简笔画，取两个loss最低的那张

优点：

- zero-shot: 不受限于数据集里含有的类型
- 能达到任意程度的抽象，只需要控制笔画数

![img](https://i0.hdslb.com/bfs/note/33c9ff5605b146ebf52507ec56e619df0372edac.png@690w_!web-note.webp)

局限性：

- <font color=blue>有背景的时候，效果不好</font>（自注意力图等不好）-> automatic mask的方式如U2Net，将物体扣出里（但是是two step了，不是end to end）
- 简笔画都是同时生成的，不像人画的时候具有序列性（做成auto-regressive，根据前一个笔画去定位下一笔在哪 ） 及其没有时序
- 必须提前制定笔画数，手动+同等抽象度不同图像需要的笔画数不一样多，（将笔画数也进行优化）

CLIP+视频



### 7、CLIP4clip



>**Empirical study** 是一种基于**观察**和**实验**数据的研究方法，==通过直接收集和分析实际数据，来验证理论或提出新发现==。这种研究方法与纯理论研究相对，它依赖于现实世界的证据，并通过系统的观测和实验设计来探索、验证或反驳假设。





CLIP4clip: An empirical study of CLIP for end to  end video clip retrieval

![img](https://i0.hdslb.com/bfs/note/b3f37256c61561cc84a635fce7cd9a52e613fd53.png@690w_!web-note.webp)



视频是有时序的。一系列的帧，10个image token(cls token)如何做相似度计算:

1.<font color=red>parametr-free 直接取平均（目前最广泛接受的）。没有考虑时序，区分不了做下和站起来</font>

2.加入时序，LSTM或transformer+位置编码

late fusion:==已经抽取好图像和文本的特征了，只是在最后看怎么融合==

![img](https://i0.hdslb.com/bfs/note/f35237114f62f04b0e31adc43307768ee5e09b85.png@690w_!web-note.webp)

3.early fusion：最开始就融合

文本和位置编码, patch喂入一个transformer

![img](https://i0.hdslb.com/bfs/note/5e23f48b32c660b0dc311c44f7d9bd39d1e90dfe.png@690w_!web-note.webp)

直接拿CLIP做视频文本的retrieval,效果直接秒杀之前的那些方法

少量数据集：直接mean效果最好（CLIP在4million上训练的，微调反而不好）

![img](https://i0.hdslb.com/bfs/note/29437bad9ab5584a443ab0c558d4d120fdd2fd8a.png@690w_!web-note.webp)

 

![img](https://i0.hdslb.com/bfs/note/eb6997663872f5e5285385dfa29ab0f164f2e6cd.png@690w_!web-note.webp)

So, 大家都是直接mean

insights:

![img](https://i0.hdslb.com/bfs/note/0b01627a7e11646d172b4bbae6bbad03fb312d98.png@690w_!web-note.webp)

 Gradient search,多试几组学习率。



### 8、ActionCLIP



ActionCLIP: 动作识别

动机：

- 动作识别中标签的定义，标记是非常困难的。
- ==遇到新类，更细粒度的类==

![img](https://i0.hdslb.com/bfs/note/8c23be82e9c0eeae5acaf529db782d4700c51a60.png@690w_!web-note.webp)

==因为这里的文本就是标好的labels，非对角线点也可能是正样本。->交叉熵换成KL散度(两个分布的相似度)==

三阶段：pre-train, prompt, finetune

![img](https://i0.hdslb.com/bfs/note/be61ae79f0eab8ec3e3a03b9927f2d8f4cf4d278.png@690w_!web-note.webp)



![img](https://i0.hdslb.com/bfs/note/64729aef9c1435af7bae2373390b6a06129ea058.png@690w_!web-note.webp)



==**shift: 在特征图上做各种各样的移动，达到更强的建模能力。没有增加额外的参数和存储**。==

19年tsm将shift用到了时序

shift window，swin transformer里有用到

![img](https://i0.hdslb.com/bfs/note/f86b1c615e5118bfc0a0686d3d072202fd96cdbc.png@690w_!web-note.webp)

multimodal framework: 把one hot的标签变成language guided的目标函数

都是RGB+分类，使用CLIP预训练好的效果更好

![img](https://i0.hdslb.com/bfs/note/8083145d8f62590077ce13a633e06c859f3c4402.png@690w_!web-note.webp)

因为识别的数据集很大，funetune足够了

![img](https://i0.hdslb.com/bfs/note/c2520edcf2d62ce6f33b89a283a63bae2818fd97.png@690w_!web-note.webp)

zero/Few-shot的能力：

![img](https://i0.hdslb.com/bfs/note/85ebba1d600ac7f5f35e94d5bbfc4adc5361cef3.png@690w_!web-note.webp)

视频还有很多难点



55:21利于下游任务



拿CLIP作为visual encoder for diverse 下游vision-language tasks的初始化参数, 再finetune



56:06clip+语音



### 9、AudioCLIP

![img](https://i0.hdslb.com/bfs/note/e0cc25b945d5c73b1ef55309480c616158f6e8ea.png@690w_!web-note.webp)

文本，视频（帧），语音成triplet

**三个相似度矩阵，loss**

zero-shot语音分类



57:303D

### 10、PointCLIP



数据集很小

只要是RGB图像，CLIP都能处理的很好

![img](https://i0.hdslb.com/bfs/note/d48af73212a28714f4ad3a0b129059eef1a4dfe3.png@690w_!web-note.webp)

prompt: 明确告诉是点云





59:21

### 11、DepthCLIP



把深度估计看成了一个分类问题而不是回归



![img](https://i0.hdslb.com/bfs/note/dae5655cd2afdbe9ffd16a5629e97e3ed746e8c5.png@690w_!web-note.webp)

类别和[0.5,1,1.5..]对应



总结：

1.仅用CLIP提取更好的特征，点乘

2.clip做teacher，蒸馏

3.不用预训练的CLIP，仅用多模态对比学习的思想



### 12、拓展总结





回顾CLIP，用对比学习的方式学习一个视觉-语言的多模态模型。


1.对比学习预训练，文本和图片分别经过编码器得到特征。对角线上为n个正样本对，其他位置为n2-1负样本对。图片特征与文本特征建立了联系，此时模型从图片学到的特征可能不仅仅是图片本身的特征，还有文本的语义信息。openAI自建大规模的数据集WIT（webimage text）

2.zero-shot推理，prompt template。单词变成句子（预训练时是句子，避免distribution gap），再经过预训练好的文本编码器，得到文本特征。

3.测试图片经过预训练好的图片编码器，得到图片的特征。将图片特征与文本特征进行cos相似度计算，进行匹配。

与图片对应的文本可以看做高级标签，文本与图像建立了联系，文本引导模型从图片中提取文本的语义信息。 



#### ①Lseg



![image-20241005221929785](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005221929785.png)

第一行图中，能够完美的将狗和树分开，为了验证模型的容错能力，加一个汽车vehicle的标签，模型中也并没有出现汽车的轮廓。另一方面，==模型也能区分子类父类，标签中不再给出dog而是给出pet，dog的轮廓同样可以被分割开来==。

第三行图中，椅子、墙壁甚至地板和天花板这种极为相似的目标也被完美的分割开来。

![image-20241005222045802](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222045802.png)


如上图，与CLIP结构非常像，模型总揽图中**图像和文本**分别经过图像编码器（Image Encoder）和文本编码器（Text Encoder）==得到密集dense的图像文本特征==。此处密集的图像特征需进一步放大（up scaling）得到新的特征的图与原图大小一致，这一步也是为分割任务的实现。然后模型的输出与ground true的监督信号做一个交叉熵损失就可以训练起来了。Image Encoder的结构就是ViT+decoder，其中decoder的作用就是把一个bottleneck feature慢慢upscale上去。

![image-20241005222223938](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222223938.png)

这里的Loss不像CLIP使用对比学的loss，而是跟那些Ground True mask做的cross entropy loss，并非一个无监督训练。这篇论文的意义在于将文本的分支加入到传统的有监督分割的pipeline模型中。通过矩阵相乘将文本和图像结合起来了。训练时可以学到language aware（语言文本意识）的视觉特征。从而在最后推理的时候能使用文本的prompt任意的得到分割的效果。

![image-20241005222301886](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222301886.png)


本文中文本编码器的参数完全使用的CLIP的文本编码器的参数，因为分割任务的数据集都比较小（10-20万），==为保证文本编码器的泛化性，就直接使用并锁住CLIP中文本编码器的参数。图像编码器使用Vit / DEit的预训练权重，使用CLIP的预训练权重效果不太好==。


​                       ![image-20241005222310597](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222310597.png)                      

Spatial Regularization Blocks这个模块是简单的conv卷积或者DWconv，这一层进一步学习文本图像融合后的特征，理解文本与图像如何交互。后边的消融实验证明，两层Spatial Regularization Blocks效果最好，但是四层Spatial Regularization Blocks突然就崩了。其实Spatial Regularization Blocks这个模块对整个性能没有多大影响，可以先不去考虑。

![image-20241005222355195](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222355195.png)

PASCAL数据集上的结果，LSeg在zero-shot 上效果要好不少，但是对于1-shot来说还是差了15个点左右。如果使用大模型（ViT-L）也还是差了6个点左右。

![image-20241005222415291](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20241005222415291.png)

<font color=red size=5>本质上再算图像特征和文本特征之间的相似性，并不是真的再做一个分类，就会把dog识别成toy玩具狗</font>。  

 

#### ② Group Vit



[Group ViT（Semantic Segmentation Emerges from Text Supervision）CV - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv18810713/?spm_id_from=333.999.0.0)



#### ③ ViLD



[ViLD（Open-Vocabulary Object Detection via Vision and Language Ko - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv18815536/?spm_id_from=333.976.0.0)





#### ④ GLIP V1/2

[GLIP_V1/V2（Ground Language-Image Pre-train）CVPR2022 - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv18815566/?spm_id_from=333.976.0.0)



#### ⑤CLIP Passo

[CLIP Passo：Semantically-Aware Object Sketching图像生成抽象的简笔画 - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv18854569/?spm_id_from=333.976.0.0)



#### ⑥CLIP4 clip



[CLIP4clip：An Empirical Study of CLIP for End to End Video Clip R - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv18854602/?spm_id_from=333.976.0.0)



